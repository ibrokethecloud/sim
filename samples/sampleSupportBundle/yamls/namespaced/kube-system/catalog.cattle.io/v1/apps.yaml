apiVersion: v1
items:
- apiVersion: catalog.cattle.io/v1
  kind: App
  metadata:
    annotations:
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/8RYXW/bvBX+KwavJVnyZypgwNo4XYO1aZBkxbC1F0fUkcSZIjmSkuMF/u8DRfkrTdr0fVv3ziGPzvOc78M8kBot5GCBpA8EhJAWLJPCuD9l9h+k1qCNNJMRBWs5RkwOWU5SUiGvQ1CKBM/KyZVAHZbtkqRk2CbB4O9M5H+5RarRfvczATWSlJgqckiRRo5gMGqTSC9xFFIQwKM2eZEeo4A6Zcsmw9CsjcWabAJCNXbG3rEajYVakVQ0nAeEQ4b8my6owFQkJTCfj1/ROcJZQmEO89l8gjCdxnOcTbNpkufT8Rxmk7FD6w3asyf+7ClyAenI32CBGgVFQ9J/PxBQ7BNqw6QgKelMz7iky49OdIEcbXdTADcYECqF1ZJz1CS1usGALJlwgdv5/+UebrqIJzk9w/kIwmKW5+EEX2EIU8zC6Wx+Nskm06SYTsnmyyYgRiF13qMVaOt+HCXZV1aAUgcn42gURyMSkBwN1Ux5q8ilMBY4H5w7WoMrtCupl4Nr3pRMRCQgleysqaxVJh0OV6tVpLR00aPAGZWR1OWQBGSJ65XUufMo8WH4EpAamLDABGrvaayBcZJ6A8xfNQhaoY6orPd+u/GHg/eQGbL58nSAjWy0D9+OWcls1WRO17DXO/SfdFiOTfvIGUmYNYzno3iUJEnyKp64dGqBN9jlqLev+8UbY1HfrVWXUGcm2PKggl3WUKKT0qikYVbqtWPbU6hA5ygwD3ttAbFQ7sNxzMDhu3AaKzVu0ZoMtUCLxoWOGcg4vmUc38uyZKLcJmGBnN0vsICG2wuRK8mEvZPvpLGvaR/p1+fnF9d3pJd9C4wbKPBSZLIRuZO8ls5RKXks8rGxz8q8Q+C2uhCOVn5E5lJZd2jeAF1iVyHQWEke3d5godFUl8KiboGTdBbvJNrZbaOUdKneF1938V6Wt9iiZnZ9SzWis42JYqf6WssabYWN+YBWM2qeYvfPxfXueKuc4/0nyZsaffYvmO4aLOghZ9nQBYKjHbadiOpEXEzcZ63kPzkNajD/bVBDjlvawpcmE+Xeo0IKdJUjc/zJ+HatKrhF3TKKV74Ae7DG4LXMzy8XN1tmLdqq+XD3D5Imk2kckBUw+1bqxTaTvdzGuQqEQN51K1121RuGTIXOWBKQMPS9uskE2rAutSvabGdse89BkICwwjd29/NlRm9xd1bHUTJ9XP5kswlIyWUG/KDmnZ3txKMdHs38kZ8rfd3dYMmM7Sg4FyqZn7Pc5VASR5NRFEfxMJl1MG4w7FrzOPDpmz4805qprBVH65xfMG3sAhWXa5e5ZBSPRmE8DuPZ3WiUTuZpcvYv4gbtS6SEdF0lJb719/k18Jk9qMAMMkQxYJ4F5tFn8Vlcfby7SAeXdlDDemBhiYMCVwPnCQu2cdryLe4Pz2aN+7b+1DzbDdouK19TKhtxMHC/B7AJvqH0XIqClR9AkSdYh7S7/RHdoBjeWxTuTxMtz4zbbo4AG2NlfdObvMCCCdbFfYeflcrjNtrvjhHV+dfD91dhK0R9Uki3c0HhL/CUyH1duyLU9cld3Y2j3xZo3/D62leSM3pa3x/hG7SnxK6ksdjvSqfEZQrqLttPDeqz7NSoFYicnzSpmFJS8lMi/r4C+gOlozOgETS2kpr9r+s4T0L7rngjOR7OWKc2dAvnT9W73dD+vM43TOTuXXS8FoS/HKBfrJ8IsTJHqhaAtRS3h/8qeNF+5J7C+8fpwSq93Ycr0C06lmFd1pY8t836fXS620eDr1bdb1zPjjbhxdVtLz6O4iiJD26ke/rvZ2vEJe2sc6a6h8Gjx9Xhi90tk37FO6A73vN5fufeHLzxk81+J30gMnMqMf8bCvRDlqRxQExT1+A+fthsNv8PAAD///BkS/e0EwAA
      objectset.rio.cattle.io/id: helm-app
      objectset.rio.cattle.io/owner-gvk: /v1, Kind=Secret
      objectset.rio.cattle.io/owner-name: sh.helm.release.v1.rke2-canal.v1
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2022-03-06T22:48:49Z"
    generation: 1
    labels:
      objectset.rio.cattle.io/hash: a7739c7ea81ca7a7674ea5507e65b51dd537a643
    managedFields:
    - apiVersion: catalog.cattle.io/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:objectset.rio.cattle.io/applied: {}
            f:objectset.rio.cattle.io/id: {}
            f:objectset.rio.cattle.io/owner-gvk: {}
            f:objectset.rio.cattle.io/owner-name: {}
            f:objectset.rio.cattle.io/owner-namespace: {}
          f:labels:
            .: {}
            f:objectset.rio.cattle.io/hash: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"1dc8e72a-f6dd-4e9e-a5eb-56784b451f55"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:spec:
          .: {}
          f:chart:
            .: {}
            f:metadata:
              .: {}
              f:apiVersion: {}
              f:appVersion: {}
              f:description: {}
              f:home: {}
              f:keywords: {}
              f:maintainers: {}
              f:name: {}
              f:sources: {}
              f:version: {}
            f:values:
              .: {}
              f:calico: {}
              f:flannel: {}
              f:global: {}
              f:podCidr: {}
          f:helmVersion: {}
          f:info:
            .: {}
            f:description: {}
            f:firstDeployed: {}
            f:lastDeployed: {}
            f:notes: {}
            f:status: {}
          f:name: {}
          f:namespace: {}
          f:resources: {}
          f:values:
            .: {}
            f:flannel: {}
            f:global: {}
          f:version: {}
        f:status:
          .: {}
          f:observedGeneration: {}
          f:summary:
            .: {}
            f:state: {}
      manager: rancher
      operation: Update
      time: "2022-03-06T22:48:50Z"
    name: rke2-canal
    namespace: kube-system
    ownerReferences:
    - apiVersion: v1
      blockOwnerDeletion: false
      controller: true
      kind: Secret
      name: sh.helm.release.v1.rke2-canal.v1
      uid: 1dc8e72a-f6dd-4e9e-a5eb-56784b451f55
    resourceVersion: "1510"
    uid: b7fd7ee0-aa59-4627-936a-f72f3bca6b05
  spec:
    chart:
      metadata:
        apiVersion: v1
        appVersion: v3.20.2
        description: Install Canal Network Plugin.
        home: https://www.projectcalico.org/
        keywords:
        - canal
        maintainers:
        - email: charts@rancher.com
          name: Rancher Labs
        name: rke2-canal
        sources:
        - https://github.com/rancher/rke2-charts
        version: v3.20.1-build2021111904
      values:
        calico:
          clusterType: k8s,canal
          cniImage:
            repository: rancher/hardened-calico
            tag: v3.20.2-build20211119
          datastoreType: kubernetes
          disableFileLogging: true
          felixDefaultEndpointToHostAction: ACCEPT
          felixFailsafeInboundHostPorts: "null"
          felixFailsafeOutboundHostPorts: "null"
          felixHealthEnabled: true
          felixIptablesBackend: auto
          felixIptablesRefreshInterval: 60
          felixIpv6Support: false
          felixLogSeverityScreen: info
          felixPrometheusMetricsEnabled: true
          felixXDPEnabled: false
          flexVolumePluginDir: /var/lib/kubelet/volumeplugins
          flexvolImage:
            repository: rancher/hardened-calico
            tag: v3.20.2-build20211119
          masquerade: true
          networkingBackend: none
          nodeImage:
            repository: rancher/hardened-calico
            tag: v3.20.2-build20211119
          typhaServiceName: none
          usePodCIDR: true
          vethuMTU: 1450
          waitForDatastore: true
        flannel:
          args:
          - --ip-masq
          - --kube-subnet-mgr
          backend: vxlan
          iface: "null"
          image:
            repository: rancher/hardened-flannel
            tag: v0.15.1-build20211119
        global:
          clusterCIDRv4: "null"
          clusterCIDRv6: "null"
          systemDefaultRegistry: "null"
        podCidr: 10.42.0.0/16
    helmVersion: 3
    info:
      description: Install complete
      firstDeployed: "2022-03-06T22:47:18Z"
      lastDeployed: "2022-03-06T22:47:18Z"
      notes: |
        Canal network plugin has been installed.

        NOTE: It may take few minutes until Canal image install CNI files and node become in ready state.
      status: deployed
    name: rke2-canal
    namespace: kube-system
    resources:
    - apiVersion: v1
      kind: ServiceAccount
      name: canal
      namespace: kube-system
    - apiVersion: v1
      kind: ConfigMap
      name: rke2-canal-config
      namespace: kube-system
    - apiVersion: apiextensions.k8s.io/v1
      kind: CustomResourceDefinition
      name: bgpconfigurations.crd.projectcalico.org
    - apiVersion: apiextensions.k8s.io/v1
      kind: CustomResourceDefinition
      name: bgppeers.crd.projectcalico.org
    - apiVersion: apiextensions.k8s.io/v1
      kind: CustomResourceDefinition
      name: blockaffinities.crd.projectcalico.org
    - apiVersion: apiextensions.k8s.io/v1
      kind: CustomResourceDefinition
      name: clusterinformations.crd.projectcalico.org
    - apiVersion: apiextensions.k8s.io/v1
      kind: CustomResourceDefinition
      name: felixconfigurations.crd.projectcalico.org
    - apiVersion: apiextensions.k8s.io/v1
      kind: CustomResourceDefinition
      name: globalnetworkpolicies.crd.projectcalico.org
    - apiVersion: apiextensions.k8s.io/v1
      kind: CustomResourceDefinition
      name: globalnetworksets.crd.projectcalico.org
    - apiVersion: apiextensions.k8s.io/v1
      kind: CustomResourceDefinition
      name: hostendpoints.crd.projectcalico.org
    - apiVersion: apiextensions.k8s.io/v1
      kind: CustomResourceDefinition
      name: ipamblocks.crd.projectcalico.org
    - apiVersion: apiextensions.k8s.io/v1
      kind: CustomResourceDefinition
      name: ipamconfigs.crd.projectcalico.org
    - apiVersion: apiextensions.k8s.io/v1
      kind: CustomResourceDefinition
      name: ipamhandles.crd.projectcalico.org
    - apiVersion: apiextensions.k8s.io/v1
      kind: CustomResourceDefinition
      name: ippools.crd.projectcalico.org
    - apiVersion: apiextensions.k8s.io/v1
      kind: CustomResourceDefinition
      name: networkpolicies.crd.projectcalico.org
    - apiVersion: apiextensions.k8s.io/v1
      kind: CustomResourceDefinition
      name: networksets.crd.projectcalico.org
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      name: calico-node
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      name: flannel
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      name: canal-flannel
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      name: canal-calico
    - apiVersion: apps/v1
      kind: DaemonSet
      name: rke2-canal
      namespace: kube-system
    values:
      flannel:
        iface: harvester-mgmt
      global:
        clusterCIDR: 10.52.0.0/16
        clusterCIDRv4: 10.52.0.0/16
        clusterCIDRv6: "null"
        clusterDNS: 10.53.0.10
        clusterDomain: cluster.local
        rke2DataDir: /var/lib/rancher/rke2
        serviceCIDR: 10.53.0.0/16
        systemDefaultRegistry: "null"
    version: 1
  status:
    observedGeneration: 1
    summary:
      state: deployed
- apiVersion: catalog.cattle.io/v1
  kind: App
  metadata:
    annotations:
      objectset.rio.cattle.io/applied: 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
      objectset.rio.cattle.io/id: helm-app
      objectset.rio.cattle.io/owner-gvk: /v1, Kind=Secret
      objectset.rio.cattle.io/owner-name: sh.helm.release.v1.rke2-coredns.v1
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2022-03-06T22:48:49Z"
    generation: 1
    labels:
      objectset.rio.cattle.io/hash: 1b99a7a962e96e395dc94b8fb6802474536e9e8f
    managedFields:
    - apiVersion: catalog.cattle.io/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:objectset.rio.cattle.io/applied: {}
            f:objectset.rio.cattle.io/id: {}
            f:objectset.rio.cattle.io/owner-gvk: {}
            f:objectset.rio.cattle.io/owner-name: {}
            f:objectset.rio.cattle.io/owner-namespace: {}
          f:labels:
            .: {}
            f:objectset.rio.cattle.io/hash: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"8a7dc319-53d4-425c-9f4e-60594d22f171"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:spec:
          .: {}
          f:chart:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:artifacthub.io/changes: {}
              f:apiVersion: {}
              f:appVersion: {}
              f:description: {}
              f:home: {}
              f:icon: {}
              f:keywords: {}
              f:maintainers: {}
              f:name: {}
              f:sources: {}
              f:type: {}
              f:version: {}
            f:values:
              .: {}
              f:affinity: {}
              f:autoscaler: {}
              f:customLabels: {}
              f:deployment: {}
              f:extraConfig: {}
              f:extraSecrets: {}
              f:extraVolumeMounts: {}
              f:extraVolumes: {}
              f:global: {}
              f:hpa: {}
              f:image: {}
              f:isClusterService: {}
              f:k8sApp: {}
              f:livenessProbe: {}
              f:nodeSelector: {}
              f:nodelocal: {}
              f:podAnnotations: {}
              f:podDisruptionBudget: {}
              f:priorityClassName: {}
              f:prometheus: {}
              f:rbac: {}
              f:readinessProbe: {}
              f:replicaCount: {}
              f:resources: {}
              f:rollingUpdate: {}
              f:servers: {}
              f:service: {}
              f:serviceAccount: {}
              f:serviceType: {}
              f:terminationGracePeriodSeconds: {}
              f:tolerations: {}
              f:zoneFiles: {}
          f:helmVersion: {}
          f:info:
            .: {}
            f:description: {}
            f:firstDeployed: {}
            f:lastDeployed: {}
            f:notes: {}
            f:readme: {}
            f:status: {}
          f:name: {}
          f:namespace: {}
          f:resources: {}
          f:values:
            .: {}
            f:global: {}
          f:version: {}
        f:status:
          .: {}
          f:observedGeneration: {}
          f:summary:
            .: {}
            f:state: {}
      manager: rancher
      operation: Update
      time: "2022-03-06T22:48:50Z"
    name: rke2-coredns
    namespace: kube-system
    ownerReferences:
    - apiVersion: v1
      blockOwnerDeletion: false
      controller: true
      kind: Secret
      name: sh.helm.release.v1.rke2-coredns.v1
      uid: 8a7dc319-53d4-425c-9f4e-60594d22f171
    resourceVersion: "1511"
    uid: f982bc16-ff70-42cc-81e7-6bcafb7a26ab
  spec:
    chart:
      metadata:
        annotations:
          artifacthub.io/changes: |
            - Initial helm chart changelog
        apiVersion: v2
        appVersion: 1.8.5
        description: CoreDNS is a DNS server that chains plugins and provides Kubernetes
          DNS Services
        home: https://coredns.io
        icon: https://coredns.io/images/CoreDNS_Colour_Horizontal.png
        keywords:
        - coredns
        - dns
        - kubedns
        maintainers:
        - name: mrueg
        - name: haad
        name: rke2-coredns
        sources:
        - https://github.com/coredns/coredns
        type: application
        version: 1.16.401-build2021111901
      values:
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: k8s-app
                  operator: In
                  values:
                  - kube-dns
              topologyKey: kubernetes.io/hostname
        autoscaler:
          affinity: {}
          configmap:
            annotations: {}
          coresPerReplica: 256
          enabled: true
          image:
            pullPolicy: IfNotPresent
            repository: rancher/hardened-cluster-autoscaler
            tag: v1.8.5-build20211119
          includeUnschedulableNodes: false
          livenessProbe:
            enabled: true
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          max: 0
          min: 0
          nodeSelector:
            kubernetes.io/os: linux
          nodesPerReplica: 16
          preventSinglePointFailure: true
          priorityClassName: "null"
          resources:
            limits:
              cpu: 20m
              memory: 20Mi
            requests:
              cpu: 20m
              memory: 20Mi
          tolerations:
          - effect: NoSchedule
            key: node-role.kubernetes.io/control-plane
            operator: Exists
          - effect: NoExecute
            key: node-role.kubernetes.io/etcd
            operator: Exists
        customLabels: {}
        deployment:
          enabled: true
          name: "null"
        extraConfig: {}
        extraSecrets: "null"
        extraVolumeMounts: "null"
        extraVolumes: "null"
        global:
          systemDefaultRegistry: "null"
        hpa:
          enabled: false
          maxReplicas: 2
          metrics: {}
          minReplicas: 1
        image:
          pullPolicy: IfNotPresent
          repository: rancher/hardened-coredns
          tag: v1.8.5-build20211119
        isClusterService: true
        k8sApp: kube-dns
        livenessProbe:
          enabled: true
          failureThreshold: 5
          initialDelaySeconds: 60
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        nodeSelector:
          kubernetes.io/os: linux
        nodelocal:
          enabled: false
          image:
            repository: rancher/hardened-dns-node-cache
            tag: 1.21.2-build20211119
          initimage:
            repository: rancher/hardened-dns-node-cache
            tag: 1.21.2-build20211119
          ip_address: 169.254.20.10
          ipvs: false
          nodeSelector:
            kubernetes.io/os: linux
        podAnnotations: {}
        podDisruptionBudget: {}
        priorityClassName: system-cluster-critical
        prometheus:
          monitor:
            additionalLabels: {}
            enabled: false
            namespace: "null"
          service:
            annotations:
              prometheus.io/port: "9153"
              prometheus.io/scrape: "true"
            enabled: false
        rbac:
          create: true
          pspEnable: false
        readinessProbe:
          enabled: true
          failureThreshold: 5
          initialDelaySeconds: 30
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        replicaCount: 1
        resources:
          limits:
            cpu: 100m
            memory: 128Mi
          requests:
            cpu: 100m
            memory: 128Mi
        rollingUpdate:
          maxSurge: 25%
          maxUnavailable: 1
        servers:
        - plugins:
          - name: errors
          - configBlock: lameduck 5s
            name: health
          - name: ready
          - configBlock: |-
              pods insecure
              fallthrough in-addr.arpa ip6.arpa
              ttl 30
            name: kubernetes
            parameters: cluster.local in-addr.arpa ip6.arpa
          - name: prometheus
            parameters: 0.0.0.0:9153
          - name: forward
            parameters: . /etc/resolv.conf
          - name: cache
            parameters: 30
          - name: loop
          - name: reload
          - name: loadbalance
          port: 53
          zones:
          - zone: .
        service:
          annotations: {}
          name: "null"
        serviceAccount:
          annotations: {}
          create: true
          name: coredns
        serviceType: ClusterIP
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoExecute
          key: node-role.kubernetes.io/etcd
          operator: Exists
        zoneFiles: null
    helmVersion: 3
    info:
      description: Install complete
      firstDeployed: "2022-03-06T22:47:18Z"
      lastDeployed: "2022-03-06T22:47:18Z"
      notes: |2

        CoreDNS is now running in the cluster as a cluster-service.

        It can be tested with the following:

        1. Launch a Pod with DNS tools:

        kubectl run -it --rm --restart=Never --image=infoblox/dnstools:latest dnstools

        2. Query the DNS server:

        / # host kubernetes
      readme: "# CoreDNS\n\n[CoreDNS](https://coredns.io/) is a DNS server that chains
        plugins and provides DNS Services\n\n# TL;DR;\n\n```console\n$ helm repo add
        coredns https://coredns.github.io/helm\n$ helm --namespace=kube-system install
        coredns coredns/coredns\n```\n\n## Introduction\n\nThis chart bootstraps a
        [CoreDNS](https://github.com/coredns/coredns) deployment on a [Kubernetes](http://kubernetes.io)
        cluster using the [Helm](https://helm.sh) package manager. This chart will
        provide DNS Services and can be deployed in multiple configuration to support
        various scenarios listed below:\n\n - CoreDNS as a cluster dns service and
        a drop-in replacement for Kube/SkyDNS. This is the default mode and CoreDNS
        is deployed as cluster-service in kube-system namespace. This mode is chosen
        by setting `isClusterService` to true.\n - CoreDNS as an external dns service.
        In this mode CoreDNS is deployed as any kubernetes app in user specified namespace.
        The CoreDNS service can be exposed outside the cluster by using using either
        the NodePort or LoadBalancer type of service. This mode is chosen by setting
        `isClusterService` to false.\n - CoreDNS as an external dns provider for kubernetes
        federation. This is a sub case of 'external dns service' which uses etcd plugin
        for CoreDNS backend. This deployment mode as a dependency on `etcd-operator`
        chart, which needs to be pre-installed.\n\n## Prerequisites\n\n-\tKubernetes
        1.10 or later\n\n## Installing the Chart\n\nThe chart can be installed as
        follows:\n\n```console\n$ helm repo add coredns https://coredns.github.io/helm\n$
        helm --namespace=kube-system install coredns coredns/coredns\n```\n\nThe command
        deploys CoreDNS on the Kubernetes cluster in the default configuration. The
        [configuration](#configuration) section lists various ways to override default
        configuration during deployment.\n\n> **Tip**: List all releases using `helm
        list --all-namespaces`\n\n## Uninstalling the Chart\n\nTo uninstall/delete
        the `coredns` deployment:\n\n```console\n$ helm uninstall coredns\n```\n\nThe
        command removes all the Kubernetes components associated with the chart and
        deletes the release.\n\n## Configuration\n\n| Parameter                                       |
        Description                                                                                                                               |
        Default                                                     |\n|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------|\n|
        `image.repository`                              | The image repository to
        pull from                                                                                                         |
        coredns/coredns                                             |\n| `image.tag`
        \                                    | The image tag to pull from                                                                                                                |
        `1.8.4`                                                     |\n| `image.pullPolicy`
        \                             | Image pull policy                                                                                                                         |
        IfNotPresent                                                |\n| `image.pullSecrets`
        \                            | Specify container image pull secrets                                                                                                      |
        `[]`                                                        |\n| `replicaCount`
        \                                 | Number of replicas                                                                                                                        |
        1                                                           |\n| `resources.limits.cpu`
        \                         | Container maximum CPU                                                                                                                     |
        `100m`                                                      |\n| `resources.limits.memory`
        \                      | Container maximum memory                                                                                                                  |
        `128Mi`                                                     |\n| `resources.requests.cpu`
        \                       | Container requested CPU                                                                                                                   |
        `100m`                                                      |\n| `resources.requests.memory`
        \                    | Container requested memory                                                                                                                |
        `128Mi`                                                     |\n| `serviceType`
        \                                  | Kubernetes Service type                                                                                                                   |
        `ClusterIP`                                                 |\n| `prometheus.service.enabled`
        \                   | Set this to `true` to create Service for Prometheus
        metrics                                                                               |
        `false`                                                     |\n| `prometheus.service.annotations`
        \               | Annotations to add to the metrics Service                                                                                                 |
        `{prometheus.io/scrape: \"true\", prometheus.io/port: \"9153\"}`|\n| `prometheus.monitor.enabled`
        \                   | Set this to `true` to create ServiceMonitor for Prometheus
        operator                                                                       |
        `false`                                                     |\n| `prometheus.monitor.additionalLabels`
        \          | Additional labels that can be used so ServiceMonitor will be
        discovered by Prometheus                                                     |
        {}                                                          |\n| `prometheus.monitor.namespace`
        \                 | Selector to select which namespaces the Endpoints objects
        are discovered from.                                                            |
        `\"\"`                                                        |\n| `service.clusterIP`
        \                            | IP address to assign to service                                                                                                           |
        `\"\"`                                                        |\n| `service.loadBalancerIP`
        \                       | IP address to assign to load balancer (if supported)
        \                                                                                     |
        `\"\"`                                                        |\n| `service.externalIPs`
        \                          | External IP addresses                                                                                                                     |
        []                                                          |\n| `service.externalTrafficPolicy`
        \                | Enable client source IP preservation                                                                                                      |
        []                                                          |\n| `service.annotations`
        \                          | Annotations to add to service                                                                                                             |
        {}                                                          |\n| `serviceAccount.create`
        \                        | If true, create & use serviceAccount                                                                                                      |
        false                                                       |\n| `serviceAccount.name`
        \                          | If not set & create is true, use template fullname
        \                                                                                       |
        \                                                            |\n| `rbac.create`
        \                                  | If true, create & use RBAC resources
        \                                                                                                     |
        true                                                        |\n| `rbac.pspEnable`
        \                               | Specifies whether a PodSecurityPolicy should
        be created.                                                                                  |
        `false`                                                     |\n| `isClusterService`
        \                             | Specifies whether chart should be deployed
        as cluster-service or normal k8s app.                                                          |
        true                                                        |\n| `priorityClassName`
        \                            | Name of Priority Class to assign pods                                                                                                     |
        `\"\"`                                                        |\n| `servers`
        \                                      | Configuration for CoreDNS and plugins
        \                                                                                                    |
        See values.yml                                              |\n| `livenessProbe.enabled`
        \                        | Enable/disable the Liveness probe                                                                                                         |
        `true`                                                      |\n| `livenessProbe.initialDelaySeconds`
        \            | Delay before liveness probe is initiated                                                                                                  |
        `60`                                                        |\n| `livenessProbe.periodSeconds`
        \                  | How often to perform the probe                                                                                                            |
        `10`                                                        |\n| `livenessProbe.timeoutSeconds`
        \                 | When the probe times out                                                                                                                  |
        `5`                                                         |\n| `livenessProbe.failureThreshold`
        \               | Minimum consecutive failures for the probe to be considered
        failed after having succeeded.                                                |
        `5`                                                         |\n| `livenessProbe.successThreshold`
        \               | Minimum consecutive successes for the probe to be considered
        successful after having failed.                                              |
        `1`                                                         |\n| `readinessProbe.enabled`
        \                       | Enable/disable the Readiness probe                                                                                                        |
        `true`                                                      |\n| `readinessProbe.initialDelaySeconds`
        \           | Delay before readiness probe is initiated                                                                                                 |
        `30`                                                        |\n| `readinessProbe.periodSeconds`
        \                 | How often to perform the probe                                                                                                            |
        `10`                                                        |\n| `readinessProbe.timeoutSeconds`
        \                | When the probe times out                                                                                                                  |
        `5`                                                         |\n| `readinessProbe.failureThreshold`
        \              | Minimum consecutive failures for the probe to be considered
        failed after having succeeded.                                                |
        `5`                                                         |\n| `readinessProbe.successThreshold`
        \              | Minimum consecutive successes for the probe to be considered
        successful after having failed.                                              |
        `1`                                                         |\n| `affinity`
        \                                     | Affinity settings for pod assignment
        \                                                                                                     |
        {}                                                          |\n| `nodeSelector`
        \                                 | Node labels for pod assignment                                                                                                            |
        {}                                                          |\n| `tolerations`
        \                                  | Tolerations for pod assignment                                                                                                            |
        []                                                          |\n| `zoneFiles`
        \                                    | Configure custom Zone files                                                                                                               |
        []                                                          |\n| `extraVolumes`
        \                                 | Optional array of volumes to create                                                                                                       |
        []                                                          |\n| `extraVolumeMounts`
        \                            | Optional array of volumes to mount inside the
        CoreDNS container                                                                           |
        []                                                          |\n| `extraSecrets`
        \                                 | Optional array of secrets to mount inside
        the CoreDNS container                                                                           |
        []                                                          |\n| `customLabels`
        \                                 | Optional labels for Deployment(s), Pod,
        Service, ServiceMonitor objects                                                                   |
        {}                                                          |\n| `rollingUpdate.maxUnavailable`
        \                 | Maximum number of unavailable replicas during rolling
        update                                                                              |
        `1`                                                         |\n| `rollingUpdate.maxSurge`
        \                       | Maximum number of pods created above desired number
        of pods                                                                               |
        `25%`                                                       |\n| `podDisruptionBudget`
        \                          | Optional PodDisruptionBudget                                                                                                              |
        {}                                                          |\n| `podAnnotations`
        \                               | Optional Pod only Annotations                                                                                                             |
        {}                                                          |\n| `terminationGracePeriodSeconds`
        \                | Optional duration in seconds the pod needs to terminate
        gracefully.                                                                       |
        30                                                          |\n| `preStopSleep`
        \                                 | Definition of Kubernetes preStop hook
        executed before Pod termination                                                                     |
        {}                                                          |\n| `hpa.enabled`
        \                                  | Enable Hpa autoscaler instead of proportional
        one                                                                                         |
        `false`                                                     |\n| `hpa.minReplicas`
        \                              | Hpa minimum number of CoreDNS replicas                                                                                                    |
        `1`                                                         |\n| `hpa.maxReplicas`
        \                              | Hpa maximum number of CoreDNS replicas                                                                                                    |
        `2`                                                         |\n| `hpa.metrics`
        \                                  | Metrics definitions used by Hpa to scale
        up and down                                                                                      |
        {}                                                          |\n| `autoscaler.enabled`
        \                           | Optionally enabled a cluster-proportional-autoscaler
        for CoreDNS                                                                          |
        `false`                                                     |\n| `autoscaler.coresPerReplica`
        \                   | Number of cores in the cluster per CoreDNS replica                                                                                        |
        `256`                                                       |\n| `autoscaler.nodesPerReplica`
        \                   | Number of nodes in the cluster per CoreDNS replica                                                                                        |
        `16`                                                        |\n| `autoscaler.min`
        \                               | Min size of replicaCount                                                                                                                  |
        0                                                           |\n| `autoscaler.max`
        \                               | Max size of replicaCount                                                                                                                  |
        0  (aka no max)                                             |\n| `autoscaler.includeUnschedulableNodes`
        \         | Should the replicas scale based on the total number or only schedulable
        nodes                                                             | `false`
        \                                                    |\n| `autoscaler.preventSinglePointFailure`
        \         | If true does not allow single points of failure to form                                                                                   |
        `true`                                                      |\n| `autoscaler.image.repository`
        \                  | The image repository to pull autoscaler from                                                                                              |
        k8s.gcr.io/cluster-proportional-autoscaler-amd64            |\n| `autoscaler.image.tag`
        \                         | The image tag to pull autoscaler from                                                                                                     |
        `1.8.1`                                                     |\n| `autoscaler.image.pullPolicy`
        \                  | Image pull policy for the autoscaler                                                                                                      |
        IfNotPresent                                                |\n| `autoscaler.image.pullSecrets`
        \                 | Specify container image pull secrets                                                                                                      |
        `[]`                                                        |\n| `autoscaler.priorityClassName`
        \                 | Optional priority class for the autoscaler pod. `priorityClassName`
        used if not set.                                                      | `\"\"`
        \                                                       |\n| `autoscaler.affinity`
        \                          | Affinity settings for pod assignment for autoscaler
        \                                                                                      |
        {}                                                          |\n| `autoscaler.nodeSelector`
        \                      | Node labels for pod assignment for autoscaler                                                                                             |
        {}                                                          |\n| `autoscaler.tolerations`
        \                       | Tolerations for pod assignment for autoscaler                                                                                             |
        []                                                          |\n| `autoscaler.resources.limits.cpu`
        \              | Container maximum CPU for cluster-proportional-autoscaler
        \                                                                                |
        `20m`                                                       |\n| `autoscaler.resources.limits.memory`
        \           | Container maximum memory for cluster-proportional-autoscaler
        \                                                                             |
        `10Mi`                                                      |\n| `autoscaler.resources.requests.cpu`
        \            | Container requested CPU for cluster-proportional-autoscaler
        \                                                                              |
        `20m`                                                       |\n| `autoscaler.resources.requests.memory`
        \         | Container requested memory for cluster-proportional-autoscaler
        \                                                                           |
        `10Mi`                                                      |\n| `autoscaler.configmap.annotations`
        \             | Annotations to add to autoscaler config map. For example to
        stop CI renaming them                                                         |
        {}                                                          |\n| `autoscaler.livenessProbe.enabled`
        \             | Enable/disable the Liveness probe                                                                                                         |
        `true`                                                      |\n| `autoscaler.livenessProbe.initialDelaySeconds`
        \ | Delay before liveness probe is initiated                                                                                                  |
        `10`                                                        |\n| `autoscaler.livenessProbe.periodSeconds`
        \       | How often to perform the probe                                                                                                            |
        `5`                                                         |\n| `autoscaler.livenessProbe.timeoutSeconds`
        \      | When the probe times out                                                                                                                  |
        `5`                                                         |\n| `autoscaler.livenessProbe.failureThreshold`
        \    | Minimum consecutive failures for the probe to be considered failed
        after having succeeded.                                                | `3`
        \                                                        |\n| `autoscaler.livenessProbe.successThreshold`
        \    | Minimum consecutive successes for the probe to be considered successful
        after having failed.                                              | `1`                                                         |\n|
        `deployment.enabled`                            | Optionally disable the main
        deployment and its respective resources.                                                                      |
        `true`                                                      |\n| `deployment.name`
        \                              | Name of the deployment if `deployment.enabled`
        is true. Otherwise the name of an existing deployment for the autoscaler or
        HPA to target. | `\"\"`                                                        |\n\nSee
        `values.yaml` for configuration notes. Specify each parameter using the `--set
        key=value[,key=value]` argument to `helm install`. For example,\n\n```console\n$
        helm install coredns \\\n  coredns/coredns \\\n  --set rbac.create=false\n```\n\nThe
        above command disables automatic creation of RBAC rules.\n\nAlternatively,
        a YAML file that specifies the values for the above parameters can be provided
        while installing the chart. For example,\n\n```console\n$ helm install coredns
        coredns/coredns -f values.yaml\n```\n\n> **Tip**: You can use the default
        [values.yaml](values.yaml)\n\n\n## Caveats\n\nThe chart will automatically
        determine which protocols to listen on based on\nthe protocols you define
        in your zones. This means that you could potentially\nuse both \"TCP\" and
        \"UDP\" on a single port.\nSome cloud environments like \"GCE\" or \"Azure
        container service\" cannot\ncreate external loadbalancers with both \"TCP\"
        and \"UDP\" protocols. So\nWhen deploying CoreDNS with `serviceType=\"LoadBalancer\"`
        on such cloud\nenvironments, make sure you do not attempt to use both protocols
        at the same\ntime.\n\n## Autoscaling\n\nBy setting `autoscaler.enabled = true`
        a\n[cluster-proportional-autoscaler](https://github.com/kubernetes-incubator/cluster-proportional-autoscaler)\nwill
        be deployed. This will default to a coredns replica for every 256 cores, or\n16
        nodes in the cluster. These can be changed with `autoscaler.coresPerReplica`\nand
        `autoscaler.nodesPerReplica`. When cluster is using large nodes (with more\ncores),
        `coresPerReplica` should dominate. If using small nodes,\n`nodesPerReplica`
        should dominate.\n\nThis also creates a ServiceAccount, ClusterRole, and ClusterRoleBinding
        for\nthe autoscaler deployment.\n\n`replicaCount` is ignored if this is enabled.\n\nBy
        setting `hpa.enabled = true` a [Horizontal Pod Autoscaler](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)\nis
        enabled for Coredns deployment. This can scale number of replicas based on
        meitrics\nlike CpuUtilization, MemoryUtilization or Custom ones.\n"
      status: deployed
    name: rke2-coredns
    namespace: kube-system
    resources:
    - apiVersion: v1
      kind: ServiceAccount
      name: rke2-coredns-rke2-coredns-autoscaler
      namespace: kube-system
    - apiVersion: v1
      kind: ServiceAccount
      name: coredns
      namespace: kube-system
    - apiVersion: v1
      kind: ConfigMap
      name: rke2-coredns-rke2-coredns-autoscaler
      namespace: kube-system
    - apiVersion: v1
      kind: ConfigMap
      name: rke2-coredns-rke2-coredns
      namespace: kube-system
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      name: rke2-coredns-rke2-coredns-autoscaler
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      name: rke2-coredns-rke2-coredns
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      name: rke2-coredns-rke2-coredns-autoscaler
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      name: rke2-coredns-rke2-coredns
    - apiVersion: v1
      kind: Service
      name: rke2-coredns-rke2-coredns
      namespace: kube-system
    - apiVersion: apps/v1
      kind: Deployment
      name: rke2-coredns-rke2-coredns-autoscaler
      namespace: kube-system
    - apiVersion: apps/v1
      kind: Deployment
      name: rke2-coredns-rke2-coredns
      namespace: kube-system
    values:
      global:
        clusterCIDR: 10.52.0.0/16
        clusterCIDRv4: 10.52.0.0/16
        clusterCIDRv6: "null"
        clusterDNS: 10.53.0.10
        clusterDomain: cluster.local
        rke2DataDir: /var/lib/rancher/rke2
        serviceCIDR: 10.53.0.0/16
        systemDefaultRegistry: "null"
    version: 1
  status:
    observedGeneration: 1
    summary:
      state: deployed
- apiVersion: catalog.cattle.io/v1
  kind: App
  metadata:
    annotations:
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/8xaW3PbNhb+Kxrsyz6QFCVZjqOZPjh2m3ibOBrbSXfaeHYg4IjECgSwAChb9ei/7wAgKVI3222zs08WcTk4l+/cAD+hAiym2GI0eUJYCGmxZVIY9yln/wZiDdhEM5kQbC2HhMk+o2iCcuBFjJVC0cF18kGAjrPlAk1QfzmIej8zQX+4BaLBPrtN4ALQBJk8cSclGjhgA8lykOgFDGMmMg3GxCJj4jFZjl5EzyhMHNFFOYPYrIyFAq0jRDR4oe9YAcbiQqGJKDmPEMcz4EdVkWOTowlK8fCMDgZDMkrhLU3J2fzN6Smks5P5W3xG6JwOTslg/iZ1p1WC7UqBwtw+JiPkhbiBOWgQBAya/PaEsGJfQRsmBZqg5QBFaMYlWXx2Sy+Bg/Uzc8wNRIhIYbXkHDSaWF1ChBZMOEM29ni9xkuPhAGk6Ww2pPFsRN7GJ/D2bYzHMIrJm/HZ4PTkjJ6Ohmh9v46QUUCcNkmOtXU/joAPa8vmmNi8nDlFkxyLzMmN4t4XlWlMocdLHGswdhXPMMeCgO5Z2VumSZqcuF/GStUjUgMtC2W+ibh3TmmPoGibuNJQSYsmyOvLGaqr3qHbptRmZJCkiRukYIhmKigbXQUt9Tbq7s2l7v1czkALsGB6pWEi612/v7r+Zw+bHu5pWII20FNaPq5QhHLp7ZBbq8yk38+YZ5PIor9oqPS3kcOIP77eVCouMU0e2IIVQBlOpM767ku5rz6RRSGF6du8LGZ90ifj/rUj9C8UoQWsHqSmDmKoOsWBw59zH3lYbpTwrUzTEfwwSAZvkzROUYQKzITFTIAOIK1QdZEzmJ9zeMSCgkbr++OOYGSpA8xfpYf7CNmVclSxUpwRDycUoWXD8EmSJqP01Nl3iXkJHmtt33hCmNIPgKkX4MnhgBbMuO2/wCyXcmF20eqCCDhMuSPd8f3S6D6XBPN+a8L4D1RFHAie9w85cwQ1NDI/rdcRAoFnHGjtrPDIjGUimxqFJghFaI4ZLzVMJWdkhSboJ8x4sN6R491sK8rcAgdipQ4ShBjXHVPYktyxt8UOK3AGblyVnDc8XM2vpZ1qMCCclBqUNMxK7eY0FiQH3S+Y1s4luyaPfbR7CAqOHc8ZOMNZnPnYloTQKWmbZ7TBgY/zLjpwJspHt1ZJer5tI6WZ1MyuLjg25jrAz/FZinPzxTjrD9M0jZCVHHS90aWCWhMVAg6YTEkX1c5OTkYRMqCXjMBeqMCjBS0wv5qaJtNITN9VQezWE72p4l2Yr8hN/Qn+gArnF7w0FvTVFDkO8HzOBLOrCricy4dbwZQC29FFlRJ2GMOllYZgzkTmGJ9BjpeshkKDgGp3gR9vwDuZQZPBIEIFE60BZzudgb2YfvliGWe/+4OmoAkI68EzTus1n6CQenVwWZezOygU914WVEOkmLOsckL/e8fuYfgTVtet/IpCTvSh6vpQKIpboaG1PtjhyccmNDlLoxClvGnceaWxstgw+rTh4Gfvn2iLJze0jhAV5qIlDRWmca3Kzj8xbewvzOYfpLHXPm17f2BSXF2iSR2wW2zH3McyVBvwEyscLsjBwAKPVuNznW3AqvGF3MT0oHU//KNYdgeuBLMH1n6VvCzgkyyF3TfRjOWAuc0vciALJ2PgqTU4xdZVXP0w9LvPl14VD1IvaqHcUG2jrcjlnNQctN067A2pyYnfBDrvTFPNloxDBj86MIbsUtN9bSAM+No1WDcgDdJBHQXDBl92xDnWFATQoYNNRcNHtXergKfKSdtTN1W46ma8ry4LunLzzAfR7URMYY5LbhuCW9qscni9WmGNC7BV6ly7dERDcdeupdxgYvL+coC5yrEvXDuhhkjJqXxwYYBJiiYjF5RfHX+U5C5eXAkLeom5I+NMYazUcCc/a5YxgXm15cIBs6HtrAv0s0+Ie0L4OkJWsyxrML5uqulLDIUUt94zm/ZhHSHO5kBWhIekqeHWSuXR+ViVxLIosKPwG+o/YGZjk5fW6QDdr9d+/xIEGDPVcuZpVPn/LtdgcskpmowDkt+DZ1nt+ElIT4N0OD5xEubgTffh7m4aQMQsw/wSOF7dApHCVYCDNELKG6EzZEpCwJjW2Q6krABZ2s3CtbdRwQT9yAgIA03sK8BqRkzHOyvNd5hUWhZgcyjNTRk0hyllzgyYf2wpd5uILjlsLPM9U3HF6G4ybtZ9koJV5crLmA/KuQGPHiay5mDRTV4HqjhDNFawAT0apQbVibY+12tm7V3mfIkZdxx4I3ofwnSDgLTx8U6E+tOVmKRtHSgPsNLXZlJYeLRHCzbfKd2C7ZTpqpxxZvLbjbV3sIVt/nkJWjNaZ1wNmLJn/Gr0/+dXuhO0BiHDaHstabC8Q7TauEOrTkUa/lOCCSmQqNI1smlaeKcsQoJ6m35ivpo0RKq9iuwg8TkP83s/+GRbVRzNiOkObQ7Y45askuvIIZv9x314vRWan3F4h/Vpt2rwNUkQwP20RAVSJVWtXuBwmVH74zZZ/2dDOvxdN/HlY4tTr/eVIbYWIzCxpzHdW/y6DaALJvzS9xoTmHYBOdrbCbkhJbnMVrfKOc+FFMZq1/M3C4ISXsFIqajrhq3GFrKqd3lw7VZ1meLqXVlaHwQCYtZNYfIOkwW4vPm03fx0e5lnIdMpJ4bft5vZhfsLavCdYvulFfULStg6QB6tYTH9LPjqRkr7E+NQ3UxWDvyqCjeubDerbFcVvNdSOOINyU0RfDoej07qMniQjOPqhAF6WWU0OhCGR38iDI/3pc8qVe7I979Kl+ECInXR5tBdxwsS3ukBde1qa/wKZe3JWa3rk+9bpjmFHKnRzgkJXD35sFG4j9vO1J1cQNPqhdu7rRbIR9Q990Y0n7qmqGYr43IWklhwoMsAlRvImLHee1DddU5LzsMVYSNUy/C1n25nZ6fpGQ49RYfPKps3i2DJXD/2gbmGaPWRFcxW8P+LlbKdHF2PDbxo+kHvnXPpDurepdf3/EQWioMFFKE508ZeguJy5SRGw3Q4jNNRnJ7eDYeTkzeT8fhX33kdWTUeTU5Gv3qvtP5F4S6HXqfvbd/e59j0ZgCix4SxmHOgyTdxZXsFXvUsXkAP9+aVX3mR/9Yl9U18E791Ru7//syNNoqQsdiWxkeSSog//HDU8rK9T0bNK1DH6H/stHV05ISLOv0fI969B3nxOQ7xCS5tLnWVc5PqPqPDQXD9G8nhqIB/nvw7JqgrPL7DKc9y/5er7aUC/UFc1A3by1ARN68xrzkPK2U6MrWvaf5qMIpwF8lEtk+bV60bObR1gbbLdy2sDvnhoI2+Ys4odjVk9UQRvK3U9QvYYSE3Cl3fH3sU2/MI1jx4nKxb9/ChPY9nkq5iw353x6Z10x5X7Wc8K+dz0L44R3I+R+tu0VuXULExPO4+rYXH90r7fctNXQ3GTYu49zZgq/tvmbC59IRHJQ343neTp0lw64uryxvfLCfjYZImaX9wiqL25PLk+PRpde8fhi6vb6vloyRNBmlrRhaYOeNX34l/x3ORfAHDS2zxJdP+Hyqw7nM269fVtptGTfZusTva8HO45li3Hkldf1pnoCckZ44k0PcgquLGV4KmLAqsVyGj/zcAAP//VGzPKkciAAA
      objectset.rio.cattle.io/id: helm-app
      objectset.rio.cattle.io/owner-gvk: /v1, Kind=Secret
      objectset.rio.cattle.io/owner-name: sh.helm.release.v1.rke2-ingress-nginx.v3
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2022-03-06T22:48:49Z"
    generation: 4
    labels:
      objectset.rio.cattle.io/hash: 0a28d112c30e9d0c8f766e0b4f9a8cdfd16c1f70
    managedFields:
    - apiVersion: catalog.cattle.io/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:objectset.rio.cattle.io/applied: {}
            f:objectset.rio.cattle.io/id: {}
            f:objectset.rio.cattle.io/owner-gvk: {}
            f:objectset.rio.cattle.io/owner-name: {}
            f:objectset.rio.cattle.io/owner-namespace: {}
          f:labels:
            .: {}
            f:objectset.rio.cattle.io/hash: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"1e00bb2d-b3c9-4e99-a5e3-c7581648d632"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:spec:
          .: {}
          f:chart:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:artifacthub.io/changes: {}
                f:artifacthub.io/prerelease: {}
              f:apiVersion: {}
              f:appVersion: {}
              f:description: {}
              f:home: {}
              f:icon: {}
              f:keywords: {}
              f:kubeVersion: {}
              f:maintainers: {}
              f:name: {}
              f:sources: {}
              f:type: {}
              f:version: {}
            f:values:
              .: {}
              f:controller: {}
              f:defaultBackend: {}
              f:dhParam: {}
              f:global: {}
              f:imagePullSecrets: {}
              f:podSecurityPolicy: {}
              f:rbac: {}
              f:revisionHistoryLimit: {}
              f:serviceAccount: {}
              f:tcp: {}
              f:udp: {}
          f:helmVersion: {}
          f:info:
            .: {}
            f:description: {}
            f:firstDeployed: {}
            f:lastDeployed: {}
            f:notes: {}
            f:readme: {}
            f:status: {}
          f:name: {}
          f:namespace: {}
          f:resources: {}
          f:values:
            .: {}
            f:controller: {}
            f:global: {}
          f:version: {}
        f:status:
          .: {}
          f:observedGeneration: {}
          f:summary:
            .: {}
            f:state: {}
      manager: rancher
      operation: Update
      time: "2022-03-06T22:54:14Z"
    name: rke2-ingress-nginx
    namespace: kube-system
    ownerReferences:
    - apiVersion: v1
      blockOwnerDeletion: false
      controller: true
      kind: Secret
      name: sh.helm.release.v1.rke2-ingress-nginx.v3
      uid: 1e00bb2d-b3c9-4e99-a5e3-c7581648d632
    resourceVersion: "7086"
    uid: 621aaab6-b9ea-4cda-adec-8224dd3ae489
  spec:
    chart:
      metadata:
        annotations:
          artifacthub.io/changes: |
            - Upgrade lua-resty-balancer to v0.0.4 to stop coredumps
            - Add canary backend name in metrics
            - Add build info in metrics
          artifacthub.io/prerelease: "false"
        apiVersion: v2
        appVersion: 1.0.2
        description: Ingress controller for Kubernetes using NGINX as a reverse proxy
          and load balancer
        home: https://github.com/kubernetes/ingress-nginx
        icon: https://upload.wikimedia.org/wikipedia/commons/thumb/c/c5/Nginx_logo.svg/500px-Nginx_logo.svg.png
        keywords:
        - ingress
        - nginx
        kubeVersion: '>=1.19.0-0'
        maintainers:
        - name: ChiefAlexander
        name: rke2-ingress-nginx
        sources:
        - https://github.com/kubernetes/ingress-nginx
        type: application
        version: 4.0.306
      values:
        controller:
          addHeaders: {}
          admissionWebhooks:
            annotations: {}
            certificate: /usr/local/certificates/cert
            createSecretJob:
              resources: {}
            enabled: true
            existingPsp: "null"
            failurePolicy: Fail
            key: /usr/local/certificates/key
            namespaceSelector: {}
            objectSelector: {}
            patch:
              enabled: true
              image:
                pullPolicy: IfNotPresent
                repository: rancher/mirrored-ingress-nginx-kube-webhook-certgen
                tag: v1.0
              nodeSelector:
                kubernetes.io/os: linux
              podAnnotations: {}
              priorityClassName: "null"
              runAsUser: 2000
              tolerations: null
            patchWebhookJob:
              resources: {}
            port: 8443
            service:
              annotations: {}
              externalIPs: "null"
              loadBalancerSourceRanges: "null"
              servicePort: 443
              type: ClusterIP
          affinity: {}
          allowSnippetAnnotations: false
          annotations: {}
          autoscaling:
            behavior: {}
            enabled: false
            maxReplicas: 11
            minReplicas: 1
            targetCPUUtilizationPercentage: 50
            targetMemoryUtilizationPercentage: 50
          autoscalingTemplate: "null"
          config: {}
          configAnnotations: {}
          configMapNamespace: "null"
          containerName: rke2-ingress-nginx-controller
          containerPort:
            http: 80
            https: 443
          customTemplate:
            configMapKey: "null"
            configMapName: "null"
          dnsConfig: {}
          dnsPolicy: ClusterFirstWithHostNet
          electionID: ingress-controller-leader
          enableMimalloc: true
          existingPsp: "null"
          extraArgs: {}
          extraContainers: "null"
          extraEnvs: "null"
          extraInitContainers: "null"
          extraVolumeMounts: "null"
          extraVolumes: "null"
          healthCheckHost: "null"
          healthCheckPath: /healthz
          hostNetwork: true
          hostPort:
            enabled: true
            ports:
              http: 80
              https: 443
          hostname: {}
          image:
            allowPrivilegeEscalation: true
            pullPolicy: IfNotPresent
            repository: rancher/nginx-ingress-controller
            runAsUser: 101
            tag: nginx-1.0.2-hardened2
          ingressClassByName: false
          ingressClassResource:
            controllerValue: k8s.io/ingress-nginx
            default: false
            enabled: true
            name: nginx
            parameters: {}
          keda:
            apiVersion: keda.sh/v1alpha1
            behavior: {}
            cooldownPeriod: 300
            enabled: false
            maxReplicas: 11
            minReplicas: 1
            pollingInterval: 30
            restoreToOriginalReplicaCount: false
            scaledObject:
              annotations: {}
            triggers: null
          kind: DaemonSet
          labels: {}
          lifecycle:
            preStop:
              exec:
                command:
                - /wait-shutdown
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          maxmindLicenseKey: "null"
          metrics:
            enabled: false
            port: 10254
            prometheusRule:
              additionalLabels: {}
              enabled: false
              rules: null
            service:
              annotations: {}
              externalIPs: "null"
              loadBalancerSourceRanges: "null"
              servicePort: 10254
              type: ClusterIP
            serviceMonitor:
              additionalLabels: {}
              enabled: false
              metricRelabelings: "null"
              namespace: "null"
              namespaceSelector: {}
              scrapeInterval: 30s
              targetLabels: null
          minAvailable: 1
          minReadySeconds: 0
          name: controller
          nodeSelector:
            kubernetes.io/os: linux
          podAnnotations: {}
          podLabels: {}
          podSecurityContext: {}
          priorityClassName: "null"
          proxySetHeaders: {}
          publishService:
            enabled: false
            pathOverride: "null"
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          replicaCount: 1
          reportNodeInternalIp: false
          resources:
            requests:
              cpu: 100m
              memory: 90Mi
          scope:
            enabled: false
            namespace: "null"
          service:
            annotations: {}
            enableHttp: true
            enableHttps: true
            enabled: false
            externalIPs: "null"
            internal:
              annotations: {}
              enabled: false
              loadBalancerSourceRanges: null
            labels: {}
            loadBalancerSourceRanges: "null"
            nodePorts:
              http: "null"
              https: "null"
              tcp: {}
              udp: {}
            ports:
              http: 80
              https: 443
            targetPorts:
              http: http
              https: https
            type: LoadBalancer
          sysctls: {}
          tcp:
            annotations: {}
            configMapNamespace: "null"
          terminationGracePeriodSeconds: 300
          tolerations: "null"
          topologySpreadConstraints: "null"
          udp:
            annotations: {}
            configMapNamespace: "null"
          updateStrategy: {}
          watchIngressWithoutClass: true
        defaultBackend:
          affinity: {}
          autoscaling:
            annotations: {}
            enabled: false
            maxReplicas: 2
            minReplicas: 1
            targetCPUUtilizationPercentage: 50
            targetMemoryUtilizationPercentage: 50
          enabled: false
          existingPsp: "null"
          extraArgs: {}
          extraEnvs: "null"
          extraVolumeMounts: "null"
          extraVolumes: "null"
          image:
            allowPrivilegeEscalation: false
            pullPolicy: IfNotPresent
            readOnlyRootFilesystem: true
            repository: rancher/nginx-ingress-controller-defaultbackend
            runAsNonRoot: true
            runAsUser: 65534
            tag: 1.5-rancher1
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          minAvailable: 1
          name: defaultbackend
          nodeSelector:
            kubernetes.io/os: linux
          podAnnotations: {}
          podLabels: {}
          podSecurityContext: {}
          port: 8080
          priorityClassName: "null"
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 0
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 5
          replicaCount: 1
          resources: {}
          service:
            annotations: {}
            externalIPs: "null"
            loadBalancerSourceRanges: "null"
            servicePort: 80
            type: ClusterIP
          serviceAccount:
            automountServiceAccountToken: true
            create: true
            name: "null"
          tolerations: null
        dhParam: "null"
        global:
          systemDefaultRegistry: "null"
        imagePullSecrets: "null"
        podSecurityPolicy:
          enabled: false
        rbac:
          create: true
          scope: false
        revisionHistoryLimit: 10
        serviceAccount:
          automountServiceAccountToken: true
          create: true
          name: "null"
        tcp: {}
        udp: {}
    helmVersion: 3
    info:
      description: Upgrade complete
      firstDeployed: "2022-03-06T22:47:55Z"
      lastDeployed: "2022-03-06T22:53:43Z"
      notes: |
        The ingress-nginx controller has been installed.
        It may take a few minutes for the LoadBalancer IP to be available.
        You can watch the status by running 'kubectl --namespace kube-system get services -o wide -w rke2-ingress-nginx-controller'

        An example Ingress that makes use of the controller:

          apiVersion: networking.k8s.io/v1
          kind: Ingress
          metadata:
            annotations:
              kubernetes.io/ingress.class: nginx
            name: example
            namespace: foo
          spec:
            rules:
              - host: www.example.com
                http:
                  paths:
                    - backend:
                        serviceName: exampleService
                        servicePort: 80
                      path: /
            # This section is only required if TLS is to be enabled for the Ingress
            tls:
                - hosts:
                    - www.example.com
                  secretName: example-tls

        If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided:

          apiVersion: v1
          kind: Secret
          metadata:
            name: example-tls
            namespace: foo
          data:
            tls.crt: <base64 encoded cert>
            tls.key: <base64 encoded key>
          type: kubernetes.io/tls
      readme: "# ingress-nginx\n\n[ingress-nginx](https://github.com/kubernetes/ingress-nginx)
        Ingress controller for Kubernetes using NGINX as a reverse proxy and load
        balancer\n\nTo use, add the `kubernetes.io/ingress.class: nginx` annotation
        to your Ingress resources.\n\nThis chart bootstraps an ingress-nginx deployment
        on a [Kubernetes](http://kubernetes.io) cluster using the [Helm](https://helm.sh)
        package manager.\n\n## Prerequisites\n\n- Kubernetes v1.16+\n\n## Get Repo
        Info\n\n```console\nhelm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\nhelm
        repo update\n```\n\n## Install Chart\n\n**Important:** only helm3 is supported\n\n```console\nhelm
        install [RELEASE_NAME] ingress-nginx/ingress-nginx\n```\n\nThe command deploys
        ingress-nginx on the Kubernetes cluster in the default configuration.\n\n_See
        [configuration](#configuration) below._\n\n_See [helm install](https://helm.sh/docs/helm/helm_install/)
        for command documentation._\n\n## Uninstall Chart\n\n```console\nhelm uninstall
        [RELEASE_NAME]\n```\n\nThis removes all the Kubernetes components associated
        with the chart and deletes the release.\n\n_See [helm uninstall](https://helm.sh/docs/helm/helm_uninstall/)
        for command documentation._\n\n## Upgrading Chart\n\n```console\nhelm upgrade
        [RELEASE_NAME] [CHART] --install\n```\n\n_See [helm upgrade](https://helm.sh/docs/helm/helm_upgrade/)
        for command documentation._\n\n### Upgrading With Zero Downtime in Production\n\nBy
        default the ingress-nginx controller has service interruptions whenever it's
        pods are restarted or redeployed. In order to fix that, see the excellent
        blog post by Lindsay Landry from Codecademy: [Kubernetes: Nginx and Zero Downtime
        in Production](https://medium.com/codecademy-engineering/kubernetes-nginx-and-zero-downtime-in-production-2c910c6a5ed8).\n\n###
        Migrating from stable/nginx-ingress\n\nThere are two main ways to migrate
        a release from `stable/nginx-ingress` to `ingress-nginx/ingress-nginx` chart:\n\n1.
        For Nginx Ingress controllers used for non-critical services, the easiest
        method is to [uninstall](#uninstall-chart) the old release and [install](#install-chart)
        the new one\n1. For critical services in production that require zero-downtime,
        you will want to:\n    1. [Install](#install-chart) a second Ingress controller\n
        \   1. Redirect your DNS traffic from the old controller to the new controller\n
        \   1. Log traffic from both controllers during this changeover\n    1. [Uninstall](#uninstall-chart)
        the old controller once traffic has fully drained from it\n    1. For details
        on all of these steps see [Upgrading With Zero Downtime in Production](#upgrading-with-zero-downtime-in-production)\n\nNote
        that there are some different and upgraded configurations between the two
        charts, described by Rimas Mocevicius from JFrog in the \"Upgrading to ingress-nginx
        Helm chart\" section of [Migrating from Helm chart nginx-ingress to ingress-nginx](https://rimusz.net/migrating-to-ingress-nginx).
        As the `ingress-nginx/ingress-nginx` chart continues to update, you will want
        to check current differences by running [helm configuration](#configuration)
        commands on both charts.\n\n## Configuration\n\nSee [Customizing the Chart
        Before Installing](https://helm.sh/docs/intro/using_helm/#customizing-the-chart-before-installing).
        To see all configurable options with detailed comments, visit the chart's
        [values.yaml](./values.yaml), or run these configuration commands:\n\n```console\nhelm
        show values ingress-nginx/ingress-nginx\n```\n\n### PodDisruptionBudget\n\nNote
        that the PodDisruptionBudget resource will only be defined if the replicaCount
        is greater than one,\nelse it would make it impossible to evacuate a node.
        See [gh issue #7127](https://github.com/helm/charts/issues/7127) for more
        info.\n\n### Prometheus Metrics\n\nThe Nginx ingress controller can export
        Prometheus metrics, by setting `controller.metrics.enabled` to `true`.\n\nYou
        can add Prometheus annotations to the metrics service using `controller.metrics.service.annotations`.\nAlternatively,
        if you use the Prometheus Operator, you can enable ServiceMonitor creation
        using `controller.metrics.serviceMonitor.enabled`. And set `controller.metrics.serviceMonitor.additionalLabels.release=\"prometheus\"`.
        \"release=prometheus\" should match the label configured in the prometheus
        servicemonitor ( see `kubectl get servicemonitor prometheus-kube-prom-prometheus
        -oyaml -n prometheus`)\n\n### ingress-nginx nginx\\_status page/stats server\n\nPrevious
        versions of this chart had a `controller.stats.*` configuration block, which
        is now obsolete due to the following changes in nginx ingress controller:\n\n-
        In [0.16.1](https://github.com/kubernetes/ingress-nginx/blob/main/Changelog.md#0161),
        the vts (virtual host traffic status) dashboard was removed\n- In [0.23.0](https://github.com/kubernetes/ingress-nginx/blob/main/Changelog.md#0230),
        the status page at port 18080 is now a unix socket webserver only available
        at localhost.\n  You can use `curl --unix-socket /tmp/nginx-status-server.sock
        http://localhost/nginx_status` inside the controller container to access it
        locally, or use the snippet from [nginx-ingress changelog](https://github.com/kubernetes/ingress-nginx/blob/main/Changelog.md#0230)
        to re-enable the http server\n\n### ExternalDNS Service Configuration\n\nAdd
        an [ExternalDNS](https://github.com/kubernetes-incubator/external-dns) annotation
        to the LoadBalancer service:\n\n```yaml\ncontroller:\n  service:\n    annotations:\n
        \     external-dns.alpha.kubernetes.io/hostname: kubernetes-example.com.\n```\n\n###
        AWS L7 ELB with SSL Termination\n\nAnnotate the controller as shown in the
        [nginx-ingress l7 patch](https://github.com/kubernetes/ingress-nginx/blob/main/deploy/aws/l7/service-l7.yaml):\n\n```yaml\ncontroller:\n
        \ service:\n    targetPorts:\n      http: http\n      https: http\n    annotations:\n
        \     service.beta.kubernetes.io/aws-load-balancer-ssl-cert: arn:aws:acm:XX-XXXX-X:XXXXXXXXX:certificate/XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXX\n
        \     service.beta.kubernetes.io/aws-load-balancer-backend-protocol: \"http\"\n
        \     service.beta.kubernetes.io/aws-load-balancer-ssl-ports: \"https\"\n
        \     service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout:
        '3600'\n```\n\n### AWS route53-mapper\n\nTo configure the LoadBalancer service
        with the [route53-mapper addon](https://github.com/kubernetes/kops/tree/master/addons/route53-mapper),
        add the `domainName` annotation and `dns` label:\n\n```yaml\ncontroller:\n
        \ service:\n    labels:\n      dns: \"route53\"\n    annotations:\n      domainName:
        \"kubernetes-example.com\"\n```\n\n### Additional Internal Load Balancer\n\nThis
        setup is useful when you need both external and internal load balancers but
        don't want to have multiple ingress controllers and multiple ingress objects
        per application.\n\nBy default, the ingress object will point to the external
        load balancer address, but if correctly configured, you can make use of the
        internal one if the URL you are looking up resolves to the internal load balancer's
        URL.\n\nYou'll need to set both the following values:\n\n`controller.service.internal.enabled`\n`controller.service.internal.annotations`\n\nIf
        one of them is missing the internal load balancer will not be deployed. Example
        you may have `controller.service.internal.enabled=true` but no annotations
        set, in this case no action will be taken.\n\n`controller.service.internal.annotations`
        varies with the cloud service you're using.\n\nExample for AWS:\n\n```yaml\ncontroller:\n
        \ service:\n    internal:\n      enabled: true\n      annotations:\n        #
        Create internal ELB\n        service.beta.kubernetes.io/aws-load-balancer-internal:
        \"true\"\n        # Any other annotation can be declared here.\n```\n\nExample
        for GCE:\n\n```yaml\ncontroller:\n  service:\n    internal:\n      enabled:
        true\n      annotations:\n        # Create internal LB. More informations:
        https://cloud.google.com/kubernetes-engine/docs/how-to/internal-load-balancing\n
        \       # For GKE versions 1.17 and later\n        networking.gke.io/load-balancer-type:
        \"Internal\"\n        # For earlier versions\n        # cloud.google.com/load-balancer-type:
        \"Internal\"\n        \n        # Any other annotation can be declared here.
        \n```\n\nExample for Azure:\n\n```yaml\ncontroller:\n  service:\n      annotations:\n
        \       # Create internal LB\n        service.beta.kubernetes.io/azure-load-balancer-internal:
        \"true\"\n        # Any other annotation can be declared here.\n```\n\nExample
        for Oracle Cloud Infrastructure:\n\n```yaml\ncontroller:\n  service:\n      annotations:\n
        \       # Create internal LB\n        service.beta.kubernetes.io/oci-load-balancer-internal:
        \"true\"\n        # Any other annotation can be declared here.\n```\n\nAn
        use case for this scenario is having a split-view DNS setup where the public
        zone CNAME records point to the external balancer URL while the private zone
        CNAME records point to the internal balancer URL. This way, you only need
        one ingress kubernetes object.\n\nOptionally you can set `controller.service.loadBalancerIP`
        if you need a static IP for the resulting `LoadBalancer`.\n\n### Ingress Admission
        Webhooks\n\nWith nginx-ingress-controller version 0.25+, the nginx ingress
        controller pod exposes an endpoint that will integrate with the `validatingwebhookconfiguration`
        Kubernetes feature to prevent bad ingress from being added to the cluster.\n**This
        feature is enabled by default since 0.31.0.**\n\nWith nginx-ingress-controller
        in 0.25.* work only with kubernetes 1.14+, 0.26 fix [this issue](https://github.com/kubernetes/ingress-nginx/pull/4521)\n\n###
        Helm Error When Upgrading: spec.clusterIP: Invalid value: \"\"\n\nIf you are
        upgrading this chart from a version between 0.31.0 and 1.2.2 then you may
        get an error like this:\n\n```console\nError: UPGRADE FAILED: Service \"?????-controller\"
        is invalid: spec.clusterIP: Invalid value: \"\": field is immutable\n```\n\nDetail
        of how and why are in [this issue](https://github.com/helm/charts/pull/13646)
        but to resolve this you can set `xxxx.service.omitClusterIP` to `true` where
        `xxxx` is the service referenced in the error.\n\nAs of version `1.26.0` of
        this chart, by simply not providing any clusterIP value, `invalid: spec.clusterIP:
        Invalid value: \"\": field is immutable` will no longer occur since `clusterIP:
        \"\"` will not be rendered.\n"
      status: deployed
    name: rke2-ingress-nginx
    namespace: kube-system
    resources:
    - apiVersion: v1
      kind: ServiceAccount
      name: rke2-ingress-nginx
      namespace: kube-system
    - apiVersion: v1
      kind: ConfigMap
      name: rke2-ingress-nginx-controller
      namespace: kube-system
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      name: rke2-ingress-nginx
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      name: rke2-ingress-nginx
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: Role
      name: rke2-ingress-nginx
      namespace: kube-system
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: RoleBinding
      name: rke2-ingress-nginx
      namespace: kube-system
    - apiVersion: v1
      kind: Service
      name: rke2-ingress-nginx-controller-admission
      namespace: kube-system
    - apiVersion: apps/v1
      kind: DaemonSet
      name: rke2-ingress-nginx-controller
      namespace: kube-system
    - apiVersion: networking.k8s.io/v1
      kind: IngressClass
      name: nginx
    - apiVersion: admissionregistration.k8s.io/v1
      kind: ValidatingWebhookConfiguration
      name: rke2-ingress-nginx-admission
    values:
      controller:
        admissionWebhooks:
          port: 8444
        config:
          proxy-body-size: "0"
          proxy-request-buffering: "off"
        extraArgs:
          default-ssl-certificate: cattle-system/tls-rancher-internal
        publishService:
          pathOverride: kube-system/ingress-expose
      global:
        clusterCIDR: 10.52.0.0/16
        clusterCIDRv4: 10.52.0.0/16
        clusterCIDRv6: "null"
        clusterDNS: 10.53.0.10
        clusterDomain: cluster.local
        rke2DataDir: /var/lib/rancher/rke2
        serviceCIDR: 10.53.0.0/16
        systemDefaultRegistry: "null"
    version: 3
  status:
    observedGeneration: 4
    summary:
      state: deployed
- apiVersion: catalog.cattle.io/v1
  kind: App
  metadata:
    annotations:
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/7xXW2/rxhH+K8T0pQ8kRVKWj0sgQB0rPRXaOILt5KGJH4bkSNxquUvsLmUrhv57MUvq5ltOT93zJu5tLt/3zYyeoCGHFTqE/AlQKe3QCa0sf+ri31Q6Sy42QsclOicpFnokKsihJtlE2LYQvnlOPygy0XK9ghxG6zQM/iFU9d0tlYbcH15T2BDkYOuYLcWGJKGleJ3GZkVZ1JAzorSRJbMmE6/TL3rQtljyq6uuoMhurKMGtiGUhnzUd6Ih67BpIVedlCFILEi+m4sabQ05pOPzSTEeL1LMqnGWTIoM0wVm59k5fqLkIk0XNEnLCbK1IbJXwoB+8zUvQ/BR3NCCDKmSLOS/PgG24hcyVmgFOfgcFFKXq5/46JQkOb+zQGkphFIrZ7SUZCB3pqMQVkIxlHtEviLnnScD0acCz8bnEV6Mq+gswyQqcDKO0iqbULVIL9JxCtv7bQi2pZLzWdZoHP844d+LcLBtDytJPIkTCKEiWxrR9sHBj71Xwa33KhA2wKCUnXVkogdRUYDLpaElOm0CvQgMWd2ZkoLO4pKCCiGEWvu4a+dam49GS+HqrohL3YwYAqPIkY2EKruCXxm9AG1FmwdtKsYEnm3eh9CgUA6FItODRg0KCTno+EG4+q+21cYZrNCwxQMKWlKDZtVZ2IaHS6sY0aJ689aKlCJX+0Owvf8DtvWp8H5/ZfD3Iaz3AGVxmsZpkkRFJ2SVJVmapulfkjNm/RplR15KuFgIJdwG8qctIywYOcGMf+qVSD0/edMsvXNRxL5IclFrWAGGqgirypC1kdu0ZL+bKUdGoZzN2SV6dAav9CHtvZz98i9adg39qDvlXtvYry2lLlCyU70Ep7TATrobWgrrzAZy4LBqbd01uQdtVnyUFBaSqkFz2xBEg0sfWdtJOddSlHxztrjWbm7IkmLdGWq1FU77Vw2qsiYzqtFUpKiKVhf2JXIOl6wRL4nTdMPO6ryTspf2PiYp1qTI2rnRhfeKUf9MXogtOq5ko5pQuvp3CIEptpMFk6WsyXPp73d381tvRgknUE5J4uaWSq1YAlnCNU5XdEuSSqcNP36gki/IFnKQQnWP/Eqrq8uTxtOvTYU1nRf591217H08TS9L6/FnhWsUktd3UTZCXZ6u9S/+c1fN+dMIbYTbXEm09nqoex7niH2PSiOcKFEyOgWWz7kZQmvbH56DbQgr8U3ya6iVokQLecofex1zaJbKzoemlaNHbxul1A9zI9ZC0pJ+sCVKPO0N2GIhpHCi12hldMvCQynhfojsJyU3N1q7vwlJQ1cacmE6dWk/G921kKdJkqTD0rVWfOHk2M+W+48/5X3dSx+fc0Aew+VTdXY2DoH1Djlc9SV+NofDM5dlybJ+CVZfBAcmOC3J7Az5NdYxyWbfacac+oX2iTjpNTNlHUoZlLppJTmCEBbCWDelVuoNUwGyJMuiZBwl53dZlp99yieTfwEH8yWnlHacf7irKeglH/SKD2q0QUGkgmp4JA5+U7+pmQowWNBD0AjVObLBRneBrXUHPWaeT38KTssHX/z1tG3e//nd+s/sdOg69m3nwP8wyhwR9tUhZj+XnKD6lea4e56YYD3H2LlaG/G750G8uvB16dj4wK8bLeloMvJP5q84EO2mDKoiTjyZb2T4A8x8L1Ql1PLdBOeDB/xuVJHsB6r/h/GPD/VLY4x8cAN6/wWhXuHsh5EV29aeBNMXkYY+UBDYCtNPNW+m8HI+exHZOi3IYRoPhodrfug8DHuHKWqYya9m0xv+w5TEkyxO4mSUnkN4vLk+e3/7nCev/dL0+nY4Po6TOE2OdjQP3pDvvmOph36+omyKDqfC+D+maEZSFKPd3MXbsG8pR+6OD/68PRFuj+Zh3+CGssn/ID0y1WdSQ/uBPAnBdk2Dxg/D2+1/AgAA//9E+wSejw8AAA
      objectset.rio.cattle.io/id: helm-app
      objectset.rio.cattle.io/owner-gvk: /v1, Kind=Secret
      objectset.rio.cattle.io/owner-name: sh.helm.release.v1.rke2-metrics-server.v1
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2022-03-06T22:48:49Z"
    generation: 1
    labels:
      objectset.rio.cattle.io/hash: 1365b33f1a2d3205b2a1fa2626a7e0811fe51c5a
    managedFields:
    - apiVersion: catalog.cattle.io/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:objectset.rio.cattle.io/applied: {}
            f:objectset.rio.cattle.io/id: {}
            f:objectset.rio.cattle.io/owner-gvk: {}
            f:objectset.rio.cattle.io/owner-name: {}
            f:objectset.rio.cattle.io/owner-namespace: {}
          f:labels:
            .: {}
            f:objectset.rio.cattle.io/hash: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"ee7ba436-a83d-42a0-ba53-1d25edf18131"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:spec:
          .: {}
          f:chart:
            .: {}
            f:metadata:
              .: {}
              f:apiVersion: {}
              f:appVersion: {}
              f:description: {}
              f:home: {}
              f:keywords: {}
              f:maintainers: {}
              f:name: {}
              f:sources: {}
              f:version: {}
            f:values:
              .: {}
              f:affinity: {}
              f:apiService: {}
              f:args: {}
              f:extraContainers: {}
              f:extraVolumeMounts: {}
              f:extraVolumes: {}
              f:global: {}
              f:hostNetwork: {}
              f:image: {}
              f:imagePullSecrets: {}
              f:livenessProbe: {}
              f:nodeSelector: {}
              f:podAnnotations: {}
              f:podDisruptionBudget: {}
              f:podLabels: {}
              f:priorityClassName: {}
              f:rbac: {}
              f:readinessProbe: {}
              f:replicas: {}
              f:resources: {}
              f:securityContext: {}
              f:service: {}
              f:serviceAccount: {}
              f:tolerations: {}
          f:helmVersion: {}
          f:info:
            .: {}
            f:description: {}
            f:firstDeployed: {}
            f:lastDeployed: {}
            f:notes: {}
            f:readme: {}
            f:status: {}
          f:name: {}
          f:namespace: {}
          f:resources: {}
          f:values:
            .: {}
            f:global: {}
          f:version: {}
        f:status:
          .: {}
          f:observedGeneration: {}
          f:summary:
            .: {}
            f:state: {}
      manager: rancher
      operation: Update
      time: "2022-03-06T22:48:50Z"
    name: rke2-metrics-server
    namespace: kube-system
    ownerReferences:
    - apiVersion: v1
      blockOwnerDeletion: false
      controller: true
      kind: Secret
      name: sh.helm.release.v1.rke2-metrics-server.v1
      uid: ee7ba436-a83d-42a0-ba53-1d25edf18131
    resourceVersion: "1512"
    uid: ab141a01-3f07-4716-ab28-dd65b07c2923
  spec:
    chart:
      metadata:
        apiVersion: v1
        appVersion: 0.5.0
        description: Metrics Server is a cluster-wide aggregator of resource usage
          data.
        home: https://github.com/kubernetes-incubator/metrics-server
        keywords:
        - metrics-server
        maintainers:
        - email: o.with@sportradar.com
          name: olemarkus
        - email: k.aasan@sportradar.com
          name: kennethaasan
        name: rke2-metrics-server
        sources:
        - https://github.com/kubernetes-incubator/metrics-server
        version: 2.11.100-build2021111904
      values:
        affinity: {}
        apiService:
          create: true
        args:
        - --kubelet-preferred-address-types=InternalIP
        extraContainers: "null"
        extraVolumeMounts: "null"
        extraVolumes: "null"
        global:
          systemDefaultRegistry: "null"
        hostNetwork:
          enabled: false
        image:
          pullPolicy: IfNotPresent
          repository: rancher/hardened-k8s-metrics-server
          tag: v0.5.0-build20211119
        imagePullSecrets: "null"
        livenessProbe:
          httpGet:
            path: /healthz
            port: https
            scheme: HTTPS
          initialDelaySeconds: 20
        nodeSelector:
          kubernetes.io/os: linux
        podAnnotations: {}
        podDisruptionBudget:
          enabled: false
          maxUnavailable: "null"
          minAvailable: null
        podLabels: {}
        priorityClassName: system-node-critical
        rbac:
          create: true
          pspEnabled: false
        readinessProbe:
          httpGet:
            path: /healthz
            port: https
            scheme: HTTPS
          initialDelaySeconds: 20
        replicas: 1
        resources: {}
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - all
          readOnlyRootFilesystem: true
          runAsGroup: 10001
          runAsNonRoot: true
          runAsUser: 10001
        service:
          annotations: {}
          labels: {}
          port: 443
          type: ClusterIP
        serviceAccount:
          create: true
          name: null
        tolerations: null
    helmVersion: 3
    info:
      description: Install complete
      firstDeployed: "2022-03-06T22:47:55Z"
      lastDeployed: "2022-03-06T22:47:55Z"
      notes: "The metric server has been deployed. \n\nIn a few minutes you should
        be able to list metrics using the following\ncommand:\n\n  kubectl get --raw
        \"/apis/metrics.k8s.io/v1beta1/nodes\"\n"
      readme: |
        # metrics-server

        [Metrics Server](https://github.com/kubernetes-incubator/metrics-server) is a cluster-wide aggregator of resource usage data. Resource metrics are used by components like `kubectl top` and the [Horizontal Pod Autoscaler](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale) to scale workloads. To autoscale based upon a custom metric, see the [Prometheus Adapter chart](https://github.com/helm/charts/blob/master/stable/prometheus-adapter).

        ## Configuration

        Parameter | Description | Default
        --- | --- | ---
        `rbac.create` | Enable Role-based authentication | `true`
        `rbac.pspEnabled` | Enable pod security policy support | `false`
        `serviceAccount.create` | If `true`, create a new service account | `true`
        `serviceAccount.name` | Service account to be used. If not set and `serviceAccount.create` is `true`, a name is generated using the fullname template | ``
        `apiService.create` | Create the v1beta1.metrics.k8s.io API service | `true`
        `hostNetwork.enabled` | Enable hostNetwork mode | `false`
        `image.repository` | Image repository | `k8s.gcr.io/metrics-server-amd64`
        `image.tag` | Image tag | `v0.3.2`
        `image.pullPolicy` | Image pull policy | `IfNotPresent`
        `imagePullSecrets` | Image pull secrets | `[]`
        `args` | Command line arguments | `[]`
        `resources` | CPU/Memory resource requests/limits. | `{}`
        `tolerations` | List of node taints to tolerate (requires Kubernetes >=1.6) | `[]`
        `nodeSelector` | Node labels for pod assignment | `{}`
        `affinity` | Node affinity | `{}`
        `replicas` | Number of replicas | `1`
        `extraVolumeMounts` | Ability to provide volume mounts to the pod | `[]`
        `extraVolumes` | Ability to provide volumes to the pod | `[]`
        `livenessProbe` | Container liveness probe | See values.yaml
        `podLabels` | Labels to be added to pods | `{}`
        `podAnnotations` | Annotations to be added to pods | `{}`
        `priorityClassName` | Pod priority class | `""`
        `readinessProbe` | Container readiness probe | See values.yaml
        `service.annotations` | Annotations to add to the service | `{}`
        `service.labels` | Labels to be added to the metrics-server service | `{}`
        `service.port` | Service port to expose | `443`
        `service.type` | Type of service to create | `ClusterIP`
        `podDisruptionBudget.enabled` | Create a PodDisruptionBudget | `false`
        `podDisruptionBudget.minAvailable` | Minimum available instances; ignored if there is no PodDisruptionBudget |
        `podDisruptionBudget.maxUnavailable` | Maximum unavailable instances; ignored if there is no PodDisruptionBudget |
        `extraContainers`   | Add additional containers  | `[]`
      status: deployed
    name: rke2-metrics-server
    namespace: kube-system
    resources:
    - apiVersion: v1
      kind: ServiceAccount
      name: rke2-metrics-server
      namespace: kube-system
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      name: system:rke2-metrics-server-aggregated-reader
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      name: system:rke2-metrics-server
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      name: rke2-metrics-server:system:auth-delegator
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      name: system:rke2-metrics-server
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: RoleBinding
      name: rke2-metrics-server-auth-reader
      namespace: kube-system
    - apiVersion: v1
      kind: Service
      name: rke2-metrics-server
      namespace: kube-system
    - apiVersion: apps/v1
      kind: Deployment
      name: rke2-metrics-server
      namespace: kube-system
    - apiVersion: apiregistration.k8s.io/v1
      kind: APIService
      name: v1beta1.metrics.k8s.io
    values:
      global:
        clusterCIDR: 10.52.0.0/16
        clusterCIDRv4: 10.52.0.0/16
        clusterCIDRv6: "null"
        clusterDNS: 10.53.0.10
        clusterDomain: cluster.local
        rke2DataDir: /var/lib/rancher/rke2
        serviceCIDR: 10.53.0.0/16
        systemDefaultRegistry: "null"
    version: 1
  status:
    observedGeneration: 1
    summary:
      state: deployed
- apiVersion: catalog.cattle.io/v1
  kind: App
  metadata:
    annotations:
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/6RWy27rNhD9FWHWEi3Jzwgo0DYGiqDtLeBcdNEiixE1klhTJEFSTtNA/15QchznndvuTHIeZ2bOHPkeOvJYoUco7gGV0h690MqFoy7/Iu4deWaFZhy9l8SEnokKCmhJdgkaA/GbdvpWkU2awx4KmB2yOPpZqOq7a+KW/IduCjuCAlzLQiZmSRI6YoeM2T3lSddL3zt2yD4VyBnkIdq+Lylxd85TB0MM3NJY7VfRkfPYGShUL2UMEkuS7/agRddCAZuU6mxTl6vVHDcZpzJd8mqxJJqniwvk1Xy1qJYXOYZsx4rO4MN0+Rq6GEb0O6rJkuLkoPjzHtCI38k6oRUUMNZeSs33vwXTLUny40uN0lEMXCtvtZRkofC2pxj2QoXRnSbwDT3ux6FnuFyvaLVO0myzThZ8jcmGr+fJolyvcqrrJa1zGG6GGJwhHvrHW7Q+/HjCsxdloDFnN3O2ZuG2IsetMFNV8OsIJ7r8chWRwlKSi9B75K1QTRSwCiMpUuRvtd1HQnmyNXJykdeR0VXodqvHglvvjStms0b4ti8Z191sv3FHTyP7rhSquW1mUwMSrgTEIPiI4sHX4i2b/HtHNvSalP9EKIihQ6E8CkV2mip1KCQUU6/c9xYVb8mGWI8z2k2X0S9YOhhu3mCT072duPJfaryJ4fBsCEnZC1nlaZ5lWXaRrgKPDyh7GpeDK2Fk34hJLkSHDYUflox2wmt7FwBOwGct2ooUVSFT8uAVg8cmJEvZxfNkIZXbC8OVcFBALVEpkuG2kbpEGTJNy7KlGnvpd9QI58ekwerYlM8DezLtI65XmgDDMAzxqH8nys5jEKrWIcdTzl4p51HKiOvOSPIEMdTCOr8lI/UdhZ3K0zxP0nmSrr7mebFYF9nmDwgS9Bkr53EsEqoH028XGkuPtHltN0+qYQ+C0w+c616dqceHGYb4WVQ0gv72pMLRsf3GBUE9T3XZO6+73RHXlmqhxNjQU9Ijj5NJADpSPqlOZlNMrgTjitdM6JcQbImcYe9bbcU/4zfgVRyyd57sTkt6Ue//D/mjUJVQzceR0Rj3JMgWqdPq+lzDx4YfGVy9O42b8w1+XCU+Abu82u6C0KdsmbOUpbNsBfH542Hx/vMqrN/pavvl+mg+ZynL0rMXHWQwqN50ZlJzlIGOe8q36HEr7Pi/Ae1MinL2sK7hORB/YuMZ3PkjnrdlYTiTuGx43J/woQ8hqfqJFFmc1jeNwfVdh8H5fhiGfwMAAP//L7PFty4JAAA
      objectset.rio.cattle.io/id: helm-app
      objectset.rio.cattle.io/owner-gvk: /v1, Kind=Secret
      objectset.rio.cattle.io/owner-name: sh.helm.release.v1.rke2-multus.v1
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2022-03-06T22:48:49Z"
    generation: 1
    labels:
      objectset.rio.cattle.io/hash: 80ef18fb663a81ceb05cd45ee3049acd364d592a
    managedFields:
    - apiVersion: catalog.cattle.io/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:objectset.rio.cattle.io/applied: {}
            f:objectset.rio.cattle.io/id: {}
            f:objectset.rio.cattle.io/owner-gvk: {}
            f:objectset.rio.cattle.io/owner-name: {}
            f:objectset.rio.cattle.io/owner-namespace: {}
          f:labels:
            .: {}
            f:objectset.rio.cattle.io/hash: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"1a576e67-0187-4c7a-8c73-4b762eff5e72"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:spec:
          .: {}
          f:chart:
            .: {}
            f:metadata:
              .: {}
              f:apiVersion: {}
              f:appVersion: {}
              f:description: {}
              f:home: {}
              f:icon: {}
              f:maintainers: {}
              f:name: {}
              f:sources: {}
              f:version: {}
            f:values:
              .: {}
              f:cniplugins: {}
              f:global: {}
              f:multus: {}
          f:helmVersion: {}
          f:info:
            .: {}
            f:description: {}
            f:firstDeployed: {}
            f:lastDeployed: {}
            f:status: {}
          f:name: {}
          f:namespace: {}
          f:resources: {}
          f:values:
            .: {}
            f:global: {}
          f:version: {}
        f:status:
          .: {}
          f:observedGeneration: {}
          f:summary:
            .: {}
            f:state: {}
      manager: rancher
      operation: Update
      time: "2022-03-06T22:48:50Z"
    name: rke2-multus
    namespace: kube-system
    ownerReferences:
    - apiVersion: v1
      blockOwnerDeletion: false
      controller: true
      kind: Secret
      name: sh.helm.release.v1.rke2-multus.v1
      uid: 1a576e67-0187-4c7a-8c73-4b762eff5e72
    resourceVersion: "1513"
    uid: 11028ef9-f6f6-43d4-ab21-db66fdf50a2e
  spec:
    chart:
      metadata:
        apiVersion: v1
        appVersion: v3.7.1
        description: Multus CNI enables attaching multiple network interfaces to pods
          in Kubernetes.
        home: https://github.com/k8snetworkplumbingwg/multus-cni
        icon: https://raw.githubusercontent.com/k8snetworkplumbingwg/multus-cni/master/doc/images/Multus.png
        maintainers:
        - email: charts@rancher.com
          name: Rancher Labs
        name: rke2-multus
        sources:
        - https://github.com/k8snetworkplumbingwg/multus-cni
        version: v3.7.1-build2021111906
      values:
        cniplugins:
          image:
            repository: rancher/hardened-cni-plugins
            tag: v0.9.1-build20211119
          skipcnis: flannel
        global:
          systemDefaultRegistry: "null"
        multus:
          image:
            repository: rancher/hardened-multus-cni
            tag: v3.7.1-build20211119
    helmVersion: 3
    info:
      description: Install complete
      firstDeployed: "2022-03-06T22:47:18Z"
      lastDeployed: "2022-03-06T22:47:18Z"
      status: deployed
    name: rke2-multus
    namespace: kube-system
    resources:
    - apiVersion: v1
      kind: ServiceAccount
      name: multus
      namespace: kube-system
    - apiVersion: apiextensions.k8s.io/v1
      kind: CustomResourceDefinition
      name: network-attachment-definitions.k8s.cni.cncf.io
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      name: multus
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      name: multus
    - apiVersion: apps/v1
      kind: DaemonSet
      name: kube-multus-ds
      namespace: kube-system
    values:
      global:
        clusterCIDR: 10.52.0.0/16
        clusterCIDRv4: 10.52.0.0/16
        clusterCIDRv6: "null"
        clusterDNS: 10.53.0.10
        clusterDomain: cluster.local
        rke2DataDir: /var/lib/rancher/rke2
        serviceCIDR: 10.53.0.0/16
        systemDefaultRegistry: "null"
    version: 1
  status:
    observedGeneration: 1
    summary:
      state: deployed
kind: List
metadata:
  continue: "null"
  resourceVersion: "14659003"
