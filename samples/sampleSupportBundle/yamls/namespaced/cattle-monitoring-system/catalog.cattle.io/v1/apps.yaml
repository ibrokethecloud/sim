apiVersion: v1
items:
- apiVersion: catalog.cattle.io/v1
  kind: App
  metadata:
    annotations:
      objectset.rio.cattle.io/applied: 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
      objectset.rio.cattle.io/id: helm-app
      objectset.rio.cattle.io/owner-gvk: /v1, Kind=Secret
      objectset.rio.cattle.io/owner-name: sh.helm.release.v1.rancher-monitoring.v89
      objectset.rio.cattle.io/owner-namespace: cattle-monitoring-system
    creationTimestamp: "2022-03-06T22:51:24Z"
    generation: 178
    labels:
      objectset.rio.cattle.io/hash: 16c757af022d091b74d37b88a30af968381b4089
    managedFields:
    - apiVersion: catalog.cattle.io/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:objectset.rio.cattle.io/applied: {}
            f:objectset.rio.cattle.io/id: {}
            f:objectset.rio.cattle.io/owner-gvk: {}
            f:objectset.rio.cattle.io/owner-name: {}
            f:objectset.rio.cattle.io/owner-namespace: {}
          f:labels:
            .: {}
            f:objectset.rio.cattle.io/hash: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"31b1cb7f-5b37-4f5b-a6d3-0bfecd35e4d9"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:spec:
          .: {}
          f:chart:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:artifacthub.io/links: {}
                f:artifacthub.io/operator: {}
                f:catalog.cattle.io/auto-install: {}
                f:catalog.cattle.io/certified: {}
                f:catalog.cattle.io/display-name: {}
                f:catalog.cattle.io/kube-version: {}
                f:catalog.cattle.io/namespace: {}
                f:catalog.cattle.io/provides-gvr: {}
                f:catalog.cattle.io/rancher-version: {}
                f:catalog.cattle.io/release-name: {}
                f:catalog.cattle.io/requests-cpu: {}
                f:catalog.cattle.io/requests-memory: {}
                f:catalog.cattle.io/type: {}
                f:catalog.cattle.io/ui-component: {}
                f:catalog.cattle.io/upstream-version: {}
                f:fleet.cattle.io/agent-namespace: {}
                f:fleet.cattle.io/bundle-id: {}
                f:fleet.cattle.io/service-account: {}
              f:apiVersion: {}
              f:appVersion: {}
              f:description: {}
              f:home: {}
              f:icon: {}
              f:keywords: {}
              f:kubeVersion: {}
              f:maintainers: {}
              f:name: {}
              f:sources: {}
              f:type: {}
              f:version: {}
            f:values:
              .: {}
              f:additionalPrometheusRulesMap: {}
              f:alertmanager: {}
              f:commonLabels: {}
              f:coreDns: {}
              f:defaultRules: {}
              f:fullnameOverride: {}
              f:global: {}
              f:grafana: {}
              f:hardenedKubelet: {}
              f:hardenedNodeExporter: {}
              f:ingressNginx: {}
              f:k3sServer: {}
              f:kube-state-metrics: {}
              f:kubeAdmControllerManager: {}
              f:kubeAdmEtcd: {}
              f:kubeAdmProxy: {}
              f:kubeAdmScheduler: {}
              f:kubeApiServer: {}
              f:kubeControllerManager: {}
              f:kubeDns: {}
              f:kubeEtcd: {}
              f:kubeProxy: {}
              f:kubeScheduler: {}
              f:kubeStateMetrics: {}
              f:kubeTargetVersionOverride: {}
              f:kubeVersionOverride: {}
              f:kubelet: {}
              f:nameOverride: {}
              f:namespaceOverride: {}
              f:nodeExporter: {}
              f:prometheus: {}
              f:prometheus-adapter: {}
              f:prometheus-node-exporter: {}
              f:prometheusOperator: {}
              f:rke2ControllerManager: {}
              f:rke2Etcd: {}
              f:rke2IngressNginx: {}
              f:rke2Proxy: {}
              f:rke2Scheduler: {}
              f:rkeControllerManager: {}
              f:rkeEtcd: {}
              f:rkeIngressNginx: {}
              f:rkeProxy: {}
              f:rkeScheduler: {}
          f:helmVersion: {}
          f:info:
            .: {}
            f:description: {}
            f:firstDeployed: {}
            f:lastDeployed: {}
            f:notes: {}
            f:readme: {}
            f:status: {}
          f:name: {}
          f:namespace: {}
          f:resources: {}
          f:values:
            .: {}
            f:alertmanager: {}
            f:global: {}
            f:grafana: {}
            f:prometheus: {}
          f:version: {}
        f:status:
          .: {}
          f:observedGeneration: {}
          f:summary:
            .: {}
            f:state: {}
      manager: rancher
      operation: Update
      time: "2022-03-22T18:57:11Z"
    name: rancher-monitoring
    namespace: cattle-monitoring-system
    ownerReferences:
    - apiVersion: v1
      blockOwnerDeletion: false
      controller: true
      kind: Secret
      name: sh.helm.release.v1.rancher-monitoring.v89
      uid: 31b1cb7f-5b37-4f5b-a6d3-0bfecd35e4d9
    resourceVersion: "14474383"
    uid: abfd5c2c-778f-4325-a673-26aa0b6d785f
  spec:
    chart:
      metadata:
        annotations:
          artifacthub.io/links: |
            - name: Chart Source
              url: https://github.com/prometheus-community/helm-charts
            - name: Upstream Project
              url: https://github.com/prometheus-operator/kube-prometheus
          artifacthub.io/operator: "true"
          catalog.cattle.io/auto-install: rancher-monitoring-crd=match
          catalog.cattle.io/certified: rancher
          catalog.cattle.io/display-name: Monitoring
          catalog.cattle.io/kube-version: '>=1.16.0-0'
          catalog.cattle.io/namespace: cattle-monitoring-system
          catalog.cattle.io/provides-gvr: monitoring.coreos.com.prometheus/v1
          catalog.cattle.io/rancher-version: '>= 2.6.0-0 <=2.6.99-0'
          catalog.cattle.io/release-name: rancher-monitoring
          catalog.cattle.io/requests-cpu: 4500m
          catalog.cattle.io/requests-memory: 4000Mi
          catalog.cattle.io/type: cluster-tool
          catalog.cattle.io/ui-component: monitoring
          catalog.cattle.io/upstream-version: 19.0.3
          fleet.cattle.io/agent-namespace: cattle-fleet-local-system
          fleet.cattle.io/bundle-id: mcc-rancher-monitoring
          fleet.cattle.io/service-account: "null"
        apiVersion: v2
        appVersion: 0.50.0
        description: Collects several related Helm charts, Grafana dashboards, and
          Prometheus rules combined with documentation and scripts to provide easy
          to operate end-to-end Kubernetes cluster monitoring with Prometheus using
          the Prometheus Operator.
        home: https://github.com/prometheus-operator/kube-prometheus
        icon: https://raw.githubusercontent.com/prometheus/prometheus.github.io/master/assets/prometheus_logo-cb55bb5c346.png
        keywords:
        - operator
        - prometheus
        - kube-prometheus
        - monitoring
        kubeVersion: '>=1.16.0-0'
        maintainers:
        - name: vsliouniaev
        - name: bismarck
        - email: gianrubio@gmail.com
          name: gianrubio
        - email: github.gkarthiks@gmail.com
          name: gkarthiks
        - email: scott@r6by.com
          name: scottrigby
        - email: miroslav.hadzhiev@gmail.com
          name: Xtigyro
        - email: arvind.iyengar@suse.com
          name: Arvind
        - email: jiaqi.luo@suse.com
          name: Jack
          url: https://github.com/jiaqiluo
        name: rancher-monitoring
        sources:
        - https://github.com/prometheus-community/helm-charts
        - https://github.com/prometheus-operator/kube-prometheus
        type: application
        version: 100.1.0+up19.0.3
      values:
        additionalPrometheusRulesMap: {}
        alertmanager:
          alertmanagerSpec:
            additionalPeers: "null"
            affinity: {}
            alertmanagerConfigNamespaceSelector: {}
            alertmanagerConfigSelector: {}
            clusterAdvertiseAddress: false
            configMaps: "null"
            containers: "null"
            externalUrl: "null"
            forceEnableClusterMode: false
            image:
              repository: rancher/mirrored-prometheus-alertmanager
              sha: "null"
              tag: v0.22.2
            initContainers: "null"
            listenLocal: false
            logFormat: logfmt
            logLevel: info
            nodeSelector: {}
            paused: false
            podAntiAffinity: "null"
            podAntiAffinityTopologyKey: kubernetes.io/hostname
            podMetadata: {}
            portName: web
            priorityClassName: "null"
            replicas: 1
            resources:
              limits:
                cpu: 1000m
                memory: 500Mi
              requests:
                cpu: 100m
                memory: 100Mi
            retention: 120h
            routePrefix: /
            secrets: "null"
            securityContext:
              fsGroup: 2000
              runAsGroup: 2000
              runAsNonRoot: true
              runAsUser: 1000
            storage: {}
            tolerations: "null"
            topologySpreadConstraints: "null"
            useExistingSecret: false
            volumeMounts: "null"
            volumes: null
          annotations: {}
          apiVersion: v2
          config:
            global:
              resolve_timeout: 5m
            receivers:
            - name: "null"
            route:
              group_by:
              - job
              group_interval: 5m
              group_wait: 30s
              receiver: "null"
              repeat_interval: 12h
              routes:
              - match:
                  alertname: Watchdog
                receiver: "null"
            templates:
            - /etc/alertmanager/config/*.tmpl
          enabled: true
          extraSecret:
            annotations: {}
            data: {}
          ingress:
            annotations: {}
            enabled: false
            hosts: "null"
            labels: {}
            paths: "null"
            tls: null
          ingressPerReplica:
            annotations: {}
            enabled: false
            hostDomain: "null"
            hostPrefix: "null"
            labels: {}
            paths: "null"
            tlsSecretName: "null"
            tlsSecretPerReplica:
              enabled: false
              prefix: alertmanager
          podDisruptionBudget:
            enabled: false
            maxUnavailable: "null"
            minAvailable: 1
          secret:
            annotations: {}
          service:
            additionalPorts: "null"
            annotations: {}
            clusterIP: "null"
            externalIPs: "null"
            labels: {}
            loadBalancerIP: "null"
            loadBalancerSourceRanges: "null"
            nodePort: 30903
            port: 9093
            targetPort: 9093
            type: ClusterIP
          serviceAccount:
            annotations: {}
            create: true
            name: "null"
          serviceMonitor:
            bearerTokenFile: "null"
            interval: "null"
            metricRelabelings: "null"
            proxyUrl: "null"
            relabelings: "null"
            scheme: "null"
            selfMonitor: true
            tlsConfig: {}
          servicePerReplica:
            annotations: {}
            enabled: false
            loadBalancerSourceRanges: "null"
            nodePort: 30904
            port: 9093
            targetPort: 9093
            type: ClusterIP
          templateFiles:
            rancher_defaults.tmpl: |-
              {{- define "slack.rancher.text" -}}
              {{ template "rancher.text_multiple" . }}
              {{- end -}}

              {{- define "rancher.text_multiple" -}}
              *[GROUP - Details]*
              One or more alarms in this group have triggered a notification.

              {{- if gt (len .GroupLabels.Values) 0 }}
              *Group Labels:*
                {{- range .GroupLabels.SortedPairs }}
                • *{{ .Name }}:* `{{ .Value }}`
                {{- end }}
              {{- end }}
              {{- if .ExternalURL }}
              *Link to AlertManager:* {{ .ExternalURL }}
              {{- end }}

              {{- range .Alerts }}
              {{ template "rancher.text_single" . }}
              {{- end }}
              {{- end -}}

              {{- define "rancher.text_single" -}}
              {{- if .Labels.alertname }}
              *[ALERT - {{ .Labels.alertname }}]*
              {{- else }}
              *[ALERT]*
              {{- end }}
              {{- if .Labels.severity }}
              *Severity:* `{{ .Labels.severity }}`
              {{- end }}
              {{- if .Labels.cluster }}
              *Cluster:*  {{ .Labels.cluster }}
              {{- end }}
              {{- if .Annotations.summary }}
              *Summary:* {{ .Annotations.summary }}
              {{- end }}
              {{- if .Annotations.message }}
              *Message:* {{ .Annotations.message }}
              {{- end }}
              {{- if .Annotations.description }}
              *Description:* {{ .Annotations.description }}
              {{- end }}
              {{- if .Annotations.runbook_url }}
              *Runbook URL:* <{{ .Annotations.runbook_url }}|:spiral_note_pad:>
              {{- end }}
              {{- with .Labels }}
              {{- with .Remove (stringSlice "alertname" "severity" "cluster") }}
              {{- if gt (len .) 0 }}
              *Additional Labels:*
                {{- range .SortedPairs }}
                • *{{ .Name }}:* `{{ .Value }}`
                {{- end }}
              {{- end }}
              {{- end }}
              {{- end }}
              {{- with .Annotations }}
              {{- with .Remove (stringSlice "summary" "message" "description" "runbook_url") }}
              {{- if gt (len .) 0 }}
              *Additional Annotations:*
                {{- range .SortedPairs }}
                • *{{ .Name }}:* `{{ .Value }}`
                {{- end }}
              {{- end }}
              {{- end }}
              {{- end }}
              {{- end -}}
          tplConfig: false
        commonLabels: {}
        coreDns:
          enabled: true
          service:
            port: 9153
            targetPort: 9153
          serviceMonitor:
            interval: "null"
            metricRelabelings: "null"
            proxyUrl: "null"
            relabelings: null
        defaultRules:
          additionalRuleLabels: {}
          annotations: {}
          appNamespacesTarget: .*
          create: true
          labels: {}
          rules:
            alertmanager: true
            etcd: true
            general: true
            k8s: true
            kubeApiserver: true
            kubeApiserverAvailability: true
            kubeApiserverError: true
            kubeApiserverSlos: true
            kubePrometheusGeneral: true
            kubePrometheusNodeAlerting: true
            kubePrometheusNodeRecording: true
            kubeScheduler: true
            kubeStateMetrics: true
            kubelet: true
            kubernetesAbsent: true
            kubernetesApps: true
            kubernetesResources: true
            kubernetesStorage: true
            kubernetesSystem: true
            network: true
            node: true
            prometheus: true
            prometheusOperator: true
            time: true
          runbookUrl: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#
        fullnameOverride: "null"
        global:
          cattle:
            rkeWindowsPathPrefix: c:\
            systemDefaultRegistry: "null"
            windows:
              enabled: false
          imagePullSecrets: "null"
          kubectl:
            pullPolicy: IfNotPresent
            repository: rancher/kubectl
            tag: v1.20.2
          rbac:
            create: true
            pspAnnotations: {}
            pspEnabled: true
            userRoles:
              aggregateToDefaultRoles: true
              create: true
        grafana:
          additionalDataSources: "null"
          admin:
            existingSecret: "null"
            passwordKey: admin-password
            userKey: admin-user
          adminPassword: prom-operator
          adminUser: admin
          affinity: {}
          autoscaling:
            enabled: false
          containerSecurityContext: {}
          dashboardProviders: {}
          dashboards: {}
          dashboardsConfigMaps: {}
          datasources: {}
          defaultDashboards:
            cleanupOnUninstall: false
            namespace: cattle-dashboards
            useExistingNamespace: false
          defaultDashboardsEnabled: true
          defaultDashboardsTimezone: utc
          deploymentStrategy:
            type: Recreate
          downloadDashboards:
            env: {}
            envFromSecret: "null"
            resources: {}
          downloadDashboardsImage:
            pullPolicy: IfNotPresent
            repository: rancher/mirrored-curlimages-curl
            sha: "null"
            tag: 7.77.0
          enableServiceLinks: true
          enabled: true
          env: {}
          envFromSecret: "null"
          envFromSecrets: "null"
          envRenderSecret: {}
          envValueFrom: {}
          extraConfigmapMounts: "null"
          extraContainerVolumes:
          - emptyDir: {}
            name: nginx-home
          - configMap:
              items:
              - key: nginx.conf
                mode: 438
                path: nginx.conf
              name: grafana-nginx-proxy-config
            name: grafana-nginx
          extraContainers: |
            - name: grafana-proxy
              args:
              - nginx
              - -g
              - daemon off;
              - -c
              - /nginx/nginx.conf
              image: "{{ template "system_default_registry" . }}{{ .Values.proxy.image.repository }}:{{ .Values.proxy.image.tag }}"
              ports:
              - containerPort: 8080
                name: nginx-http
                protocol: TCP
              volumeMounts:
              - mountPath: /nginx
                name: grafana-nginx
              - mountPath: /var/cache/nginx
                name: nginx-home
              securityContext:
                runAsUser: 101
                runAsGroup: 101
          extraEmptyDirMounts: "null"
          extraExposePorts: "null"
          extraInitContainers: "null"
          extraLabels: {}
          extraSecretMounts: "null"
          extraVolumeMounts: "null"
          forceDeployDashboards: false
          forceDeployDatasources: false
          global:
            cattle:
              rkeWindowsPathPrefix: c:\
              systemDefaultRegistry: "null"
              windows:
                enabled: false
            fleet:
              clusterLabels:
                management.cattle.io/cluster-display-name: local
                management.cattle.io/cluster-name: local
                provider.cattle.io: harvester
            imagePullSecrets: "null"
            kubectl:
              pullPolicy: IfNotPresent
              repository: rancher/kubectl
              tag: v1.20.2
            rbac:
              create: true
              pspAnnotations: {}
              pspEnabled: true
              userRoles:
                aggregateToDefaultRoles: true
                create: true
          grafana.ini:
            analytics:
              check_for_updates: true
            auth:
              disable_login_form: false
            auth.anonymous:
              enabled: true
              org_role: Viewer
            auth.basic:
              enabled: false
            dashboards:
              default_home_dashboard_path: /tmp/dashboards/rancher-default-home.json
            grafana_net:
              url: https://grafana.net
            log:
              mode: console
            paths:
              data: /var/lib/grafana/
              logs: /var/log/grafana
              plugins: /var/lib/grafana/plugins
              provisioning: /etc/grafana/provisioning
            security:
              allow_embedding: true
            users:
              auto_assign_org_role: Viewer
          hostAliases: "null"
          image:
            pullPolicy: IfNotPresent
            repository: rancher/mirrored-grafana-grafana
            sha: "null"
            tag: 7.5.11
          imageRenderer:
            enabled: false
            env:
              HTTP_HOST: 0.0.0.0
            grafanaSubPath: "null"
            hostAliases: "null"
            image:
              pullPolicy: Always
              repository: rancher/mirrored-grafana-grafana-image-renderer
              sha: "null"
              tag: 3.0.1
            networkPolicy:
              limitEgress: false
              limitIngress: true
            podPortName: http
            priorityClassName: "null"
            replicas: 1
            resources: {}
            revisionHistoryLimit: 10
            securityContext: {}
            service:
              enabled: true
              port: 8081
              portName: http
              targetPort: 8081
            serviceAccountName: "null"
          ingress:
            annotations: {}
            enabled: false
            extraPaths: "null"
            hosts: "null"
            labels: {}
            path: /
            pathType: Prefix
            tls: null
          initChownData:
            enabled: true
            image:
              pullPolicy: IfNotPresent
              repository: rancher/mirrored-library-busybox
              sha: "null"
              tag: 1.31.1
            resources: {}
          ldap:
            config: "null"
            enabled: false
            existingSecret: "null"
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
            initialDelaySeconds: 60
            timeoutSeconds: 30
          namespaceOverride: "null"
          nodeSelector: {}
          notifiers: {}
          persistence:
            accessModes:
            - ReadWriteOnce
            enabled: false
            finalizers:
            - kubernetes.io/pvc-protection
            inMemory:
              enabled: false
            size: 10Gi
            type: pvc
          plugins: "null"
          podDisruptionBudget: {}
          podPortName: grafana
          proxy:
            image:
              repository: rancher/mirrored-library-nginx
              tag: 1.21.1-alpine
          rbac:
            create: true
            extraClusterRoleRules: "null"
            extraRoleRules: "null"
            namespaced: false
            pspAnnotations: {}
            pspEnabled: true
          readinessProbe:
            httpGet:
              path: /api/health
              port: 3000
          replicas: 1
          resources:
            limits:
              cpu: 200m
              memory: 200Mi
            requests:
              cpu: 100m
              memory: 100Mi
          revisionHistoryLimit: 10
          securityContext:
            fsGroup: 472
            runAsGroup: 472
            runAsNonRoot: true
            runAsUser: 472
          service:
            annotations: {}
            enabled: true
            labels: {}
            nodePort: 30950
            port: 80
            portName: nginx-http
            targetPort: 8080
            type: ClusterIP
          serviceAccount:
            autoMount: true
            create: true
          serviceMonitor:
            enabled: false
            interval: "null"
            labels: {}
            metricRelabelings: "null"
            path: /metrics
            relabelings: "null"
            scheme: http
            scrapeTimeout: 30s
            selfMonitor: true
            tlsConfig: {}
          sidecar:
            dashboards:
              SCProvider: true
              annotations: {}
              enabled: true
              folder: /tmp/dashboards
              label: grafana_dashboard
              multicluster:
                etcd:
                  enabled: false
                global:
                  enabled: false
              provider:
                allowUiUpdates: false
                disableDelete: false
                folder: "null"
                foldersFromFilesStructure: false
                name: sidecarProvider
                orgid: 1
                type: file
              resource: both
              searchNamespace: cattle-dashboards
            datasources:
              annotations: {}
              createPrometheusReplicasDatasources: false
              defaultDatasourceEnabled: true
              enabled: true
              label: grafana_datasource
              resource: both
            enableUniqueFilenames: false
            image:
              repository: rancher/mirrored-kiwigrid-k8s-sidecar
              sha: "null"
              tag: 1.12.3
            imagePullPolicy: IfNotPresent
            notifiers:
              enabled: false
              label: grafana_notifier
              resource: both
            resources: {}
          smtp:
            existingSecret: "null"
            passwordKey: password
            userKey: user
          testFramework:
            enabled: false
            image: rancher/mirrored-bats-bats
            imagePullPolicy: IfNotPresent
            securityContext:
              runAsNonRoot: true
              runAsUser: 1000
            tag: v1.1.0
          tolerations: null
        hardenedKubelet:
          clients:
            affinity: {}
            command:
            - pushprox-client
            copyCertsImage:
              repository: rancher/mirrored-library-busybox
              tag: 1.31.1
            deployment:
              enabled: false
              replicas: 0
            enabled: true
            https:
              caCertFile: "null"
              certDir: "null"
              certFile: "null"
              enabled: true
              insecureSkipVerify: true
              keyFile: "null"
              useServiceAccountCredentials: true
            image:
              repository: rancher/pushprox-client
              tag: v0.1.0-rancher2-client
            nodeSelector: {}
            port: 10015
            proxyUrl: "null"
            rbac:
              additionalRules:
              - nonResourceURLs:
                - /metrics/cadvisor
                verbs:
                - get
              - apiGroups:
                - "null"
                resources:
                - nodes/metrics
                verbs:
                - get
            resources: {}
            tolerations:
            - effect: NoExecute
              operator: Exists
            - effect: NoSchedule
              operator: Exists
            useLocalhost: true
          component: kubelet
          enabled: false
          global:
            cattle:
              rkeWindowsPathPrefix: c:\
              systemDefaultRegistry: "null"
              windows:
                enabled: false
            fleet:
              clusterLabels:
                management.cattle.io/cluster-display-name: local
                management.cattle.io/cluster-name: local
                provider.cattle.io: harvester
            imagePullSecrets: "null"
            kubectl:
              pullPolicy: IfNotPresent
              repository: rancher/kubectl
              tag: v1.20.2
            rbac:
              create: true
              pspAnnotations: {}
              pspEnabled: true
              userRoles:
                aggregateToDefaultRoles: true
                create: true
          metricsPort: 10250
          namespaceOverride: "null"
          proxy:
            command:
            - pushprox-proxy
            enabled: true
            image:
              repository: rancher/pushprox-proxy
              tag: v0.1.0-rancher2-proxy
            nodeSelector: {}
            port: 8080
            resources: {}
            tolerations: null
          serviceMonitor:
            enabled: true
            endpoints:
            - honorLabels: true
              port: metrics
              relabelings:
              - sourceLabels:
                - __metrics_path__
                targetLabel: metrics_path
            - honorLabels: true
              path: /metrics/cadvisor
              port: metrics
              relabelings:
              - sourceLabels:
                - __metrics_path__
                targetLabel: metrics_path
            - honorLabels: true
              path: /metrics/probes
              port: metrics
              relabelings:
              - sourceLabels:
                - __metrics_path__
                targetLabel: metrics_path
        hardenedNodeExporter:
          clients:
            affinity: {}
            command:
            - pushprox-client
            copyCertsImage:
              repository: rancher/mirrored-library-busybox
              tag: 1.31.1
            deployment:
              enabled: false
              replicas: 0
            enabled: true
            https:
              caCertFile: "null"
              certDir: "null"
              certFile: "null"
              enabled: false
              insecureSkipVerify: false
              keyFile: "null"
              useServiceAccountCredentials: false
            image:
              repository: rancher/pushprox-client
              tag: v0.1.0-rancher2-client
            nodeSelector: {}
            port: 10016
            proxyUrl: "null"
            rbac:
              additionalRules: null
            resources: {}
            tolerations:
            - effect: NoExecute
              operator: Exists
            - effect: NoSchedule
              operator: Exists
            useLocalhost: true
          component: node-exporter
          enabled: false
          global:
            cattle:
              rkeWindowsPathPrefix: c:\
              systemDefaultRegistry: "null"
              windows:
                enabled: false
            fleet:
              clusterLabels:
                management.cattle.io/cluster-display-name: local
                management.cattle.io/cluster-name: local
                provider.cattle.io: harvester
            imagePullSecrets: "null"
            kubectl:
              pullPolicy: IfNotPresent
              repository: rancher/kubectl
              tag: v1.20.2
            rbac:
              create: true
              pspAnnotations: {}
              pspEnabled: true
              userRoles:
                aggregateToDefaultRoles: true
                create: true
          metricsPort: 9796
          namespaceOverride: "null"
          proxy:
            command:
            - pushprox-proxy
            enabled: true
            image:
              repository: rancher/pushprox-proxy
              tag: v0.1.0-rancher2-proxy
            nodeSelector: {}
            port: 8080
            resources: {}
            tolerations: null
          serviceMonitor:
            enabled: true
            endpoints:
            - port: metrics
        ingressNginx:
          enabled: false
          namespace: ingress-nginx
          service:
            port: 9913
            targetPort: 10254
          serviceMonitor:
            interval: "null"
            metricRelabelings: "null"
            proxyUrl: "null"
            relabelings: null
        k3sServer:
          clients:
            affinity: {}
            command:
            - pushprox-client
            copyCertsImage:
              repository: rancher/mirrored-library-busybox
              tag: 1.31.1
            deployment:
              enabled: false
              replicas: 0
            enabled: true
            https:
              caCertFile: "null"
              certDir: "null"
              certFile: "null"
              enabled: true
              insecureSkipVerify: true
              keyFile: "null"
              useServiceAccountCredentials: true
            image:
              repository: rancher/pushprox-client
              tag: v0.1.0-rancher2-client
            nodeSelector: {}
            port: 10013
            proxyUrl: "null"
            rbac:
              additionalRules:
              - nonResourceURLs:
                - /metrics/cadvisor
                verbs:
                - get
              - apiGroups:
                - "null"
                resources:
                - nodes/metrics
                verbs:
                - get
            resources: {}
            tolerations:
            - effect: NoExecute
              operator: Exists
            - effect: NoSchedule
              operator: Exists
            useLocalhost: true
          component: k3s-server
          enabled: false
          global:
            cattle:
              rkeWindowsPathPrefix: c:\
              systemDefaultRegistry: "null"
              windows:
                enabled: false
            fleet:
              clusterLabels:
                management.cattle.io/cluster-display-name: local
                management.cattle.io/cluster-name: local
                provider.cattle.io: harvester
            imagePullSecrets: "null"
            kubectl:
              pullPolicy: IfNotPresent
              repository: rancher/kubectl
              tag: v1.20.2
            rbac:
              create: true
              pspAnnotations: {}
              pspEnabled: true
              userRoles:
                aggregateToDefaultRoles: true
                create: true
          metricsPort: 10250
          namespaceOverride: "null"
          proxy:
            command:
            - pushprox-proxy
            enabled: true
            image:
              repository: rancher/pushprox-proxy
              tag: v0.1.0-rancher2-proxy
            nodeSelector: {}
            port: 8080
            resources: {}
            tolerations: null
          serviceMonitor:
            enabled: true
            endpoints:
            - honorLabels: true
              port: metrics
              relabelings:
              - sourceLabels:
                - __metrics_path__
                targetLabel: metrics_path
            - honorLabels: true
              path: /metrics/cadvisor
              port: metrics
              relabelings:
              - sourceLabels:
                - __metrics_path__
                targetLabel: metrics_path
            - honorLabels: true
              path: /metrics/probes
              port: metrics
              relabelings:
              - sourceLabels:
                - __metrics_path__
                targetLabel: metrics_path
        kube-state-metrics:
          affinity: {}
          autosharding:
            enabled: false
          collectors:
          - certificatesigningrequests
          - configmaps
          - cronjobs
          - daemonsets
          - deployments
          - endpoints
          - horizontalpodautoscalers
          - ingresses
          - jobs
          - limitranges
          - mutatingwebhookconfigurations
          - namespaces
          - networkpolicies
          - nodes
          - persistentvolumeclaims
          - persistentvolumes
          - poddisruptionbudgets
          - pods
          - replicasets
          - replicationcontrollers
          - resourcequotas
          - secrets
          - services
          - statefulsets
          - storageclasses
          - validatingwebhookconfigurations
          - volumeattachments
          containerSecurityContext: {}
          customLabels: {}
          extraArgs: "null"
          global:
            cattle:
              rkeWindowsPathPrefix: c:\
              systemDefaultRegistry: "null"
              windows:
                enabled: false
            fleet:
              clusterLabels:
                management.cattle.io/cluster-display-name: local
                management.cattle.io/cluster-name: local
                provider.cattle.io: harvester
            imagePullSecrets: "null"
            kubectl:
              pullPolicy: IfNotPresent
              repository: rancher/kubectl
              tag: v1.20.2
            rbac:
              create: true
              pspAnnotations: {}
              pspEnabled: true
              userRoles:
                aggregateToDefaultRoles: true
                create: true
          hostNetwork: false
          image:
            pullPolicy: IfNotPresent
            repository: rancher/mirrored-kube-state-metrics-kube-state-metrics
            tag: v2.2.0
          imagePullSecrets: "null"
          kubeTargetVersionOverride: "null"
          kubeconfig:
            enabled: false
          metricAllowlist: "null"
          metricAnnotationsAllowList: "null"
          metricDenylist: "null"
          metricLabelsAllowlist: "null"
          namespaceOverride: "null"
          namespaces: "null"
          nodeSelector: {}
          podAnnotations: {}
          podDisruptionBudget: {}
          podSecurityPolicy:
            additionalVolumes: "null"
            annotations: {}
            enabled: true
          prometheus:
            monitor:
              additionalLabels: {}
              enabled: false
              honorLabels: false
              metricRelabelings: "null"
              namespace: "null"
              relabelings: null
          prometheusScrape: true
          rbac:
            create: true
            useClusterRole: true
          replicas: 1
          resources:
            limits:
              cpu: 100m
              memory: 200Mi
            requests:
              cpu: 100m
              memory: 130Mi
          securityContext:
            enabled: true
            fsGroup: 65534
            runAsGroup: 65534
            runAsNonRoot: true
            runAsUser: 65534
          selfMonitor:
            enabled: false
          service:
            annotations: {}
            loadBalancerIP: "null"
            nodePort: 0
            port: 8080
            type: ClusterIP
          serviceAccount:
            annotations: {}
            create: true
            imagePullSecrets: null
          tolerations: null
        kubeAdmControllerManager:
          clients:
            affinity: {}
            command:
            - pushprox-client
            copyCertsImage:
              repository: rancher/mirrored-library-busybox
              tag: 1.31.1
            deployment:
              enabled: false
              replicas: 0
            enabled: true
            https:
              caCertFile: "null"
              certDir: "null"
              certFile: "null"
              enabled: true
              insecureSkipVerify: true
              keyFile: "null"
              useServiceAccountCredentials: true
            image:
              repository: rancher/pushprox-client
              tag: v0.1.0-rancher2-client
            nodeSelector:
              node-role.kubernetes.io/master: "null"
            port: 10011
            proxyUrl: "null"
            rbac:
              additionalRules: null
            resources: {}
            tolerations:
            - effect: NoExecute
              operator: Exists
            - effect: NoSchedule
              operator: Exists
            useLocalhost: true
          component: kube-controller-manager
          enabled: false
          global:
            cattle:
              rkeWindowsPathPrefix: c:\
              systemDefaultRegistry: "null"
              windows:
                enabled: false
            fleet:
              clusterLabels:
                management.cattle.io/cluster-display-name: local
                management.cattle.io/cluster-name: local
                provider.cattle.io: harvester
            imagePullSecrets: "null"
            kubectl:
              pullPolicy: IfNotPresent
              repository: rancher/kubectl
              tag: v1.20.2
            rbac:
              create: true
              pspAnnotations: {}
              pspEnabled: true
              userRoles:
                aggregateToDefaultRoles: true
                create: true
          metricsPort: 10257
          namespaceOverride: "null"
          proxy:
            command:
            - pushprox-proxy
            enabled: true
            image:
              repository: rancher/pushprox-proxy
              tag: v0.1.0-rancher2-proxy
            nodeSelector: {}
            port: 8080
            resources: {}
            tolerations: null
          serviceMonitor:
            enabled: true
            endpoints:
            - port: metrics
        kubeAdmEtcd:
          clients:
            affinity: {}
            command:
            - pushprox-client
            copyCertsImage:
              repository: rancher/mirrored-library-busybox
              tag: 1.31.1
            deployment:
              enabled: false
              replicas: 0
            enabled: true
            https:
              caCertFile: "null"
              certDir: "null"
              certFile: "null"
              enabled: false
              insecureSkipVerify: false
              keyFile: "null"
              useServiceAccountCredentials: false
            image:
              repository: rancher/pushprox-client
              tag: v0.1.0-rancher2-client
            nodeSelector:
              node-role.kubernetes.io/master: "null"
            port: 10014
            proxyUrl: "null"
            rbac:
              additionalRules: null
            resources: {}
            tolerations:
            - effect: NoExecute
              operator: Exists
            - effect: NoSchedule
              operator: Exists
            useLocalhost: true
          component: kube-etcd
          enabled: false
          global:
            cattle:
              rkeWindowsPathPrefix: c:\
              systemDefaultRegistry: "null"
              windows:
                enabled: false
            fleet:
              clusterLabels:
                management.cattle.io/cluster-display-name: local
                management.cattle.io/cluster-name: local
                provider.cattle.io: harvester
            imagePullSecrets: "null"
            kubectl:
              pullPolicy: IfNotPresent
              repository: rancher/kubectl
              tag: v1.20.2
            rbac:
              create: true
              pspAnnotations: {}
              pspEnabled: true
              userRoles:
                aggregateToDefaultRoles: true
                create: true
          metricsPort: 2381
          namespaceOverride: "null"
          proxy:
            command:
            - pushprox-proxy
            enabled: true
            image:
              repository: rancher/pushprox-proxy
              tag: v0.1.0-rancher2-proxy
            nodeSelector: {}
            port: 8080
            resources: {}
            tolerations: null
          serviceMonitor:
            enabled: true
            endpoints:
            - port: metrics
        kubeAdmProxy:
          clients:
            affinity: {}
            command:
            - pushprox-client
            copyCertsImage:
              repository: rancher/mirrored-library-busybox
              tag: 1.31.1
            deployment:
              enabled: false
              replicas: 0
            enabled: true
            https:
              caCertFile: "null"
              certDir: "null"
              certFile: "null"
              enabled: false
              insecureSkipVerify: false
              keyFile: "null"
              useServiceAccountCredentials: false
            image:
              repository: rancher/pushprox-client
              tag: v0.1.0-rancher2-client
            nodeSelector: {}
            port: 10013
            proxyUrl: "null"
            rbac:
              additionalRules: null
            resources: {}
            tolerations:
            - effect: NoExecute
              operator: Exists
            - effect: NoSchedule
              operator: Exists
            useLocalhost: true
          component: kube-proxy
          enabled: false
          global:
            cattle:
              rkeWindowsPathPrefix: c:\
              systemDefaultRegistry: "null"
              windows:
                enabled: false
            fleet:
              clusterLabels:
                management.cattle.io/cluster-display-name: local
                management.cattle.io/cluster-name: local
                provider.cattle.io: harvester
            imagePullSecrets: "null"
            kubectl:
              pullPolicy: IfNotPresent
              repository: rancher/kubectl
              tag: v1.20.2
            rbac:
              create: true
              pspAnnotations: {}
              pspEnabled: true
              userRoles:
                aggregateToDefaultRoles: true
                create: true
          metricsPort: 10249
          namespaceOverride: "null"
          proxy:
            command:
            - pushprox-proxy
            enabled: true
            image:
              repository: rancher/pushprox-proxy
              tag: v0.1.0-rancher2-proxy
            nodeSelector: {}
            port: 8080
            resources: {}
            tolerations: null
          serviceMonitor:
            enabled: true
            endpoints:
            - port: metrics
        kubeAdmScheduler:
          clients:
            affinity: {}
            command:
            - pushprox-client
            copyCertsImage:
              repository: rancher/mirrored-library-busybox
              tag: 1.31.1
            deployment:
              enabled: false
              replicas: 0
            enabled: true
            https:
              caCertFile: "null"
              certDir: "null"
              certFile: "null"
              enabled: true
              insecureSkipVerify: true
              keyFile: "null"
              useServiceAccountCredentials: true
            image:
              repository: rancher/pushprox-client
              tag: v0.1.0-rancher2-client
            nodeSelector:
              node-role.kubernetes.io/master: "null"
            port: 10012
            proxyUrl: "null"
            rbac:
              additionalRules: null
            resources: {}
            tolerations:
            - effect: NoExecute
              operator: Exists
            - effect: NoSchedule
              operator: Exists
            useLocalhost: true
          component: kube-scheduler
          enabled: false
          global:
            cattle:
              rkeWindowsPathPrefix: c:\
              systemDefaultRegistry: "null"
              windows:
                enabled: false
            fleet:
              clusterLabels:
                management.cattle.io/cluster-display-name: local
                management.cattle.io/cluster-name: local
                provider.cattle.io: harvester
            imagePullSecrets: "null"
            kubectl:
              pullPolicy: IfNotPresent
              repository: rancher/kubectl
              tag: v1.20.2
            rbac:
              create: true
              pspAnnotations: {}
              pspEnabled: true
              userRoles:
                aggregateToDefaultRoles: true
                create: true
          metricsPort: 10259
          namespaceOverride: "null"
          proxy:
            command:
            - pushprox-proxy
            enabled: true
            image:
              repository: rancher/pushprox-proxy
              tag: v0.1.0-rancher2-proxy
            nodeSelector: {}
            port: 8080
            resources: {}
            tolerations: null
          serviceMonitor:
            enabled: true
            endpoints:
            - port: metrics
        kubeApiServer:
          enabled: true
          serviceMonitor:
            interval: "null"
            jobLabel: component
            metricRelabelings: "null"
            proxyUrl: "null"
            relabelings: "null"
            selector:
              matchLabels:
                component: apiserver
                provider: kubernetes
          tlsConfig:
            insecureSkipVerify: false
            serverName: kubernetes
        kubeControllerManager:
          enabled: false
          endpoints: "null"
          service:
            enabled: true
            port: 10252
            targetPort: 10252
          serviceMonitor:
            enabled: true
            https: false
            insecureSkipVerify: "null"
            interval: "null"
            metricRelabelings: "null"
            proxyUrl: "null"
            relabelings: "null"
            serverName: null
        kubeDns:
          enabled: false
          service:
            dnsmasq:
              port: 10054
              targetPort: 10054
            skydns:
              port: 10055
              targetPort: 10055
          serviceMonitor:
            dnsmasqMetricRelabelings: "null"
            dnsmasqRelabelings: "null"
            interval: "null"
            metricRelabelings: "null"
            proxyUrl: "null"
            relabelings: null
        kubeEtcd:
          enabled: false
          endpoints: "null"
          service:
            enabled: true
            port: 2379
            targetPort: 2379
          serviceMonitor:
            caFile: "null"
            certFile: "null"
            enabled: true
            insecureSkipVerify: false
            interval: "null"
            keyFile: "null"
            metricRelabelings: "null"
            proxyUrl: "null"
            relabelings: "null"
            scheme: http
            serverName: "null"
        kubeProxy:
          enabled: false
          endpoints: "null"
          service:
            enabled: true
            port: 10249
            targetPort: 10249
          serviceMonitor:
            enabled: true
            https: false
            interval: "null"
            metricRelabelings: "null"
            proxyUrl: "null"
            relabelings: null
        kubeScheduler:
          enabled: false
          endpoints: "null"
          service:
            enabled: true
            port: 10251
            targetPort: 10251
          serviceMonitor:
            enabled: true
            https: false
            insecureSkipVerify: "null"
            interval: "null"
            metricRelabelings: "null"
            proxyUrl: "null"
            relabelings: "null"
            serverName: null
        kubeStateMetrics:
          enabled: true
          serviceMonitor:
            honorLabels: true
            interval: "null"
            metricRelabelings: "null"
            proxyUrl: "null"
            relabelings: "null"
            scrapeTimeout: "null"
            selectorOverride: {}
            selfMonitor:
              enabled: false
        kubeTargetVersionOverride: "null"
        kubeVersionOverride: "null"
        kubelet:
          enabled: true
          namespace: kube-system
          serviceMonitor:
            cAdvisor: true
            cAdvisorMetricRelabelings: "null"
            cAdvisorRelabelings:
            - sourceLabels:
              - __metrics_path__
              targetLabel: metrics_path
            https: true
            interval: "null"
            metricRelabelings: "null"
            probes: true
            probesMetricRelabelings: "null"
            probesRelabelings:
            - sourceLabels:
              - __metrics_path__
              targetLabel: metrics_path
            proxyUrl: "null"
            relabelings:
            - sourceLabels:
              - __metrics_path__
              targetLabel: metrics_path
            resource: false
            resourcePath: /metrics/resource/v1alpha1
            resourceRelabelings:
            - sourceLabels:
              - __metrics_path__
              targetLabel: metrics_path
        nameOverride: rancher-monitoring
        namespaceOverride: cattle-monitoring-system
        nodeExporter:
          enabled: true
          jobLabel: jobLabel
          serviceMonitor:
            interval: "null"
            metricRelabelings: "null"
            proxyUrl: "null"
            relabelings: "null"
            scrapeTimeout: "null"
        prometheus:
          additionalPodMonitors: "null"
          additionalRulesForClusterRole: "null"
          additionalServiceMonitors: "null"
          annotations: {}
          enabled: true
          extraSecret:
            annotations: {}
            data: {}
          ingress:
            annotations: {}
            enabled: false
            hosts: "null"
            labels: {}
            paths: "null"
            tls: null
          ingressPerReplica:
            annotations: {}
            enabled: false
            hostDomain: "null"
            hostPrefix: "null"
            labels: {}
            paths: "null"
            tlsSecretName: "null"
            tlsSecretPerReplica:
              enabled: false
              prefix: prometheus
          podDisruptionBudget:
            enabled: false
            maxUnavailable: "null"
            minAvailable: 1
          podSecurityPolicy:
            allowedCapabilities: "null"
            allowedHostPaths: "null"
            volumes: null
          prometheusSpec:
            additionalAlertManagerConfigs: "null"
            additionalAlertManagerConfigsSecret: {}
            additionalAlertRelabelConfigs: "null"
            additionalPrometheusSecretsAnnotations: {}
            additionalRemoteRead: "null"
            additionalRemoteWrite: "null"
            additionalScrapeConfigs: "null"
            additionalScrapeConfigsSecret: {}
            affinity: {}
            alertingEndpoints: "null"
            allowOverlappingBlocks: false
            apiserverConfig: {}
            arbitraryFSAccessThroughSMs: false
            configMaps: "null"
            containers: |
              - name: prometheus-proxy
                args:
                - nginx
                - -g
                - daemon off;
                - -c
                - /nginx/nginx.conf
                image: "{{ template "system_default_registry" . }}{{ .Values.prometheus.prometheusSpec.proxy.image.repository }}:{{ .Values.prometheus.prometheusSpec.proxy.image.tag }}"
                ports:
                - containerPort: 8081
                  name: nginx-http
                  protocol: TCP
                volumeMounts:
                - mountPath: /nginx
                  name: prometheus-nginx
                - mountPath: /var/cache/nginx
                  name: nginx-home
                securityContext:
                  runAsUser: 101
                  runAsGroup: 101
            disableCompaction: false
            enableAdminAPI: false
            enableFeatures: "null"
            enforcedLabelLimit: false
            enforcedLabelNameLengthLimit: false
            enforcedLabelValueLengthLimit: false
            enforcedNamespaceLabel: "null"
            enforcedSampleLimit: false
            enforcedTargetLimit: false
            evaluationInterval: "null"
            externalLabels: {}
            externalUrl: "null"
            ignoreNamespaceSelectors: false
            image:
              repository: rancher/mirrored-prometheus-prometheus
              sha: "null"
              tag: v2.28.1
            initContainers: "null"
            listenLocal: false
            logFormat: logfmt
            logLevel: info
            nodeSelector: {}
            overrideHonorLabels: false
            overrideHonorTimestamps: false
            paused: false
            podAntiAffinity: "null"
            podAntiAffinityTopologyKey: kubernetes.io/hostname
            podMetadata: {}
            podMonitorNamespaceSelector: {}
            podMonitorSelector: {}
            podMonitorSelectorNilUsesHelmValues: false
            portName: nginx-http
            priorityClassName: "null"
            probeNamespaceSelector: {}
            probeSelector: {}
            probeSelectorNilUsesHelmValues: true
            prometheusExternalLabelName: "null"
            prometheusExternalLabelNameClear: false
            prometheusRulesExcludedFromEnforce: "null"
            proxy:
              image:
                repository: rancher/mirrored-library-nginx
                tag: 1.21.1-alpine
            query: {}
            queryLogFile: false
            remoteRead: "null"
            remoteWrite: "null"
            remoteWriteDashboards: false
            replicaExternalLabelName: "null"
            replicaExternalLabelNameClear: false
            replicas: 1
            resources:
              limits:
                cpu: 1000m
                memory: 3000Mi
              requests:
                cpu: 750m
                memory: 750Mi
            retention: 10d
            retentionSize: "null"
            routePrefix: /
            ruleNamespaceSelector: {}
            ruleSelector: {}
            ruleSelectorNilUsesHelmValues: false
            scrapeInterval: "null"
            scrapeTimeout: "null"
            secrets: "null"
            securityContext:
              fsGroup: 2000
              runAsGroup: 2000
              runAsNonRoot: true
              runAsUser: 1000
            serviceMonitorNamespaceSelector: {}
            serviceMonitorSelector: {}
            serviceMonitorSelectorNilUsesHelmValues: false
            shards: 1
            storageSpec: {}
            thanos: {}
            tolerations: "null"
            topologySpreadConstraints: "null"
            volumeMounts: "null"
            volumes:
            - emptyDir: {}
              name: nginx-home
            - configMap:
                defaultMode: 438
                name: prometheus-nginx-proxy-config
              name: prometheus-nginx
            walCompression: false
            web: {}
          service:
            annotations: {}
            clusterIP: "null"
            externalIPs: "null"
            labels: {}
            loadBalancerIP: "null"
            loadBalancerSourceRanges: "null"
            nodePort: 30090
            port: 9090
            sessionAffinity: "null"
            targetPort: 8081
            type: ClusterIP
          serviceAccount:
            annotations: {}
            create: true
            name: "null"
          serviceMonitor:
            bearerTokenFile: "null"
            interval: "null"
            metricRelabelings: "null"
            relabelings: "null"
            scheme: "null"
            selfMonitor: true
            tlsConfig: {}
          servicePerReplica:
            annotations: {}
            enabled: false
            loadBalancerSourceRanges: "null"
            nodePort: 30091
            port: 9090
            targetPort: 9090
            type: ClusterIP
          thanosIngress:
            annotations: {}
            enabled: false
            hosts: "null"
            labels: {}
            nodePort: 30901
            paths: "null"
            servicePort: 10901
            tls: null
          thanosService:
            annotations: {}
            clusterIP: None
            enabled: false
            httpNodePort: 30902
            httpPort: 10902
            httpPortName: http
            labels: {}
            nodePort: 30901
            port: 10901
            portName: grpc
            targetHttpPort: http
            targetPort: grpc
            type: ClusterIP
          thanosServiceExternal:
            annotations: {}
            enabled: false
            httpNodePort: 30902
            httpPort: 10902
            httpPortName: http
            labels: {}
            loadBalancerIP: "null"
            loadBalancerSourceRanges: "null"
            nodePort: 30901
            port: 10901
            portName: grpc
            targetHttpPort: http
            targetPort: grpc
            type: LoadBalancer
          thanosServiceMonitor:
            bearerTokenFile: "null"
            enabled: false
            interval: "null"
            metricRelabelings: "null"
            relabelings: "null"
            scheme: "null"
            tlsConfig: {}
        prometheus-adapter:
          affinity: {}
          certManager:
            caCertDuration: 43800h
            certDuration: 8760h
            enabled: false
          dnsConfig: {}
          enabled: true
          extraArguments: "null"
          extraVolumeMounts: "null"
          extraVolumes: "null"
          global:
            cattle:
              rkeWindowsPathPrefix: c:\
              systemDefaultRegistry: "null"
              windows:
                enabled: false
            fleet:
              clusterLabels:
                management.cattle.io/cluster-display-name: local
                management.cattle.io/cluster-name: local
                provider.cattle.io: harvester
            imagePullSecrets: "null"
            kubectl:
              pullPolicy: IfNotPresent
              repository: rancher/kubectl
              tag: v1.20.2
            rbac:
              create: true
              pspAnnotations: {}
              pspEnabled: true
              userRoles:
                aggregateToDefaultRoles: true
                create: true
          hostNetwork:
            enabled: false
          image:
            pullPolicy: IfNotPresent
            repository: rancher/mirrored-prometheus-adapter-prometheus-adapter
            tag: v0.9.0
          listenPort: 6443
          logLevel: 4
          metricsRelistInterval: 1m
          nodeSelector: {}
          podAnnotations: {}
          podDisruptionBudget:
            enabled: false
            maxUnavailable: 1
          podLabels: {}
          podSecurityContext:
            fsGroup: 10001
          priorityClassName: "null"
          prometheus:
            path: "null"
            port: 9090
            url: http://rancher-monitoring-prometheus.cattle-monitoring-system.svc
          psp:
            create: true
          rbac:
            create: true
          replicas: 1
          resources: {}
          rules:
            custom: "null"
            default: true
            external: "null"
            resource: {}
          service:
            annotations: {}
            port: 443
            type: ClusterIP
          serviceAccount:
            annotations: {}
            create: true
          strategy:
            rollingUpdate:
              maxSurge: 25%
              maxUnavailable: 25%
            type: RollingUpdate
          tls:
            ca: '# Public CA file that signed the APIService'
            certificate: '# Public key of the APIService'
            enable: false
            key: '# Private key of the APIService'
          tolerations: null
        prometheus-node-exporter:
          affinity: {}
          configmaps: "null"
          containerSecurityContext: {}
          dnsConfig: {}
          endpoints: "null"
          extraArgs:
          - --collector.filesystem.ignored-mount-points=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
          - --collector.filesystem.ignored-fs-types=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
          extraHostVolumeMounts: "null"
          extraInitContainers: "null"
          global:
            cattle:
              rkeWindowsPathPrefix: c:\
              systemDefaultRegistry: "null"
              windows:
                enabled: false
            fleet:
              clusterLabels:
                management.cattle.io/cluster-display-name: local
                management.cattle.io/cluster-name: local
                provider.cattle.io: harvester
            imagePullSecrets: "null"
            kubectl:
              pullPolicy: IfNotPresent
              repository: rancher/kubectl
              tag: v1.20.2
            rbac:
              create: true
              pspAnnotations: {}
              pspEnabled: true
              userRoles:
                aggregateToDefaultRoles: true
                create: true
          hostNetwork: true
          hostPID: true
          hostRootFsMount: true
          image:
            pullPolicy: IfNotPresent
            repository: rancher/mirrored-prometheus-node-exporter
            tag: v1.2.2
          namespaceOverride: "null"
          nodeSelector: {}
          podAnnotations:
            cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
          podLabels:
            jobLabel: node-exporter
          prometheus:
            monitor:
              additionalLabels: {}
              enabled: false
              namespace: "null"
              proxyUrl: "null"
              relabelings: "null"
              scheme: http
              scrapeTimeout: 10s
              tlsConfig: {}
          rbac:
            create: true
            pspAnnotations: {}
            pspEnabled: true
          resources:
            limits:
              cpu: 200m
              memory: 50Mi
            requests:
              cpu: 100m
              memory: 30Mi
          secrets: "null"
          securityContext:
            fsGroup: 65534
            runAsGroup: 65534
            runAsNonRoot: true
            runAsUser: 65534
          service:
            annotations:
              prometheus.io/scrape: "true"
            listenOnAllInterfaces: true
            port: 9796
            portName: metrics
            targetPort: 9796
            type: ClusterIP
          serviceAccount:
            annotations: {}
            automountServiceAccountToken: false
            create: true
            imagePullSecrets: null
          sidecarVolumeMount: "null"
          sidecars: "null"
          tolerations:
          - effect: NoSchedule
            operator: Exists
          - effect: NoExecute
            operator: Exists
          updateStrategy:
            rollingUpdate:
              maxUnavailable: 1
            type: RollingUpdate
        prometheusOperator:
          admissionWebhooks:
            caBundle: "null"
            certManager:
              enabled: false
            enabled: true
            failurePolicy: Fail
            patch:
              affinity: {}
              enabled: true
              image:
                pullPolicy: IfNotPresent
                repository: rancher/mirrored-ingress-nginx-kube-webhook-certgen
                sha: "null"
                tag: v1.0
              nodeSelector: {}
              podAnnotations: {}
              priorityClassName: "null"
              resources: {}
              securityContext:
                runAsGroup: 2000
                runAsNonRoot: true
                runAsUser: 2000
              tolerations: null
          affinity: {}
          alertmanagerInstanceNamespaces: "null"
          configReloaderCpu: 100m
          configReloaderMemory: 50Mi
          denyNamespaces: "null"
          dnsConfig: {}
          enabled: true
          hostNetwork: false
          image:
            pullPolicy: IfNotPresent
            repository: rancher/mirrored-prometheus-operator-prometheus-operator
            sha: "null"
            tag: v0.50.0
          kubeletService:
            enabled: true
            namespace: kube-system
          namespaces: {}
          nodeSelector: {}
          podAnnotations: {}
          podLabels: {}
          prometheusConfigReloaderImage:
            repository: rancher/mirrored-prometheus-operator-prometheus-config-reloader
            sha: "null"
            tag: v0.50.0
          prometheusInstanceNamespaces: "null"
          resources:
            limits:
              cpu: 200m
              memory: 500Mi
            requests:
              cpu: 100m
              memory: 100Mi
          secretFieldSelector: "null"
          securityContext:
            fsGroup: 65534
            runAsGroup: 65534
            runAsNonRoot: true
            runAsUser: 65534
          service:
            additionalPorts: "null"
            annotations: {}
            clusterIP: "null"
            externalIPs: "null"
            labels: {}
            loadBalancerIP: "null"
            loadBalancerSourceRanges: "null"
            nodePort: 30080
            nodePortTls: 30443
            type: ClusterIP
          serviceAccount:
            create: true
            name: "null"
          serviceMonitor:
            interval: "null"
            metricRelabelings: "null"
            relabelings: "null"
            scrapeTimeout: "null"
            selfMonitor: true
          thanosImage:
            repository: rancher/mirrored-thanos-thanos
            sha: "null"
            tag: v0.17.2
          thanosRulerInstanceNamespaces: "null"
          tls:
            enabled: true
            internalPort: 8443
            tlsMinVersion: VersionTLS13
          tolerations: null
        rke2ControllerManager:
          clients:
            affinity: {}
            command:
            - pushprox-client
            copyCertsImage:
              repository: rancher/mirrored-library-busybox
              tag: 1.31.1
            deployment:
              enabled: false
              replicas: 0
            enabled: true
            https:
              caCertFile: "null"
              certDir: "null"
              certFile: "null"
              enabled: false
              insecureSkipVerify: false
              keyFile: "null"
              useServiceAccountCredentials: false
            image:
              repository: rancher/pushprox-client
              tag: v0.1.0-rancher2-client
            nodeSelector:
              node-role.kubernetes.io/master: "true"
            port: 10011
            proxyUrl: "null"
            rbac:
              additionalRules: null
            resources: {}
            tolerations:
            - effect: NoExecute
              operator: Exists
            - effect: NoSchedule
              operator: Exists
            useLocalhost: true
          component: kube-controller-manager
          enabled: false
          global:
            cattle:
              rkeWindowsPathPrefix: c:\
              systemDefaultRegistry: "null"
              windows:
                enabled: false
            fleet:
              clusterLabels:
                management.cattle.io/cluster-display-name: local
                management.cattle.io/cluster-name: local
                provider.cattle.io: harvester
            imagePullSecrets: "null"
            kubectl:
              pullPolicy: IfNotPresent
              repository: rancher/kubectl
              tag: v1.20.2
            rbac:
              create: true
              pspAnnotations: {}
              pspEnabled: true
              userRoles:
                aggregateToDefaultRoles: true
                create: true
          metricsPort: 10252
          namespaceOverride: "null"
          proxy:
            command:
            - pushprox-proxy
            enabled: true
            image:
              repository: rancher/pushprox-proxy
              tag: v0.1.0-rancher2-proxy
            nodeSelector: {}
            port: 8080
            resources: {}
            tolerations: null
          serviceMonitor:
            enabled: true
            endpoints:
            - port: metrics
        rke2Etcd:
          clients:
            affinity: {}
            command:
            - pushprox-client
            copyCertsImage:
              repository: rancher/mirrored-library-busybox
              tag: 1.31.1
            deployment:
              enabled: false
              replicas: 0
            enabled: true
            https:
              caCertFile: "null"
              certDir: "null"
              certFile: "null"
              enabled: false
              insecureSkipVerify: false
              keyFile: "null"
              useServiceAccountCredentials: false
            image:
              repository: rancher/pushprox-client
              tag: v0.1.0-rancher2-client
            nodeSelector:
              node-role.kubernetes.io/etcd: "true"
            port: 10014
            proxyUrl: "null"
            rbac:
              additionalRules: null
            resources: {}
            tolerations:
            - effect: NoExecute
              operator: Exists
            - effect: NoSchedule
              operator: Exists
            useLocalhost: true
          component: kube-etcd
          enabled: false
          global:
            cattle:
              rkeWindowsPathPrefix: c:\
              systemDefaultRegistry: "null"
              windows:
                enabled: false
            fleet:
              clusterLabels:
                management.cattle.io/cluster-display-name: local
                management.cattle.io/cluster-name: local
                provider.cattle.io: harvester
            imagePullSecrets: "null"
            kubectl:
              pullPolicy: IfNotPresent
              repository: rancher/kubectl
              tag: v1.20.2
            rbac:
              create: true
              pspAnnotations: {}
              pspEnabled: true
              userRoles:
                aggregateToDefaultRoles: true
                create: true
          metricsPort: 2381
          namespaceOverride: "null"
          proxy:
            command:
            - pushprox-proxy
            enabled: true
            image:
              repository: rancher/pushprox-proxy
              tag: v0.1.0-rancher2-proxy
            nodeSelector: {}
            port: 8080
            resources: {}
            tolerations: null
          serviceMonitor:
            enabled: true
            endpoints:
            - port: metrics
        rke2IngressNginx:
          clients:
            affinity:
              podAffinity:
                requiredDuringSchedulingIgnoredDuringExecution:
                - labelSelector:
                    matchExpressions:
                    - key: app.kubernetes.io/component
                      operator: In
                      values:
                      - controller
                  namespaces:
                  - kube-system
                  topologyKey: kubernetes.io/hostname
            command:
            - pushprox-client
            copyCertsImage:
              repository: rancher/mirrored-library-busybox
              tag: 1.31.1
            deployment:
              enabled: false
              replicas: 1
            enabled: true
            https:
              caCertFile: "null"
              certDir: "null"
              certFile: "null"
              enabled: false
              insecureSkipVerify: false
              keyFile: "null"
              useServiceAccountCredentials: false
            image:
              repository: rancher/pushprox-client
              tag: v0.1.0-rancher2-client
            nodeSelector: {}
            port: 10015
            proxyUrl: "null"
            rbac:
              additionalRules: null
            resources: {}
            tolerations:
            - effect: NoExecute
              operator: Exists
            - effect: NoSchedule
              operator: Exists
            useLocalhost: true
          component: ingress-nginx
          enabled: false
          global:
            cattle:
              rkeWindowsPathPrefix: c:\
              systemDefaultRegistry: "null"
              windows:
                enabled: false
            fleet:
              clusterLabels:
                management.cattle.io/cluster-display-name: local
                management.cattle.io/cluster-name: local
                provider.cattle.io: harvester
            imagePullSecrets: "null"
            kubectl:
              pullPolicy: IfNotPresent
              repository: rancher/kubectl
              tag: v1.20.2
            rbac:
              create: true
              pspAnnotations: {}
              pspEnabled: true
              userRoles:
                aggregateToDefaultRoles: true
                create: true
          metricsPort: 10254
          namespaceOverride: "null"
          proxy:
            command:
            - pushprox-proxy
            enabled: true
            image:
              repository: rancher/pushprox-proxy
              tag: v0.1.0-rancher2-proxy
            nodeSelector: {}
            port: 8080
            resources: {}
            tolerations: null
          serviceMonitor:
            enabled: true
            endpoints:
            - port: metrics
        rke2Proxy:
          clients:
            affinity: {}
            command:
            - pushprox-client
            copyCertsImage:
              repository: rancher/mirrored-library-busybox
              tag: 1.31.1
            deployment:
              enabled: false
              replicas: 0
            enabled: true
            https:
              caCertFile: "null"
              certDir: "null"
              certFile: "null"
              enabled: false
              insecureSkipVerify: false
              keyFile: "null"
              useServiceAccountCredentials: false
            image:
              repository: rancher/pushprox-client
              tag: v0.1.0-rancher2-client
            nodeSelector: {}
            port: 10013
            proxyUrl: "null"
            rbac:
              additionalRules: null
            resources: {}
            tolerations: "null"
            useLocalhost: true
          component: kube-proxy
          enabled: false
          global:
            cattle:
              rkeWindowsPathPrefix: c:\
              systemDefaultRegistry: "null"
              windows:
                enabled: false
            fleet:
              clusterLabels:
                management.cattle.io/cluster-display-name: local
                management.cattle.io/cluster-name: local
                provider.cattle.io: harvester
            imagePullSecrets: "null"
            kubectl:
              pullPolicy: IfNotPresent
              repository: rancher/kubectl
              tag: v1.20.2
            rbac:
              create: true
              pspAnnotations: {}
              pspEnabled: true
              userRoles:
                aggregateToDefaultRoles: true
                create: true
          metricsPort: 10249
          namespaceOverride: "null"
          proxy:
            command:
            - pushprox-proxy
            enabled: true
            image:
              repository: rancher/pushprox-proxy
              tag: v0.1.0-rancher2-proxy
            nodeSelector: {}
            port: 8080
            resources: {}
            tolerations: null
          serviceMonitor:
            enabled: true
            endpoints:
            - port: metrics
          tolerations:
          - effect: NoExecute
            operator: Exists
          - effect: NoSchedule
            operator: Exists
        rke2Scheduler:
          clients:
            affinity: {}
            command:
            - pushprox-client
            copyCertsImage:
              repository: rancher/mirrored-library-busybox
              tag: 1.31.1
            deployment:
              enabled: false
              replicas: 0
            enabled: true
            https:
              caCertFile: "null"
              certDir: "null"
              certFile: "null"
              enabled: false
              insecureSkipVerify: false
              keyFile: "null"
              useServiceAccountCredentials: false
            image:
              repository: rancher/pushprox-client
              tag: v0.1.0-rancher2-client
            nodeSelector:
              node-role.kubernetes.io/master: "true"
            port: 10012
            proxyUrl: "null"
            rbac:
              additionalRules: null
            resources: {}
            tolerations:
            - effect: NoExecute
              operator: Exists
            - effect: NoSchedule
              operator: Exists
            useLocalhost: true
          component: kube-scheduler
          enabled: false
          global:
            cattle:
              rkeWindowsPathPrefix: c:\
              systemDefaultRegistry: "null"
              windows:
                enabled: false
            fleet:
              clusterLabels:
                management.cattle.io/cluster-display-name: local
                management.cattle.io/cluster-name: local
                provider.cattle.io: harvester
            imagePullSecrets: "null"
            kubectl:
              pullPolicy: IfNotPresent
              repository: rancher/kubectl
              tag: v1.20.2
            rbac:
              create: true
              pspAnnotations: {}
              pspEnabled: true
              userRoles:
                aggregateToDefaultRoles: true
                create: true
          metricsPort: 10251
          namespaceOverride: "null"
          proxy:
            command:
            - pushprox-proxy
            enabled: true
            image:
              repository: rancher/pushprox-proxy
              tag: v0.1.0-rancher2-proxy
            nodeSelector: {}
            port: 8080
            resources: {}
            tolerations: null
          serviceMonitor:
            enabled: true
            endpoints:
            - port: metrics
        rkeControllerManager:
          clients:
            affinity: {}
            command:
            - pushprox-client
            copyCertsImage:
              repository: rancher/mirrored-library-busybox
              tag: 1.31.1
            deployment:
              enabled: false
              replicas: 0
            enabled: true
            https:
              caCertFile: "null"
              certDir: "null"
              certFile: "null"
              enabled: false
              insecureSkipVerify: false
              keyFile: "null"
              useServiceAccountCredentials: false
            image:
              repository: rancher/pushprox-client
              tag: v0.1.0-rancher2-client
            nodeSelector:
              node-role.kubernetes.io/controlplane: "true"
            port: 10011
            proxyUrl: "null"
            rbac:
              additionalRules: null
            resources: {}
            tolerations:
            - effect: NoExecute
              operator: Exists
            - effect: NoSchedule
              operator: Exists
            useLocalhost: true
          component: kube-controller-manager
          enabled: false
          global:
            cattle:
              rkeWindowsPathPrefix: c:\
              systemDefaultRegistry: "null"
              windows:
                enabled: false
            fleet:
              clusterLabels:
                management.cattle.io/cluster-display-name: local
                management.cattle.io/cluster-name: local
                provider.cattle.io: harvester
            imagePullSecrets: "null"
            kubectl:
              pullPolicy: IfNotPresent
              repository: rancher/kubectl
              tag: v1.20.2
            rbac:
              create: true
              pspAnnotations: {}
              pspEnabled: true
              userRoles:
                aggregateToDefaultRoles: true
                create: true
          metricsPort: 10252
          namespaceOverride: "null"
          proxy:
            command:
            - pushprox-proxy
            enabled: true
            image:
              repository: rancher/pushprox-proxy
              tag: v0.1.0-rancher2-proxy
            nodeSelector: {}
            port: 8080
            resources: {}
            tolerations: null
          serviceMonitor:
            enabled: true
            endpoints:
            - port: metrics
        rkeEtcd:
          clients:
            affinity: {}
            command:
            - pushprox-client
            copyCertsImage:
              repository: rancher/mirrored-library-busybox
              tag: 1.31.1
            deployment:
              enabled: false
              replicas: 0
            enabled: true
            https:
              caCertFile: kube-ca.pem
              certDir: /etc/kubernetes/ssl
              certFile: kube-etcd-*.pem
              enabled: true
              insecureSkipVerify: false
              keyFile: kube-etcd-*-key.pem
              useServiceAccountCredentials: false
            image:
              repository: rancher/pushprox-client
              tag: v0.1.0-rancher2-client
            nodeSelector:
              node-role.kubernetes.io/etcd: "true"
            port: 10014
            proxyUrl: "null"
            rbac:
              additionalRules: null
            resources: {}
            tolerations:
            - effect: NoExecute
              operator: Exists
            - effect: NoSchedule
              operator: Exists
            useLocalhost: false
          component: kube-etcd
          enabled: false
          global:
            cattle:
              rkeWindowsPathPrefix: c:\
              systemDefaultRegistry: "null"
              windows:
                enabled: false
            fleet:
              clusterLabels:
                management.cattle.io/cluster-display-name: local
                management.cattle.io/cluster-name: local
                provider.cattle.io: harvester
            imagePullSecrets: "null"
            kubectl:
              pullPolicy: IfNotPresent
              repository: rancher/kubectl
              tag: v1.20.2
            rbac:
              create: true
              pspAnnotations: {}
              pspEnabled: true
              userRoles:
                aggregateToDefaultRoles: true
                create: true
          metricsPort: 2379
          namespaceOverride: "null"
          proxy:
            command:
            - pushprox-proxy
            enabled: true
            image:
              repository: rancher/pushprox-proxy
              tag: v0.1.0-rancher2-proxy
            nodeSelector: {}
            port: 8080
            resources: {}
            tolerations: null
          serviceMonitor:
            enabled: true
            endpoints:
            - port: metrics
        rkeIngressNginx:
          clients:
            affinity: {}
            command:
            - pushprox-client
            copyCertsImage:
              repository: rancher/mirrored-library-busybox
              tag: 1.31.1
            deployment:
              enabled: false
              replicas: 0
            enabled: true
            https:
              caCertFile: "null"
              certDir: "null"
              certFile: "null"
              enabled: false
              insecureSkipVerify: false
              keyFile: "null"
              useServiceAccountCredentials: false
            image:
              repository: rancher/pushprox-client
              tag: v0.1.0-rancher2-client
            nodeSelector:
              node-role.kubernetes.io/worker: "true"
            port: 10015
            proxyUrl: "null"
            rbac:
              additionalRules: null
            resources: {}
            tolerations:
            - effect: NoExecute
              operator: Exists
            - effect: NoSchedule
              operator: Exists
            useLocalhost: true
          component: ingress-nginx
          enabled: false
          global:
            cattle:
              rkeWindowsPathPrefix: c:\
              systemDefaultRegistry: "null"
              windows:
                enabled: false
            fleet:
              clusterLabels:
                management.cattle.io/cluster-display-name: local
                management.cattle.io/cluster-name: local
                provider.cattle.io: harvester
            imagePullSecrets: "null"
            kubectl:
              pullPolicy: IfNotPresent
              repository: rancher/kubectl
              tag: v1.20.2
            rbac:
              create: true
              pspAnnotations: {}
              pspEnabled: true
              userRoles:
                aggregateToDefaultRoles: true
                create: true
          metricsPort: 10254
          namespaceOverride: "null"
          proxy:
            command:
            - pushprox-proxy
            enabled: true
            image:
              repository: rancher/pushprox-proxy
              tag: v0.1.0-rancher2-proxy
            nodeSelector: {}
            port: 8080
            resources: {}
            tolerations: null
          serviceMonitor:
            enabled: true
            endpoints:
            - port: metrics
        rkeProxy:
          clients:
            affinity: {}
            command:
            - pushprox-client
            copyCertsImage:
              repository: rancher/mirrored-library-busybox
              tag: 1.31.1
            deployment:
              enabled: false
              replicas: 0
            enabled: true
            https:
              caCertFile: "null"
              certDir: "null"
              certFile: "null"
              enabled: false
              insecureSkipVerify: false
              keyFile: "null"
              useServiceAccountCredentials: false
            image:
              repository: rancher/pushprox-client
              tag: v0.1.0-rancher2-client
            nodeSelector: {}
            port: 10013
            proxyUrl: "null"
            rbac:
              additionalRules: null
            resources: {}
            tolerations:
            - effect: NoExecute
              operator: Exists
            - effect: NoSchedule
              operator: Exists
            useLocalhost: true
          component: kube-proxy
          enabled: false
          global:
            cattle:
              rkeWindowsPathPrefix: c:\
              systemDefaultRegistry: "null"
              windows:
                enabled: false
            fleet:
              clusterLabels:
                management.cattle.io/cluster-display-name: local
                management.cattle.io/cluster-name: local
                provider.cattle.io: harvester
            imagePullSecrets: "null"
            kubectl:
              pullPolicy: IfNotPresent
              repository: rancher/kubectl
              tag: v1.20.2
            rbac:
              create: true
              pspAnnotations: {}
              pspEnabled: true
              userRoles:
                aggregateToDefaultRoles: true
                create: true
          metricsPort: 10249
          namespaceOverride: "null"
          proxy:
            command:
            - pushprox-proxy
            enabled: true
            image:
              repository: rancher/pushprox-proxy
              tag: v0.1.0-rancher2-proxy
            nodeSelector: {}
            port: 8080
            resources: {}
            tolerations: null
          serviceMonitor:
            enabled: true
            endpoints:
            - port: metrics
        rkeScheduler:
          clients:
            affinity: {}
            command:
            - pushprox-client
            copyCertsImage:
              repository: rancher/mirrored-library-busybox
              tag: 1.31.1
            deployment:
              enabled: false
              replicas: 0
            enabled: true
            https:
              caCertFile: "null"
              certDir: "null"
              certFile: "null"
              enabled: false
              insecureSkipVerify: false
              keyFile: "null"
              useServiceAccountCredentials: false
            image:
              repository: rancher/pushprox-client
              tag: v0.1.0-rancher2-client
            nodeSelector:
              node-role.kubernetes.io/controlplane: "true"
            port: 10012
            proxyUrl: "null"
            rbac:
              additionalRules: null
            resources: {}
            tolerations:
            - effect: NoExecute
              operator: Exists
            - effect: NoSchedule
              operator: Exists
            useLocalhost: true
          component: kube-scheduler
          enabled: false
          global:
            cattle:
              rkeWindowsPathPrefix: c:\
              systemDefaultRegistry: "null"
              windows:
                enabled: false
            fleet:
              clusterLabels:
                management.cattle.io/cluster-display-name: local
                management.cattle.io/cluster-name: local
                provider.cattle.io: harvester
            imagePullSecrets: "null"
            kubectl:
              pullPolicy: IfNotPresent
              repository: rancher/kubectl
              tag: v1.20.2
            rbac:
              create: true
              pspAnnotations: {}
              pspEnabled: true
              userRoles:
                aggregateToDefaultRoles: true
                create: true
          metricsPort: 10251
          namespaceOverride: "null"
          proxy:
            command:
            - pushprox-proxy
            enabled: true
            image:
              repository: rancher/pushprox-proxy
              tag: v0.1.0-rancher2-proxy
            nodeSelector: {}
            port: 8080
            resources: {}
            tolerations: null
          serviceMonitor:
            enabled: true
            endpoints:
            - port: metrics
    helmVersion: 3
    info:
      description: Upgrade complete
      firstDeployed: "2022-03-06T22:51:05Z"
      lastDeployed: "2022-03-22T18:55:44Z"
      notes: |
        rancher-monitoring has been installed. Check its status by running:
          kubectl --namespace cattle-monitoring-system get pods -l "release=rancher-monitoring"

        Visit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create & configure Alertmanager and Prometheus instances using the Operator.
      readme: |
        # kube-prometheus-stack

        Installs the [kube-prometheus stack](https://github.com/prometheus-operator/kube-prometheus), a collection of Kubernetes manifests, [Grafana](http://grafana.com/) dashboards, and [Prometheus rules](https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/) combined with documentation and scripts to provide easy to operate end-to-end Kubernetes cluster monitoring with [Prometheus](https://prometheus.io/) using the [Prometheus Operator](https://github.com/prometheus-operator/prometheus-operator).

        See the [kube-prometheus](https://github.com/prometheus-operator/kube-prometheus) README for details about components, dashboards, and alerts.

        _Note: This chart was formerly named `prometheus-operator` chart, now renamed to more clearly reflect that it installs the `kube-prometheus` project stack, within which Prometheus Operator is only one component._

        ## Prerequisites

        - Kubernetes 1.16+
        - Helm 3+

        ## Get Repo Info

        ```console
        helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
        helm repo update
        ```

        _See [helm repo](https://helm.sh/docs/helm/helm_repo/) for command documentation._

        ## Install Chart

        ```console
        # Helm
        $ helm install [RELEASE_NAME] prometheus-community/kube-prometheus-stack
        ```

        _See [configuration](#configuration) below._

        _See [helm install](https://helm.sh/docs/helm/helm_install/) for command documentation._

        ## Dependencies

        By default this chart installs additional, dependent charts:

        - [prometheus-community/kube-state-metrics](https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-state-metrics)
        - [prometheus-community/prometheus-node-exporter](https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-node-exporter)
        - [grafana/grafana](https://github.com/grafana/helm-charts/tree/main/charts/grafana)

        To disable dependencies during installation, see [multiple releases](#multiple-releases) below.

        _See [helm dependency](https://helm.sh/docs/helm/helm_dependency/) for command documentation._

        ## Uninstall Chart

        ```console
        # Helm
        $ helm uninstall [RELEASE_NAME]
        ```

        This removes all the Kubernetes components associated with the chart and deletes the release.

        _See [helm uninstall](https://helm.sh/docs/helm/helm_uninstall/) for command documentation._

        CRDs created by this chart are not removed by default and should be manually cleaned up:

        ```console
        kubectl delete crd alertmanagerconfigs.monitoring.coreos.com
        kubectl delete crd alertmanagers.monitoring.coreos.com
        kubectl delete crd podmonitors.monitoring.coreos.com
        kubectl delete crd probes.monitoring.coreos.com
        kubectl delete crd prometheuses.monitoring.coreos.com
        kubectl delete crd prometheusrules.monitoring.coreos.com
        kubectl delete crd servicemonitors.monitoring.coreos.com
        kubectl delete crd thanosrulers.monitoring.coreos.com
        ```

        ## Upgrading Chart

        ```console
        # Helm
        $ helm upgrade [RELEASE_NAME] prometheus-community/kube-prometheus-stack
        ```

        With Helm v3, CRDs created by this chart are not updated by default and should be manually updated.
        Consult also the [Helm Documentation on CRDs](https://helm.sh/docs/chart_best_practices/custom_resource_definitions).

        _See [helm upgrade](https://helm.sh/docs/helm/helm_upgrade/) for command documentation._

        ### Upgrading an existing Release to a new major version

        A major chart version change (like v1.2.3 -> v2.0.0) indicates that there is an incompatible breaking change needing manual actions.

        ### From 18.x to 19.x

        `kubeStateMetrics.serviceMonitor.namespaceOverride` was removed.
        Please use `kube-state-metrics.namespaceOverride` instead.

        ### From 17.x to 18.x

        Version 18 upgrades prometheus-operator from 0.49.x to 0.50.x. Helm does not automatically upgrade or install new CRDs on a chart upgrade, so you have to install the CRDs manually before updating:

        ```console
        kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.50.0/example/prometheus-operator-crd/monitoring.coreos.com_alertmanagerconfigs.yaml
        kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.50.0/example/prometheus-operator-crd/monitoring.coreos.com_alertmanagers.yaml
        kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.50.0/example/prometheus-operator-crd/monitoring.coreos.com_podmonitors.yaml
        kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.50.0/example/prometheus-operator-crd/monitoring.coreos.com_probes.yaml
        kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.50.0/example/prometheus-operator-crd/monitoring.coreos.com_prometheuses.yaml
        kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.50.0/example/prometheus-operator-crd/monitoring.coreos.com_prometheusrules.yaml
        kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.50.0/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml
        kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.50.0/example/prometheus-operator-crd/monitoring.coreos.com_thanosrulers.yaml
        ```

        ### From 16.x to 17.x

        Version 17 upgrades prometheus-operator from 0.48.x to 0.49.x. Helm does not automatically upgrade or install new CRDs on a chart upgrade, so you have to install the CRDs manually before updating:

        ```console
        kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.49.0/example/prometheus-operator-crd/monitoring.coreos.com_alertmanagerconfigs.yaml
        kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.49.0/example/prometheus-operator-crd/monitoring.coreos.com_alertmanagers.yaml
        kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.49.0/example/prometheus-operator-crd/monitoring.coreos.com_podmonitors.yaml
        kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.49.0/example/prometheus-operator-crd/monitoring.coreos.com_probes.yaml
        kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.49.0/example/prometheus-operator-crd/monitoring.coreos.com_prometheuses.yaml
        kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.49.0/example/prometheus-operator-crd/monitoring.coreos.com_prometheusrules.yaml
        kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.49.0/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml
        kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.49.0/example/prometheus-operator-crd/monitoring.coreos.com_thanosrulers.yaml
        ```

        ### From 15.x to 16.x

        Version 16 upgrades kube-state-metrics to v2.0.0. This includes changed command-line arguments and removed metrics, see this [blog post](https://kubernetes.io/blog/2021/04/13/kube-state-metrics-v-2-0/). This version also removes Grafana dashboards that supported Kubernetes 1.14 or earlier.

        ### From 14.x to 15.x

        Version 15 upgrades prometheus-operator from 0.46.x to 0.47.x. Helm does not automatically upgrade or install new CRDs on a chart upgrade, so you have to install the CRDs manually before updating:

        ```console
        kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.47.0/example/prometheus-operator-crd/monitoring.coreos.com_alertmanagerconfigs.yaml
        kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.47.0/example/prometheus-operator-crd/monitoring.coreos.com_alertmanagers.yaml
        kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.47.0/example/prometheus-operator-crd/monitoring.coreos.com_podmonitors.yaml
        kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.47.0/example/prometheus-operator-crd/monitoring.coreos.com_probes.yaml
        kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.47.0/example/prometheus-operator-crd/monitoring.coreos.com_prometheuses.yaml
        kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.47.0/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml
        kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.47.0/example/prometheus-operator-crd/monitoring.coreos.com_thanosrulers.yaml
        ```

        ### From 13.x to 14.x

        Version 14 upgrades prometheus-operator from 0.45.x to 0.46.x. Helm does not automatically upgrade or install new CRDs on a chart upgrade, so you have to install the CRDs manually before updating:

        ```console
        kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.46.0/example/prometheus-operator-crd/monitoring.coreos.com_alertmanagerconfigs.yaml
        kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.46.0/example/prometheus-operator-crd/monitoring.coreos.com_alertmanagers.yaml
        kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.46.0/example/prometheus-operator-crd/monitoring.coreos.com_podmonitors.yaml
        kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.46.0/example/prometheus-operator-crd/monitoring.coreos.com_probes.yaml
        kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.46.0/example/prometheus-operator-crd/monitoring.coreos.com_prometheuses.yaml
        kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.46.0/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml
        kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.46.0/example/prometheus-operator-crd/monitoring.coreos.com_thanosrulers.yaml
        ```

        ### From 12.x to 13.x

        Version 13 upgrades prometheus-operator from 0.44.x to 0.45.x. Helm does not automatically upgrade or install new CRDs on a chart upgrade, so you have to install the CRD manually before updating:

        ```console
        kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.45.0/example/prometheus-operator-crd/monitoring.coreos.com_alertmanagerconfigs.yaml
        kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.45.0/example/prometheus-operator-crd/monitoring.coreos.com_prometheuses.yaml
        kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.45.0/example/prometheus-operator-crd/monitoring.coreos.com_alertmanagers.yaml
        ```

        ### From 11.x to 12.x

        Version 12 upgrades prometheus-operator from 0.43.x to 0.44.x. Helm does not automatically upgrade or install new CRDs on a chart upgrade, so you have to install the CRD manually before updating:

        ```console
        kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/release-0.44/example/prometheus-operator-crd/monitoring.coreos.com_prometheuses.yaml
        ```

        The chart was migrated to support only helm v3 and later.

        ### From 10.x to 11.x

        Version 11 upgrades prometheus-operator from 0.42.x to 0.43.x. Starting with 0.43.x an additional `AlertmanagerConfigs` CRD is introduced. Helm does not automatically upgrade or install new CRDs on a chart upgrade, so you have to install the CRD manually before updating:

        ```console
        kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/release-0.43/example/prometheus-operator-crd/monitoring.coreos.com_alertmanagerconfigs.yaml
        ```

        Version 11 removes the deprecated tlsProxy via ghostunnel in favor of native TLS support the prometheus-operator gained with v0.39.0.

        ### From 9.x to 10.x

        Version 10 upgrades prometheus-operator from 0.38.x to 0.42.x. Starting with 0.40.x an additional `Probes` CRD is introduced. Helm does not automatically upgrade or install new CRDs on a chart upgrade, so you have to install the CRD manually before updating:

        ```console
        kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/release-0.42/example/prometheus-operator-crd/monitoring.coreos.com_probes.yaml
        ```

        ### From 8.x to 9.x

        Version 9 of the helm chart removes the existing `additionalScrapeConfigsExternal` in favour of `additionalScrapeConfigsSecret`. This change lets users specify the secret name and secret key to use for the additional scrape configuration of prometheus. This is useful for users that have prometheus-operator as a subchart and also have a template that creates the additional scrape configuration.

        ### From 7.x to 8.x

        Due to new template functions being used in the rules in version 8.x.x of the chart, an upgrade to Prometheus Operator and Prometheus is necessary in order to support them. First, upgrade to the latest version of 7.x.x

        ```console
        helm upgrade [RELEASE_NAME] prometheus-community/kube-prometheus-stack --version 7.5.0
        ```

        Then upgrade to 8.x.x

        ```console
        helm upgrade [RELEASE_NAME] prometheus-community/kube-prometheus-stack --version [8.x.x]
        ```

        Minimal recommended Prometheus version for this chart release is `2.12.x`

        ### From 6.x to 7.x

        Due to a change in grafana subchart, version 7.x.x now requires Helm >= 2.12.0.

        ### From 5.x to 6.x

        Due to a change in deployment labels of kube-state-metrics, the upgrade requires `helm upgrade --force` in order to re-create the deployment. If this is not done an error will occur indicating that the deployment cannot be modified:

        ```console
        invalid: spec.selector: Invalid value: v1.LabelSelector{MatchLabels:map[string]string{"app.kubernetes.io/name":"kube-state-metrics"}, MatchExpressions:[]v1.LabelSelectorRequirement(nil)}: field is immutable
        ```

        If this error has already been encountered, a `helm history` command can be used to determine which release has worked, then `helm rollback` to the release, then `helm upgrade --force` to this new one

        ## Configuration

        See [Customizing the Chart Before Installing](https://helm.sh/docs/intro/using_helm/#customizing-the-chart-before-installing). To see all configurable options with detailed comments:

        ```console
        helm show values prometheus-community/kube-prometheus-stack
        ```

        You may also run `helm show values` on this chart's [dependencies](#dependencies) for additional options.

        ### Rancher Monitoring Configuration

        The following table shows values exposed by Rancher Monitoring's additions to the chart:

        | Parameter | Description | Default |
        | ----- | ----------- | ------ |
        | `nameOverride` | Provide a name that should be used instead of the chart name when naming all resources deployed by this chart |`"rancher-monitoring"`|
        | `namespaceOverride` | Override the deployment namespace | `"cattle-monitoring-system"` |
        | `global.rbac.userRoles.create` | Create default user ClusterRoles to allow users to interact with Prometheus CRs, ConfigMaps, and Secrets | `true` |
        | `global.rbac.userRoles.aggregateToDefaultRoles` | Aggregate default user ClusterRoles into default k8s ClusterRoles | `true` |
        | `prometheus-adapter.enabled` | Whether to install [prometheus-adapter](https://github.com/helm/charts/tree/master/stable/prometheus-adapter) within the cluster | `true` |
        | `prometheus-adapter.prometheus.url` | A URL pointing to the Prometheus deployment within your cluster. The default value is set based on the assumption that you plan to deploy the default Prometheus instance from this chart where `.Values.namespaceOverride=cattle-monitoring-system` and `.Values.nameOverride=rancher-monitoring` | `http://rancher-monitoring-prometheus.cattle-monitoring-system.svc` |
        | `prometheus-adapter.prometheus.port` | The port on the Prometheus deployment that Prometheus Adapter can make requests to | `9090` |
        | `prometheus.prometheusSpec.ignoreNamespaceSelectors` | Ignore NamespaceSelector settings from the PodMonitor and ServiceMonitor configs. If true, PodMonitors and ServiceMonitors can only discover Pods and Services within the namespace they are deployed into | `false` |

        The following values are enabled for different distributions via [rancher-pushprox](https://github.com/rancher/dev-charts/tree/master/packages/rancher-pushprox). See the rancher-pushprox `README.md` for more information on what all values can be configured for the PushProxy chart.

        | Parameter | Description | Default |
        | ----- | ----------- | ------ |
        | `rkeControllerManager.enabled` | Create a PushProx installation for monitoring kube-controller-manager metrics in RKE clusters | `false` |
        | `rkeScheduler.enabled` | Create a PushProx installation for monitoring kube-scheduler metrics in RKE clusters | `false` |
        | `rkeProxy.enabled` | Create a PushProx installation for monitoring kube-proxy metrics in RKE clusters | `false` |
        | `rkeIngressNginx.enabled` | Create a PushProx installation for monitoring ingress-nginx metrics in RKE clusters | `false` |
        | `rkeEtcd.enabled` | Create a PushProx installation for monitoring etcd metrics in RKE clusters | `false` |
        | `rke2IngressNginx.enabled` | Create a PushProx installation for monitoring ingress-nginx metrics in RKE2 clusters | `false` |
        | `k3sServer.enabled` | Create a PushProx installation for monitoring k3s-server metrics (accounts for kube-controller-manager, kube-scheduler, and kube-proxy metrics) in k3s clusters | `false` |
        | `kubeAdmControllerManager.enabled` | Create a PushProx installation for monitoring kube-controller-manager metrics in kubeAdm clusters | `false` |
        | `kubeAdmScheduler.enabled` | Create a PushProx installation for monitoring kube-scheduler metrics in kubeAdm clusters | `false` |
        | `kubeAdmProxy.enabled` | Create a PushProx installation for monitoring kube-proxy metrics in kubeAdm clusters | `false` |
        | `kubeAdmEtcd.enabled` | Create a PushProx installation for monitoring etcd metrics in kubeAdm clusters | `false` |


        ### Multiple releases

        The same chart can be used to run multiple Prometheus instances in the same cluster if required. To achieve this, it is necessary to run only one instance of prometheus-operator and a pair of alertmanager pods for an HA configuration, while all other components need to be disabled. To disable a dependency during installation, set `kubeStateMetrics.enabled`, `nodeExporter.enabled` and `grafana.enabled` to `false`.

        ## Work-Arounds for Known Issues

        ### Running on private GKE clusters

        When Google configure the control plane for private clusters, they automatically configure VPC peering between your Kubernetes cluster’s network and a separate Google managed project. In order to restrict what Google are able to access within your cluster, the firewall rules configured restrict access to your Kubernetes pods. This means that in order to use the webhook component with a GKE private cluster, you must configure an additional firewall rule to allow the GKE control plane access to your webhook pod.

        You can read more information on how to add firewall rules for the GKE control plane nodes in the [GKE docs](https://cloud.google.com/kubernetes-engine/docs/how-to/private-clusters#add_firewall_rules)

        Alternatively, you can disable the hooks by setting `prometheusOperator.admissionWebhooks.enabled=false`.

        ## PrometheusRules Admission Webhooks

        With Prometheus Operator version 0.30+, the core Prometheus Operator pod exposes an endpoint that will integrate with the `validatingwebhookconfiguration` Kubernetes feature to prevent malformed rules from being added to the cluster.

        ### How the Chart Configures the Hooks

        A validating and mutating webhook configuration requires the endpoint to which the request is sent to use TLS. It is possible to set up custom certificates to do this, but in most cases, a self-signed certificate is enough. The setup of this component requires some more complex orchestration when using helm. The steps are created to be idempotent and to allow turning the feature on and off without running into helm quirks.

        1. A pre-install hook provisions a certificate into the same namespace using a format compatible with provisioning using end-user certificates. If the certificate already exists, the hook exits.
        2. The prometheus operator pod is configured to use a TLS proxy container, which will load that certificate.
        3. Validating and Mutating webhook configurations are created in the cluster, with their failure mode set to Ignore. This allows rules to be created by the same chart at the same time, even though the webhook has not yet been fully set up - it does not have the correct CA field set.
        4. A post-install hook reads the CA from the secret created by step 1 and patches the Validating and Mutating webhook configurations. This process will allow a custom CA provisioned by some other process to also be patched into the webhook configurations. The chosen failure policy is also patched into the webhook configurations

        ### Alternatives

        It should be possible to use [jetstack/cert-manager](https://github.com/jetstack/cert-manager) if a more complete solution is required, but it has not been tested.

        You can enable automatic self-signed TLS certificate provisioning via cert-manager by setting the `prometheusOperator.admissionWebhooks.certManager.enabled` value to true.

        ### Limitations

        Because the operator can only run as a single pod, there is potential for this component failure to cause rule deployment failure. Because this risk is outweighed by the benefit of having validation, the feature is enabled by default.

        ## Developing Prometheus Rules and Grafana Dashboards

        This chart Grafana Dashboards and Prometheus Rules are just a copy from [prometheus-operator/prometheus-operator](https://github.com/prometheus-operator/prometheus-operator) and other sources, synced (with alterations) by scripts in [hack](hack) folder. In order to introduce any changes you need to first [add them to the original repo](https://github.com/prometheus-operator/kube-prometheus/blob/master/docs/developing-prometheus-rules-and-grafana-dashboards.md) and then sync there by scripts.

        ## Further Information

        For more in-depth documentation of configuration options meanings, please see

        - [Prometheus Operator](https://github.com/prometheus-operator/prometheus-operator)
        - [Prometheus](https://prometheus.io/docs/introduction/overview/)
        - [Grafana](https://github.com/grafana/helm-charts/tree/main/charts/grafana#grafana-helm-chart)

        ## prometheus.io/scrape

        The prometheus operator does not support annotation-based discovery of services, using the `PodMonitor` or `ServiceMonitor` CRD in its place as they provide far more configuration options.
        For information on how to use PodMonitors/ServiceMonitors, please see the documentation on the `prometheus-operator/prometheus-operator` documentation here:

        - [ServiceMonitors](https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/user-guides/getting-started.md#include-servicemonitors)
        - [PodMonitors](https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/user-guides/getting-started.md#include-podmonitors)
        - [Running Exporters](https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/user-guides/running-exporters.md)

        By default, Prometheus discovers PodMonitors and ServiceMonitors within its namespace, that are labeled with the same release tag as the prometheus-operator release.
        Sometimes, you may need to discover custom PodMonitors/ServiceMonitors, for example used to scrape data from third-party applications.
        An easy way of doing this, without compromising the default PodMonitors/ServiceMonitors discovery, is allowing Prometheus to discover all PodMonitors/ServiceMonitors within its namespace, without applying label filtering.
        To do so, you can set `prometheus.prometheusSpec.podMonitorSelectorNilUsesHelmValues` and `prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues` to `false`.

        ## Migrating from stable/prometheus-operator chart

        ## Zero downtime

        Since `kube-prometheus-stack` is fully compatible with the `stable/prometheus-operator` chart, a migration without downtime can be achieved.
        However, the old name prefix needs to be kept. If you want the new name please follow the step by step guide below (with downtime).

        You can override the name to achieve this:

        ```console
        helm upgrade prometheus-operator prometheus-community/kube-prometheus-stack -n monitoring --reuse-values --set nameOverride=prometheus-operator
        ```

        **Note**: It is recommended to run this first with `--dry-run --debug`.

        ## Redeploy with new name (downtime)

        If the **prometheus-operator** values are compatible with the new **kube-prometheus-stack** chart, please follow the below steps for migration:

        > The guide presumes that chart is deployed in `monitoring` namespace and the deployments are running there. If in other namespace, please replace the `monitoring` to the deployed namespace.

        1. Patch the PersistenceVolume created/used by the prometheus-operator chart to `Retain` claim policy:

            ```console
            kubectl patch pv/<PersistentVolume name> -p '{"spec":{"persistentVolumeReclaimPolicy":"Retain"}}'
            ```

            **Note:** To execute the above command, the user must have a cluster wide permission. Please refer [Kubernetes RBAC](https://kubernetes.io/docs/reference/access-authn-authz/rbac/)

        2. Uninstall the **prometheus-operator** release and delete the existing PersistentVolumeClaim, and verify PV become Released.

            ```console
            helm uninstall prometheus-operator -n monitoring
            kubectl delete pvc/<PersistenceVolumeClaim name> -n monitoring
            ```

            Additionally, you have to manually remove the remaining `prometheus-operator-kubelet` service.

            ```console
            kubectl delete service/prometheus-operator-kubelet -n kube-system
            ```

            You can choose to remove all your existing CRDs (ServiceMonitors, Podmonitors, etc.) if you want to.

        3. Remove current `spec.claimRef` values to change the PV's status from Released to Available.

            ```console
            kubectl patch pv/<PersistentVolume name> --type json -p='[{"op": "remove", "path": "/spec/claimRef"}]' -n monitoring
            ```

        **Note:** To execute the above command, the user must have a cluster wide permission. Please refer to [Kubernetes RBAC](https://kubernetes.io/docs/reference/access-authn-authz/rbac/)

        After these steps, proceed to a fresh **kube-prometheus-stack** installation and make sure the current release of **kube-prometheus-stack** matching the `volumeClaimTemplate` values in the `values.yaml`.

        The binding is done via matching a specific amount of storage requested and with certain access modes.

        For example, if you had storage specified as this with **prometheus-operator**:

        ```yaml
        volumeClaimTemplate:
          spec:
            storageClassName: gp2
            accessModes: ["ReadWriteOnce"]
            resources:
             requests:
               storage: 50Gi
        ```

        You have to specify matching `volumeClaimTemplate` with 50Gi storage and `ReadWriteOnce` access mode.

        Additionally, you should check the current AZ of your legacy installation's PV, and configure the fresh release to use the same AZ as the old one. If the pods are in a different AZ than the PV, the release will fail to bind the existing one, hence creating a new PV.

        This can be achieved either by specifying the labels through `values.yaml`, e.g. setting `prometheus.prometheusSpec.nodeSelector` to:

        ```yaml
        nodeSelector:
          failure-domain.beta.kubernetes.io/zone: east-west-1a
        ```

        or passing these values as `--set` overrides during installation.

        The new release should now re-attach your previously released PV with its content.

        ## Migrating from coreos/prometheus-operator chart

        The multiple charts have been combined into a single chart that installs prometheus operator, prometheus, alertmanager, grafana as well as the multitude of exporters necessary to monitor a cluster.

        There is no simple and direct migration path between the charts as the changes are extensive and intended to make the chart easier to support.

        The capabilities of the old chart are all available in the new chart, including the ability to run multiple prometheus instances on a single cluster - you will need to disable the parts of the chart you do not wish to deploy.

        You can check out the tickets for this change [here](https://github.com/prometheus-operator/prometheus-operator/issues/592) and [here](https://github.com/helm/charts/pull/6765).

        ### High-level overview of Changes

        #### Added dependencies

        The chart has added 3 [dependencies](#dependencies).

        - Node-Exporter, Kube-State-Metrics: These components are loaded as dependencies into the chart, and are relatively simple components
        - Grafana: The Grafana chart is more feature-rich than this chart - it contains a sidecar that is able to load data sources and dashboards from configmaps deployed into the same cluster. For more information check out the [documentation for the chart](https://github.com/grafana/helm-charts/blob/main/charts/grafana/README.md)

        #### Kubelet Service

        Because the kubelet service has a new name in the chart, make sure to clean up the old kubelet service in the `kube-system` namespace to prevent counting container metrics twice.

        #### Persistent Volumes

        If you would like to keep the data of the current persistent volumes, it should be possible to attach existing volumes to new PVCs and PVs that are created using the conventions in the new chart. For example, in order to use an existing Azure disk for a helm release called `prometheus-migration` the following resources can be created:

        ```yaml
        apiVersion: v1
        kind: PersistentVolume
        metadata:
          name: pvc-prometheus-migration-prometheus-0
        spec:
          accessModes:
          - ReadWriteOnce
          azureDisk:
            cachingMode: None
            diskName: pvc-prometheus-migration-prometheus-0
            diskURI: /subscriptions/f5125d82-2622-4c50-8d25-3f7ba3e9ac4b/resourceGroups/sample-migration-resource-group/providers/Microsoft.Compute/disks/pvc-prometheus-migration-prometheus-0
            fsType: ""
            kind: Managed
            readOnly: false
          capacity:
            storage: 1Gi
          persistentVolumeReclaimPolicy: Delete
          storageClassName: prometheus
          volumeMode: Filesystem
        ```

        ```yaml
        apiVersion: v1
        kind: PersistentVolumeClaim
        metadata:
          labels:
            app.kubernetes.io/name: prometheus
            prometheus: prometheus-migration-prometheus
          name: prometheus-prometheus-migration-prometheus-db-prometheus-prometheus-migration-prometheus-0
          namespace: monitoring
        spec:
          accessModes:
          - ReadWriteOnce
          resources:
            requests:
              storage: 1Gi
          storageClassName: prometheus
          volumeMode: Filesystem
          volumeName: pvc-prometheus-migration-prometheus-0
        ```

        The PVC will take ownership of the PV and when you create a release using a persistent volume claim template it will use the existing PVCs as they match the naming convention used by the chart. For other cloud providers similar approaches can be used.

        #### KubeProxy

        The metrics bind address of kube-proxy is default to `127.0.0.1:10249` that prometheus instances **cannot** access to. You should expose metrics by changing `metricsBindAddress` field value to `0.0.0.0:10249` if you want to collect them.

        Depending on the cluster, the relevant part `config.conf` will be in ConfigMap `kube-system/kube-proxy` or `kube-system/kube-proxy-config`. For example:

        ```console
        kubectl -n kube-system edit cm kube-proxy
        ```

        ```yaml
        apiVersion: v1
        data:
          config.conf: |-
            apiVersion: kubeproxy.config.k8s.io/v1alpha1
            kind: KubeProxyConfiguration
            # ...
            # metricsBindAddress: 127.0.0.1:10249
            metricsBindAddress: 0.0.0.0:10249
            # ...
          kubeconfig.conf: |-
            # ...
        kind: ConfigMap
        metadata:
          labels:
            app: kube-proxy
          name: kube-proxy
          namespace: kube-system
        ```
      status: deployed
    name: rancher-monitoring
    namespace: cattle-monitoring-system
    resources:
    - apiVersion: v1
      kind: Namespace
      name: cattle-dashboards
    - apiVersion: networking.k8s.io/v1
      kind: NetworkPolicy
      name: default-allow-all
      namespace: cattle-monitoring-system
    - apiVersion: networking.k8s.io/v1
      kind: NetworkPolicy
      name: default-allow-all
      namespace: cattle-dashboards
    - apiVersion: policy/v1beta1
      kind: PodSecurityPolicy
      name: rancher-monitoring-grafana
    - apiVersion: policy/v1beta1
      kind: PodSecurityPolicy
      name: rancher-monitoring-kube-state-metrics
    - apiVersion: policy/v1beta1
      kind: PodSecurityPolicy
      name: rancher-monitoring-prometheus-adapter
    - apiVersion: policy/v1beta1
      kind: PodSecurityPolicy
      name: rancher-monitoring-prometheus-node-exporter
      namespace: cattle-monitoring-system
    - apiVersion: policy/v1beta1
      kind: PodSecurityPolicy
      name: rancher-monitoring-operator
    - apiVersion: policy/v1beta1
      kind: PodSecurityPolicy
      name: rancher-monitoring-prometheus
    - apiVersion: policy/v1beta1
      kind: PodSecurityPolicy
      name: rancher-monitoring-patch-sa
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: ServiceAccount
      name: rancher-monitoring-grafana
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: ServiceAccount
      name: rancher-monitoring-kube-state-metrics
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: ServiceAccount
      name: rancher-monitoring-prometheus-adapter
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: ServiceAccount
      name: rancher-monitoring-prometheus-node-exporter
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: ServiceAccount
      name: rancher-monitoring-operator
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: ServiceAccount
      name: rancher-monitoring-prometheus
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: ServiceAccount
      name: rancher-monitoring-patch-sa
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: Secret
      name: rancher-monitoring-grafana
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-grafana-config-dashboards
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-grafana
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: ConfigMap
      name: grafana-nginx-proxy-config
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-prometheus-adapter
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-grafana-datasource
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-apiserver
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-cluster-total
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-k8s-coredns
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-k8s-resources-cluster
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-k8s-resources-namespace
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-k8s-resources-node
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-k8s-resources-pod
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-k8s-resources-workload
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-k8s-resources-workloads-namespace
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-kubelet
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-namespace-by-pod
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-namespace-by-workload
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-node-cluster-rsrc-use
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-node-rsrc-use
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-nodes
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-persistentvolumesusage
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-pod-total
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-prometheus
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-statefulset
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-workload-total
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: prometheus-nginx-proxy-config
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-default-dashboards-cluster
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-default-dashboards-home
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-default-dashboards-k8s
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-default-dashboards-nodes
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-default-dashboards-pods
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-default-dashboards-workloads
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: PersistentVolumeClaim
      name: rancher-monitoring-grafana
      namespace: cattle-monitoring-system
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      name: rancher-monitoring-grafana-clusterrole
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      name: psp-rancher-monitoring-kube-state-metrics
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      name: rancher-monitoring-kube-state-metrics
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      name: prometheus-adapter-resource-reader
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      name: prometheus-adapter-server-resources
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      name: prometheus-adapter-psp
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      name: psp-rancher-monitoring-prometheus-node-exporter
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      name: rancher-monitoring-operator
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      name: rancher-monitoring-operator-psp
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      name: rancher-monitoring-prometheus
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      name: rancher-monitoring-prometheus-psp
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      name: monitoring-admin
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      name: monitoring-edit
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      name: monitoring-view
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      name: monitoring-ui-view
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      name: rancher-monitoring-patch-sa
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      name: rancher-monitoring-grafana-clusterrolebinding
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      name: rancher-monitoring-kube-state-metrics
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      name: psp-rancher-monitoring-kube-state-metrics
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      name: prometheus-adapter-system-auth-delegator
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      name: prometheus-adapter-resource-reader
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      name: prometheus-adapter-hpa-controller
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      name: prometheus-adapter-psp
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      name: psp-rancher-monitoring-prometheus-node-exporter
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      name: rancher-monitoring-operator
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      name: rancher-monitoring-operator-psp
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      name: rancher-monitoring-prometheus
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      name: rancher-monitoring-prometheus-psp
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      name: rancher-monitoring-patch-sa
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: Role
      name: rancher-monitoring-grafana
      namespace: cattle-monitoring-system
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: Role
      name: monitoring-config-admin
      namespace: cattle-monitoring-system
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: Role
      name: monitoring-config-edit
      namespace: cattle-monitoring-system
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: Role
      name: monitoring-config-view
      namespace: cattle-monitoring-system
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: Role
      name: monitoring-dashboard-admin
      namespace: cattle-dashboards
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: Role
      name: monitoring-dashboard-edit
      namespace: cattle-dashboards
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: Role
      name: monitoring-dashboard-view
      namespace: cattle-dashboards
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: RoleBinding
      name: rancher-monitoring-grafana
      namespace: cattle-monitoring-system
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: RoleBinding
      name: prometheus-adapter-auth-reader
      namespace: kube-system
    - apiVersion: v1
      kind: Service
      name: rancher-monitoring-grafana
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: Service
      name: rancher-monitoring-kube-state-metrics
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: Service
      name: rancher-monitoring-prometheus-adapter
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: Service
      name: rancher-monitoring-prometheus-node-exporter
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: Service
      name: rancher-monitoring-coredns
      namespace: kube-system
    - apiVersion: v1
      kind: Service
      name: rancher-monitoring-operator
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: Service
      name: rancher-monitoring-prometheus
      namespace: cattle-monitoring-system
    - apiVersion: apps/v1
      kind: DaemonSet
      name: rancher-monitoring-prometheus-node-exporter
      namespace: cattle-monitoring-system
    - apiVersion: apps/v1
      kind: Deployment
      name: rancher-monitoring-grafana
      namespace: cattle-monitoring-system
    - apiVersion: apps/v1
      kind: Deployment
      name: rancher-monitoring-kube-state-metrics
      namespace: cattle-monitoring-system
    - apiVersion: apps/v1
      kind: Deployment
      name: rancher-monitoring-prometheus-adapter
      namespace: cattle-monitoring-system
    - apiVersion: apps/v1
      kind: Deployment
      name: rancher-monitoring-operator
      namespace: cattle-monitoring-system
    - apiVersion: apiregistration.k8s.io/v1
      kind: APIService
      name: v1beta1.custom.metrics.k8s.io
    - apiVersion: admissionregistration.k8s.io/v1
      kind: MutatingWebhookConfiguration
      name: rancher-monitoring-admission
    - apiVersion: monitoring.coreos.com/v1
      kind: Prometheus
      name: rancher-monitoring-prometheus
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-general.rules
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-k8s.rules
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-kube-apiserver-availability.rules
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-kube-apiserver-burnrate.rules
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-kube-apiserver-histogram.rules
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-kube-apiserver-slos
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-kube-apiserver.rules
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-kube-prometheus-general.rules
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-kube-prometheus-node-recording.rules
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-kube-state-metrics
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-kubelet.rules
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-kubernetes-apps
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-kubernetes-resources
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-kubernetes-storage
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-kubernetes-system-apiserver
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-kubernetes-system-kubelet
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-kubernetes-system
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-node-exporter.rules
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-node-exporter
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-node-network
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-node.rules
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-prometheus-operator
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-prometheus
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: ServiceMonitor
      name: rancher-monitoring-coredns
      namespace: kube-system
    - apiVersion: monitoring.coreos.com/v1
      kind: ServiceMonitor
      name: rancher-monitoring-apiserver
      namespace: default
    - apiVersion: monitoring.coreos.com/v1
      kind: ServiceMonitor
      name: rancher-monitoring-kube-state-metrics
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: ServiceMonitor
      name: rancher-monitoring-kubelet
      namespace: kube-system
    - apiVersion: monitoring.coreos.com/v1
      kind: ServiceMonitor
      name: rancher-monitoring-node-exporter
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: ServiceMonitor
      name: rancher-monitoring-grafana
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: ServiceMonitor
      name: rancher-monitoring-operator
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: ServiceMonitor
      name: rancher-monitoring-prometheus
      namespace: cattle-monitoring-system
    - apiVersion: admissionregistration.k8s.io/v1
      kind: ValidatingWebhookConfiguration
      name: rancher-monitoring-admission
    values:
      alertmanager:
        enabled: false
      global:
        fleet:
          clusterLabels:
            management.cattle.io/cluster-display-name: local
            management.cattle.io/cluster-name: local
            provider.cattle.io: harvester
      grafana:
        persistence:
          accessModes:
          - ReadWriteOnce
          enabled: true
          size: "10"
          storageClassName: longhorn
          type: pvc
      prometheus:
        prometheusSpec:
          evaluationInterval: 1m
          resources:
            requests:
              cpu: 500m
          retention: 5d
          retentionSize: 50GiB
          scrapeInterval: 1m
          storageSpec:
            volumeClaimTemplate:
              spec:
                accessModes:
                - ReadWriteOnce
                resources:
                  requests:
                    storage: 50Gi
                storageClassName: longhorn
                volumeMode: Filesystem
    version: 89
  status:
    observedGeneration: 178
    summary:
      state: deployed
- apiVersion: catalog.cattle.io/v1
  kind: App
  metadata:
    annotations:
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/7xWwW7jNhD9FWF6rKSI8tpJBPSw3QBF0RZdOGkPbfcwIkcSG4oUSEqFa+jfC9KOnd24sbEt9iaBj3ycN2+etIWePAr0CNUWUGvj0UujXXg19Z/EvSOfW2lyjt4ryqW5kgIq6Ej1GQ4DpP+KM39pslk7PUIFVxNLkx+kFt/cE7fkz27T2BNU4Lo8MOWWFKGjfGK5Rc07sllvtPTGSt1m3Ip8YiW76FQ3IA9H79aeH+M2zlMPcwrcUtThQfbkPPYDVHpUKgWFNalX1enQdVDBSlwvy9ubekXX5ULcXFPNqWGcsQKZuF6ysmEroiULbPtaTxcGu/Wzt04hlrimhixpTg6q37eAg/yVrJNGQwVTEKhWhj/+HKB3pMjHlQaVoxS40d4apchC5e1IKTxKHZp96NnndmWMnllxzhc13Wa3vBHZm2LBs5ojZSte0HJVl6s3CwbzhzkFNxAPIvMOrQ8Pr9iUo0dl2mct4GS9bCSJo6iQnsB1UggKwoRiTyIuVP7lxr0y2bnWNorIP9uHLWl/yqYRmCnDUR15P91dj1ooyqLYPefZxbSO7CQ5Zci5GbWHCoIvX5pHkONWDjvXwPfaeVTKJb6j5N36ziWNsclL0hzOe9xvhrCOw6Akj82FFKYDOyuKnOXF1+PAbvMiX4TrTahGihZolalR7c3gFYWnnUh31OCo/Jpa6bzdxMLmFGSPbURZGowLN9kcL3flOlIq3AnbUHhgvolzagTdkyLujYVqO6fgjSL7ZMWQEOHwMBoH3RYpSN2YwPWxeL8MrUVBCTf9oMgH/zXSOn9HgzKb6N2yKMusWGTF6qEsq2VRMfYbhAw6iSrLB3ZTLVfVIqIsoYiSf3WiI0H0P/TbZL1bSeKcJb5Dn8gXTYUUnEc/OqhAPPH+f7FlyZnRngyswSjJN1cTq8lj8N8+j94bcU98tNJv3kfIOYNlPWpsYwxc+Ak4FZ2HNIzD8nY/K1+S+p3RjWx/wuESVtmQ859Na2vkOY6+M1b+HS2eP964EBYfXUiNzpNdG0UXC/Hfqb6VWkjdXs744XRaxBiMsbE7+8fD1323syf9PCX3qExINyjcPIV7DGVIX9/zCXawZpKC7BEZ/qnQThTQMM8hSA7xx0o2H4cw/HuEwCbxHel9/EBVpODGvseQZNt5nv8JAAD//0y0d8fbCQAA
      objectset.rio.cattle.io/id: helm-app
      objectset.rio.cattle.io/owner-gvk: /v1, Kind=Secret
      objectset.rio.cattle.io/owner-name: sh.helm.release.v1.rancher-monitoring-crd.v121
      objectset.rio.cattle.io/owner-namespace: cattle-monitoring-system
    creationTimestamp: "2022-03-06T22:50:20Z"
    generation: 242
    labels:
      objectset.rio.cattle.io/hash: 6d75298b6e723d87ebcef1c110a1d7512f16ee51
    managedFields:
    - apiVersion: catalog.cattle.io/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:objectset.rio.cattle.io/applied: {}
            f:objectset.rio.cattle.io/id: {}
            f:objectset.rio.cattle.io/owner-gvk: {}
            f:objectset.rio.cattle.io/owner-name: {}
            f:objectset.rio.cattle.io/owner-namespace: {}
          f:labels:
            .: {}
            f:objectset.rio.cattle.io/hash: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6ccc3be9-9cfd-403c-bcae-6c0e56b26431"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:spec:
          .: {}
          f:chart:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:catalog.cattle.io/certified: {}
                f:catalog.cattle.io/hidden: {}
                f:catalog.cattle.io/namespace: {}
                f:catalog.cattle.io/release-name: {}
                f:fleet.cattle.io/agent-namespace: {}
                f:fleet.cattle.io/bundle-id: {}
                f:fleet.cattle.io/service-account: {}
              f:apiVersion: {}
              f:description: {}
              f:name: {}
              f:type: {}
              f:version: {}
            f:values:
              .: {}
              f:global: {}
              f:image: {}
              f:nodeSelector: {}
              f:tolerations: {}
          f:helmVersion: {}
          f:info:
            .: {}
            f:description: {}
            f:firstDeployed: {}
            f:lastDeployed: {}
            f:readme: {}
            f:status: {}
          f:name: {}
          f:namespace: {}
          f:resources: {}
          f:values:
            .: {}
            f:global: {}
          f:version: {}
        f:status:
          .: {}
          f:observedGeneration: {}
          f:summary:
            .: {}
            f:state: {}
      manager: rancher
      operation: Update
      time: "2022-03-22T18:57:19Z"
    name: rancher-monitoring-crd
    namespace: cattle-monitoring-system
    ownerReferences:
    - apiVersion: v1
      blockOwnerDeletion: false
      controller: true
      kind: Secret
      name: sh.helm.release.v1.rancher-monitoring-crd.v121
      uid: 6ccc3be9-9cfd-403c-bcae-6c0e56b26431
    resourceVersion: "14474472"
    uid: 62df8550-acd3-40f7-ab79-9815ad3f430c
  spec:
    chart:
      metadata:
        annotations:
          catalog.cattle.io/certified: rancher
          catalog.cattle.io/hidden: "true"
          catalog.cattle.io/namespace: cattle-monitoring-system
          catalog.cattle.io/release-name: rancher-monitoring-crd
          fleet.cattle.io/agent-namespace: cattle-fleet-local-system
          fleet.cattle.io/bundle-id: mcc-rancher-monitoring-crd
          fleet.cattle.io/service-account: "null"
        apiVersion: v1
        description: Installs the CRDs for rancher-monitoring.
        name: rancher-monitoring-crd
        type: application
        version: 100.1.0+up19.0.3
      values:
        global:
          cattle:
            systemDefaultRegistry: "null"
        image:
          repository: rancher/shell
          tag: v0.1.8
        nodeSelector: {}
        tolerations: null
    helmVersion: 3
    info:
      description: Upgrade complete
      firstDeployed: "2022-03-06T22:50:11Z"
      lastDeployed: "2022-03-22T18:56:31Z"
      readme: |-
        # rancher-monitoring-crd
        A Rancher chart that installs the CRDs used by rancher-monitoring.

        ## How does this chart work?

        This chart marshalls all of the CRD files placed in the `crd-manifest` directory into a ConfigMap that is installed onto a cluster alongside relevant RBAC (ServiceAccount, ClusterRoleBinding, ClusterRole, and PodSecurityPolicy).

        Once the relevant dependent resourcees are installed / upgraded / rolled back, this chart executes a post-install / post-upgrade / post-rollback Job that:
        - Patches any existing versions of the CRDs contained within the `crd-manifest` on the cluster to set `spec.preserveUnknownFields=false`; this step is required since, based on [Kubernetes docs](https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#field-pruning) and a [known workaround](https://github.com/kubernetes-sigs/controller-tools/issues/476#issuecomment-691519936), such CRDs cannot be upgraded normally from `apiextensions.k8s.io/v1beta1` to `apiextensions.k8s.io/v1`.
        - Runs a `kubectl apply` on the CRDs that are contained within the crd-manifest ConfigMap to upgrade CRDs in the cluster

        On an uninstall, this chart executes a separate post-delete Job that:
        - Patches any existing versions of the CRDs contained within `crd-manifest` on the cluster to set `metadata.finalizers=[]`
        - Runs a `kubectl delete` on the CRDs that are contained within the crd-manifest ConfigMap to clean up the CRDs from the cluster

        Note: If the relevant CRDs already existed in the cluster at the time of install, this chart will absorb ownership of the lifecycle of those CRDs; therefore, on a `helm uninstall`, those CRDs will also be removed from the cluster alongside this chart.

        ## Why can't we just place the CRDs in the templates/ directory of the main chart?

        In Helm today, you cannot declare a CRD and declare a resource of that CRD's kind in templates/ without encountering a failure on render.

        ## [Helm 3] Why can't we just place the CRDs in the crds/ directory of the main chart?

        The Helm 3 `crds/` directory only supports the installation of CRDs, but does not support the upgrade and removal of CRDs, unlike what this chart facilitiates.
      status: deployed
    name: rancher-monitoring-crd
    namespace: cattle-monitoring-system
    resources:
    - apiVersion: policy/v1beta1
      kind: PodSecurityPolicy
      name: rancher-monitoring-crd-manager
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: ServiceAccount
      name: rancher-monitoring-crd-manager
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-crd-manifest
      namespace: cattle-monitoring-system
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      name: rancher-monitoring-crd-manager
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      name: rancher-monitoring-crd-manager
    values:
      global:
        fleet:
          clusterLabels:
            management.cattle.io/cluster-display-name: local
            management.cattle.io/cluster-name: local
            provider.cattle.io: harvester
    version: 121
  status:
    observedGeneration: 242
    summary:
      state: deployed
kind: List
metadata:
  continue: "null"
  resourceVersion: "14659119"
